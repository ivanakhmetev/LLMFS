{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Механизм внимания\n",
    "\n",
    "В предыдущей главе мы готовили человеческий текст к восприятию машиной.  \n",
    "Теперь рассмотрим основной механизм больших языковых моделей, разбив рассмотрение на 4 усложняющихся шага.\n",
    "\n",
    "![](images/llm3.2.png)\n",
    "\n",
    "![](images/llm3.3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Проблема длины контекста (шаг 0)\n",
    "\n",
    "Что такое перевод? - Это сопоставления токенов из разных словарей (с позиции этой книги).\n",
    "\n",
    "А т.к. у различных языков различные грамматики, то минимальный смысловой токен - предложение (перевод слово - к слову невозможен, рисунок).   \n",
    "Т.е. необходим учет контекста, причем чем он длиннее - тем лучше, но его сложнее вычислить - проблема.\n",
    "\n",
    "Решением этой проблемы были рекурентные нейронные сети (РНН). В этих сетях результат работы одного блока, идет на вход следующего, откуда и берется название (рекурентный член последовательности вычисляется на основе предыдущего): \n",
    "1. Входной текст подается в шифратор \n",
    "2. Шифратор обновляет значения слоев (состояния скрытых слоев, hidden states), так, что в последнем слое зашифрован контекст всего входящего текста.\n",
    "3. Дешифратор получает этот последний слой, чтобы генерировать перевод предложения (уже не рекурентно, опираясь каждый раз на слой-контекст, который не меняется).\n",
    "\n",
    "Т.е. нейронная сеть, построенная на рекурентном механизме способна работать с контекстом как векторным представлением, но без динамики - дешифратору доступно только последнее состояние контекста. Таким образом, по построению, возникает ограничение на длину и сложность входного текста. \n",
    "\n",
    "![](images/llm3.4.png)\n",
    "\n",
    "![](images/llm3.5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Примитивное внимание: учет контекста (шаг 1)\n",
    "\n",
    "В 2014 году был предложен механизм, позволяющий дешифратору получать все предыдущие состояния, а не только последнее. Причем дешифратор может ранжировать вклад скрытых слоев при генерации выходного токена.  \n",
    "  \n",
    "Ранг токена называется его весом (матрица весов слоя). \n",
    "\n",
    "В общем, идея внимания - это идея построения не просто понятного машине контекста, а эффективное его построение. На этом шаге объекты(токены) контекста восприятия машины количественно измеряются как более и менее важные.\n",
    "\n",
    "![](images/llm3.6.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Стандартное внимание (шаг 2)\n",
    "#### 3.3.1. Вычисление ранга относительно одной позиции.  \n",
    "\n",
    "На этом шаге показавыается способ измерения важности токенов - вычисление весов внимания (attention weight). Обратим внимание, что эти веса вычисляются относительно позиции. Т.е. важность токена может быть разной относительно контекста.\n",
    "\n",
    "Есть входящая последовательность токенов.  \n",
    "Каждый элемент последовательности, представлен как 3-мерный вектор (размерность 3 выбрана для простоты иллюстраций).\n",
    "\n",
    "Наша цель — вычислить векторы контекста относительно каждого элемента входящей последовательности.  \n",
    "Для этого: \n",
    "- Вычислим промежуточные значения помощью скалярного произведения. Эти промежуточные значения называются, называются оценками внимания (attention score).\n",
    "- Нормализуем эти значения. Нормализованные оценки внимания называются весами (attention weight).\n",
    "\n",
    "Скалярное произведение определим как сумму поэлементного умножения. \n",
    "\"Физический смысл\" скалярного произведения - определение степени \"перекрытия\" векторов. Так, ортогональные вектора (1, 0) и (0, 1) вообще не \"перекрываются\" и дадут 0. Т.е. чем больше значение скалярное произведение - тем больше их \"перекрытие\" и оценка внимания. \n",
    "\n",
    "![](images/llm3.8.png)\n",
    "\n",
    "![](images/llm3.9.png) \n",
    "\n",
    "![](images/llm3.10.png)\n",
    "\n",
    "![](images/llm3.11.png) \n",
    "\n",
    "![](images/llm3.13.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7596/3718887927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'install torch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# векторное представление входящий последовательности\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m inputs = torch.tensor(\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.3.0-cp39-cp39-win_amd64.whl (159.7 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\anaconda\\lib\\site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch) (2.6.3)\n",
      "Collecting typing-extensions>=4.8.0\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda\\lib\\site-packages (from torch) (2021.10.1)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Collecting tbb==2021.*\n",
      "  Downloading tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "Collecting intel-openmp==2021.*\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\anaconda\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: tbb, intel-openmp, typing-extensions, mkl, torch\n",
      "  Attempting uninstall: tbb\n",
      "    Found existing installation: TBB 0.2\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "import torch\n",
    "\n",
    "# векторное представление входящий последовательности\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# вычисление оценок внимания\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# 1ый вариант нормализации \n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 2ой вариант нормализации. Нормализация с помощью softmax считается предпочтительнее, т.к. гарантирует, что веса всегда будут положительными. \n",
    "# Это позволяет интерпретировать выходные данные как вероятности или относительную важность (больший вес - большая важность).\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    " \n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисление векторного представления контекста относительно второго элемента входящей последовательности - inputs[1]\n",
    "\n",
    "query = inputs[1] \n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2_naive[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Вычисление ранга относительно всех позиций\n",
    "\n",
    "Результаты предыдущего шага:  \n",
    "\n",
    "![](images/llm3.12.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисление оценок внимания с помощью скалярного произведения\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код выше можно записать с помощью векторного произвденеия вектора inputs и транспонированного inputs.T (вычислительно эффективнее)\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 2, as illustrated in Figure 3.12, we now normalize each row so that the values in each row sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нормализация со стандартной логистической функцией\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тест нормализации\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# шаг 3 - скалярное (матричное) произведение векторов весов внимания и входящих токенов\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs) # Значения вектора весов контекста z_2, полученные сейчас и в е 3.3.1 совпали.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Вычисление ранга относительно всех возможных позиций (trainable weights)\n",
    "\n",
    "Мы научились вычислять векторное представление контекста, связывая токены, векторные представления которых \"накладываются\". \n",
    "Но как расширить \"глубину\" контекста от предложений и абзацев до глав и книг? Необходимо усложнить механизм внимания, как-то учесть значимость контекста не только конкретного входного текста, но и всех возможных входных текстов. А т.к. \"все возможные\" тексты обработать нельзя, искомый механизм должен быть механизмом изменения, т.е. обучения от входного текста: \n",
    "- вводятся изменяющиеся матрицы вход, ключ, значение\n",
    "-  \n",
    "\n",
    "\n",
    "\n",
    "Реализуем логику улучшения векторного представления контекста с помощью изменения весов внимания:\n",
    "- для каждого входящего токена создадим пару ключ - значения, где:\n",
    "    - значение получено умножением матрицы весов на входящий токен\n",
    "    - \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Наиболее заметным отличием является введение весовых матриц, которые обновляются во время обучения модели. Эти обучаемые весовые матрицы имеют решающее значение для того, чтобы модель (в частности, модуль внимания внутри модели) могла научиться создавать «хорошие» векторы контекста. (Обратите внимание, что мы будем обучать LLM в главе 5.)\n",
    "\n",
    "Мы рассмотрим этот механизм самообслуживания в двух подразделах. Сначала мы напишем код шаг за шагом, как и раньше. Во-вторых, мы организуем код в компактный класс Python, который можно будет импортировать в архитектуру LLM, код которой мы напишем в главе 4.\n",
    "3.4.1 Пошаговое вычисление весов внимания\n",
    "\n",
    "Мы будем реализовывать механизм самообслуживания шаг за шагом, вводя три обучаемые весовые матрицы Wq, Wk и Wv. Эти три матрицы используются для проецирования встроенных входных токенов x(i) в векторы запроса, ключа и значения, как показано на рисунке 3.14.\n",
    "\n",
    "\n",
    "Рисунок 3.14. На первом этапе механизма самообслуживания с обучаемыми весовыми матрицами мы вычисляем векторы запроса (q), ключа (k) и значения (v) для входных элементов x. Как и в предыдущих разделах, мы обозначаем второй вход, x(2), как вход запроса. Вектор запроса q(2) получается путем умножения матриц входных данных x(2) и весовой матрицы Wq. Аналогичным образом мы получаем векторы ключа и значения путем умножения матриц с использованием весовых матриц Wk и Wv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее в разделе 3.3.1 мы определили второй входной элемент x(2) как запрос при вычислении упрощенных весов внимания для вычисления вектора контекста z(2). Позже, в разделе 3.3.2, мы обобщили это для вычисления всех векторов контекста z(1) ... z(T) для входного предложения из шести слов «Ваше путешествие начинается с одного шага».\n",
    "\n",
    "Аналогично, в целях иллюстрации мы начнем с вычисления только одного вектора контекста z(2). В следующем разделе мы изменим этот код для расчета всех векторов контекста.\n",
    "\n",
    "Начнем с определения нескольких переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в моделях, подобных GPT, входные и выходные измерения обычно одинаковы, но в целях иллюстрации, чтобы лучше следить за вычислениями, мы выбираем здесь разные входные (d_in=3) и выходные измерения (d_out=2).\n",
    "\n",
    "Далее мы инициализируем три весовые матрицы Wq, Wk и Wv, показанные на рисунке 3.14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы устанавливаем require_grad=False, чтобы уменьшить беспорядок в выходных данных в целях иллюстрации, но если бы мы использовали весовые матрицы для обучения модели, мы бы установили require_grad=True для обновления этих матриц во время обучения модели.\n",
    "\n",
    "Далее мы вычисляем векторы запроса, ключа и значения, как показано ранее на рисунке 3.14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = x_2 @ W_query \n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим на основе результатов запроса, в результате получается двумерный вектор, поскольку мы устанавливаем количество столбцов соответствующей весовой матрицы через d_out равным 2:\n",
    "\n",
    "тензор([0,4306, 1,4551])\n",
    "\n",
    "\n",
    "Весовые параметры и веса внимания\n",
    "\n",
    "Обратите внимание, что в весовых матрицах W термин «вес» является сокращением от «весовых параметров» — значений нейронной сети, которые оптимизируются во время обучения. Это не следует путать с весами внимания. Как мы уже видели в предыдущем разделе, веса внимания определяют степень, в которой вектор контекста зависит от различных частей входных данных, т. е. в какой степени сеть фокусируется на различных частях входных данных.\n",
    "\n",
    "Подводя итог, можно сказать, что весовые параметры — это фундаментальные, изученные коэффициенты, которые определяют связи в сети, а веса внимания — это динамические, зависящие от контекста значения.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Несмотря на то, что наша временная цель состоит в том, чтобы вычислить только один вектор контекста, z(2), нам по-прежнему требуются векторы ключа и значения для всех входных элементов, поскольку они участвуют в вычислении весов внимания относительно запроса q(2), как показано на рисунке 3.14.\n",
    "\n",
    "Мы можем получить все ключи и значения путем умножения матриц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "#A we can tell from the outputs, we successfully projected the 6 input tokens from a 3D onto a 2D embedding space:\n",
    "\n",
    "keys.shape: torch.Size([6, 2])\n",
    "values.shape: torch.Size([6, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вторым шагом будет вычисление показателей внимания, как показано на рис. 3.15.\n",
    "\n",
    "Рис. 3.15. Вычисление оценки внимания представляет собой вычисление скалярного произведения, аналогичное тому, что мы использовали в упрощенном механизме самообслуживания в разделе 3.3. Новым аспектом здесь является то, что мы не вычисляем скалярное произведение между входными элементами напрямую, а используем запрос и ключ, полученные путем преобразования входных данных с помощью соответствующих весовых матриц.\n",
    "\n",
    "Сначала вычислим показатель внимания ω22:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опять же, мы можем обобщить эти вычисления на все показатели внимания посредством умножения матриц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, в качестве быстрой проверки второй элемент в выходных данных соответствует attn_score_22, который мы вычислили ранее:\n",
    "\n",
    "тензор([1,2705, 1,8524, 1,8111, 1,0795, 0,5577, 1,5440])\n",
    "\n",
    "На третьем этапе мы переходим от оценок внимания к весам внимания, как показано на рис. 3.16.\n",
    "\n",
    "Рисунок 3.16. После вычисления оценок внимания ω следующим шагом является нормализация этих оценок с помощью функции softmax для получения весов внимания α.\n",
    "\n",
    "Затем, как показано на рис. 3.16, мы вычисляем веса внимания, масштабируя оценки внимания и используя функцию softmax, которую мы использовали ранее. Отличие от предыдущей версии состоит в том, что теперь мы масштабируем оценки внимания, разделив их на квадратный корень из вложения размер клавиш (обратите внимание, что извлечение квадратного корня математически эквивалентно возведению в степень на 0,5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обоснование внимания к точечному продукту\n",
    "\n",
    "Причина нормализации по размеру встраивания состоит в том, чтобы улучшить производительность обучения за счет исключения небольших градиентов. Например, при увеличении размера внедрения, который обычно превышает тысячу для GPT-подобных LLM, большие скалярные произведения могут привести к очень небольшим градиентам во время обратного распространения ошибки из-за примененной к ним функции softmax. По мере увеличения скалярного произведения функция softmax ведет себя скорее как ступенчатая функция, в результате чего градиенты приближаются к нулю. Эти небольшие градиенты могут резко замедлить обучение или привести к его застою.\n",
    "\n",
    "Масштабирование с помощью квадратного корня из измерения внедрения является причиной того, почему этот механизм самообслуживания также называется вниманием масштабированного скалярного произведения.\n",
    "\n",
    "Теперь последний шаг — вычислить векторы контекста, как показано на рисунке 3.17.\n",
    "\n",
    "Рис. 3.17. На заключительном этапе вычисления внутреннего внимания мы вычисляем вектор контекста, объединяя все векторы значений через веса внимания.\n",
    "\n",
    "Подобно разделу 3.3, где мы вычисляли вектор контекста как взвешенную сумму входных векторов, теперь мы вычисляем вектор контекста как взвешенную сумму векторов значений. Здесь веса внимания служат весовым коэффициентом, который взвешивает соответствующую важность каждого вектора значений. Как и в разделе 3.3, мы можем использовать умножение матриц для получения результата за один шаг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Содержимое результирующего вектора следующее:\n",
    "\n",
    "тензор([0,3061, 0,8210])\n",
    "\n",
    "До сих пор мы вычислили только один вектор контекста z(2). В следующем разделе мы обобщим код для вычисления всех векторов контекста во входной последовательности от z(1) до z(T).\n",
    "\n",
    "\n",
    "Зачем нужен запрос, ключ и значение?\n",
    "\n",
    "Термины «ключ», «запрос» и «значение» в контексте механизмов внимания заимствованы из области поиска информации и баз данных, где аналогичные понятия используются для хранения, поиска и извлечения информации.\n",
    "\n",
    "«Запрос» аналогичен поисковому запросу в базе данных. Он представляет текущий элемент (например, слово или токен в предложении), на котором модель фокусируется или пытается понять. Запрос используется для проверки других частей входной последовательности, чтобы определить, сколько внимания им следует уделить.\n",
    "\n",
    "«Ключ» подобен ключу базы данных, используемому для индексации и поиска. В механизме внимания каждый элемент входной последовательности (например, каждое слово в предложении) имеет связанный ключ. Эти ключи используются для сопоставления с запросом.\n",
    "\n",
    "«Значение» в этом контексте аналогично значению в паре «ключ-значение» в базе данных. Он представляет собой фактическое содержимое или представление входных элементов. Как только модель определяет, какие ключи (и, следовательно, какие части входных данных) наиболее релевантны запросу (текущий элемент фокуса), она извлекает соответствующие значения.\n",
    "\n",
    "\n",
    "3.4.2. Реализация компактного класса Python с самообслуживанием\n",
    "\n",
    "В предыдущих разделах мы выполнили множество шагов по вычислению результатов самообслуживания. В основном это было сделано в целях иллюстрации, чтобы мы могли проходить по одному шагу за раз. На практике, учитывая реализацию LLM, описанную в следующей главе, полезно организовать этот код в класс Python следующим образом:\n",
    "\n",
    "Листинг 3.1. Компактный класс самообслуживания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    " \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом коде PyTorch SelfAttention_v1 — это класс, производный от nn.Module, который является фундаментальным строительным блоком моделей PyTorch и предоставляет необходимые функциональные возможности для создания слоев модели и управления ими.\n",
    "\n",
    "Метод __init__ инициализирует обучаемые весовые матрицы (W_query, W_key и W_value) для запросов, ключей и значений, каждая из которых преобразует входное измерение d_in в выходное измерение d_out.\n",
    "\n",
    "Во время прямого прохода, используя метод Forward, мы вычисляем оценки внимания (attn_scores) путем умножения запросов и ключей, нормализуя эти оценки с помощью softmax. Наконец, мы создаем вектор контекста, взвешивая значения с помощью этих нормализованных показателей внимания.\n",
    "\n",
    "Мы можем использовать этот класс следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для быстрой проверки обратите внимание, что вторая строка ([0.3061, 0.8210]) соответствует содержимому context_vec_2 в предыдущем разделе.\n",
    "\n",
    "Рисунок 3.18 суммирует механизм самообслуживания, который мы только что реализовали.\n",
    "\n",
    "Рисунок 3.18. Внимательно мы преобразуем входные векторы во входную матрицу X с помощью трех весовых матриц: Wq, Wk и Wv. Затем мы вычисляем матрицу весов внимания на основе полученных запросов (Q) и ключей (K). Используя веса и значения внимания (V), мы затем вычисляем векторы контекста (Z). (Для наглядности на этом рисунке мы фокусируемся на одном входном тексте с n токенами, а не на пакете из нескольких входных данных. Следовательно, тензор трехмерного ввода в этом контексте упрощается до двумерной матрицы. Этот подход обеспечивает более простую визуализацию. и понимание происходящих процессов.)\n",
    "\n",
    "\n",
    "\n",
    "Как показано на рис. 3.18, самовнимание включает в себя обучаемые весовые матрицы Wq, Wk и Wv. Эти матрицы преобразуют входные данные в запросы, ключи и значения, которые являются важнейшими компонентами механизма внимания. Поскольку во время обучения модель получает больше данных, она корректирует эти обучаемые веса, как мы увидим в следующих главах.\n",
    "\n",
    "Мы можем улучшить реализацию SelfAttention_v1, используя слои nn.Linear PyTorch, которые эффективно выполняют умножение матриц, когда модули смещения отключены. Кроме того, существенным преимуществом использования nn.Linear вместо реализации nn.Parameter(torch.rand(...) вручную) является то, что nn.Linear имеет оптимизированную схему инициализации весов, что способствует более стабильному и эффективному обучению модели.\n",
    "\n",
    "Листинг 3.2. Класс самообслуживания, использующий линейные слои PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    " \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use the SelfAttention_v2 similar to SelfAttention_v1:\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что SelfAttention_v1 и SelfAttention_v2 дают разные выходные данные, поскольку они используют разные начальные веса для весовых матриц, поскольку nn.Linear использует более сложную схему инициализации весов.\n",
    "\n",
    "\n",
    "\n",
    "Упражнение 3.1. Сравнение SelfAttention_v1 и SelfAttention_v2\n",
    "\n",
    "Обратите внимание, что nn.Linear в SelfAttention_v2 использует другую схему инициализации веса, чем nn.Parameter(torch.rand(d_in, d_out)) в SelfAttention_v1, что приводит к тому, что оба механизма выдают разные результаты. Чтобы проверить, что обе реализации, SelfAttention_v1 и SelfAttention_v2, в остальном похожи, мы можем перенести весовые матрицы из объекта SelfAttention_v2 в SelfAttention_v1, чтобы оба объекта затем давали одинаковые результаты.\n",
    "\n",
    "Ваша задача — правильно назначить веса экземпляра SelfAttention_v2 экземпляру SelfAttention_v1. Для этого нужно понять взаимосвязь весов в обоих вариантах. (Подсказка: nn.Linear хранит матрицу весов в транспонированной форме.) После присваивания вы должны заметить, что оба экземпляра выдают одинаковые выходные данные.\n",
    "\n",
    "\n",
    "\n",
    "В следующем разделе мы усовершенствуем механизм само-внимания, уделяя особое внимание включению причинных и многоголовых элементов. Причинный аспект предполагает изменение механизма внимания, чтобы предотвратить доступ модели к будущей информации в последовательности, что имеет решающее значение для таких задач, как моделирование языка, где предсказание каждого слова должно зависеть только от предыдущих слов.\n",
    "\n",
    "Компонент «многоголовость» предполагает разделение механизма внимания на несколько «голов». Каждая голова изучает различные аспекты данных, что позволяет модели одновременно обрабатывать информацию из разных подпространств представления в разных позициях. Это улучшает производительность модели в сложных задачах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Скрытие будущих слов с помощью причинного внимания\n",
    "\n",
    "В этом разделе мы модифицируем стандартный механизм само-внимания, чтобы создать механизм причинно-следственного внимания, который необходим для разработки LLM в последующих главах.\n",
    "\n",
    "Причинное внимание, также известное как замаскированное внимание, представляет собой специализированную форму внимания к себе. Это ограничивает модель рассмотрением только предыдущих и текущих входных данных в последовательности при обработке любого данного токена. В этом отличие от стандартного механизма самообслуживания, который позволяет получить доступ ко всей последовательности ввода одновременно.\n",
    "\n",
    "Следовательно, при вычислении показателей внимания механизм причинно-следственного внимания гарантирует, что модель учитывает только токены, которые встречаются на или до текущего токена в последовательности.\n",
    "\n",
    "Чтобы добиться этого в LLM, подобных GPT, для каждого обработанного токена мы маскируем будущие токены, которые идут после текущего токена во входном тексте, как показано на рисунке 3.19.\n",
    "\n",
    "Рисунок 3.19. В каузальном внимании мы маскируем веса внимания над диагональю, так что для данного входного сигнала LLM не может получить доступ к будущим токенам при вычислении векторов контекста с использованием весов внимания. Например, для слова «путешествие» во второй строке мы сохраняем веса внимания только для слов до («Ваш») и в текущей позиции («путешествие»).\n",
    "\n",
    "\n",
    "\n",
    "Как показано на рисунке 3.19, мы маскируем веса внимания над диагональю и нормализуем немаскированные веса внимания так, чтобы сумма весов внимания равнялась 1 в каждой строке. В следующем разделе мы реализуем эту процедуру маскировки и нормализации в коде.\n",
    "3.5.1 Применение маски причинного внимания\n",
    "\n",
    "В этом разделе мы реализуем маску причинного внимания в коде. Начнем с процедуры, представленной на рис. 3.20.\n",
    "\n",
    "Рисунок 3.20. Один из способов получить матрицу весов замаскированного внимания для причинного внимания — применить функцию softmax к показателям внимания, обнулив элементы над диагональю и нормализовав полученную матрицу.\n",
    "\n",
    "\n",
    "\n",
    "Чтобы реализовать шаги по применению маски причинного внимания для получения замаскированных весов внимания, как показано на рисунке 3.20, давайте поработаем с показателями и весами внимания из предыдущего раздела, чтобы закодировать механизм причинного внимания.\n",
    "\n",
    "На первом этапе, показанном на рисунке 3.20, мы вычисляем веса внимания с помощью функции softmax, как мы это делали в предыдущих разделах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can implement step 2 in Figure 3.20 using PyTorch's tril function to create a mask where the values above the diagonal are zero:\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n",
    "\n",
    "The resulting mask is as follows:\n",
    "\n",
    "\n",
    "tensor([[1., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 0.],\n",
    "        [1., 1., 1., 1., 1., 1.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, we can multiply this mask with the attention weights to zero out the values above the diagonal:\n",
    "\n",
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)\n",
    "\n",
    "As we can see, the elements above the diagonal are successfully zeroed out:\n",
    "\n",
    "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
    "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
    "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<MulBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " The third step in Figure 3.20 is to renormalize the attention weights to sum up to 1 again in each row. We can achieve this by dividing each element in each row by the sum in each row:\n",
    "\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)\n",
    "\n",
    "\n",
    "\n",
    "The result is an attention weight matrix where the attention weights above the diagonal are zeroed out and where the rows sum to 1:\n",
    "\n",
    "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
    "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<DivBackward0>)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.5 Hiding future words with causal attention\n",
    "\n",
    "In this section, we modify the standard self-attention mechanism to create a causal attention mechanism, which is essential for developing an LLM in the subsequent chapters.\n",
    "\n",
    "Causal attention, also known as masked attention, is a specialized form of self-attention. It restricts a model to only consider previous and current inputs in a sequence when processing any given token. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.\n",
    "\n",
    "Consequently, when computing attention scores, the causal attention mechanism ensures that the model only factors in tokens that occur at or before the current token in the sequence.\n",
    "\n",
    "To achieve this in GPT-like LLMs, for each token processed, we mask out the future tokens, which come after the current token in the input text, as illustrated in Figure 3.19.\n",
    "\n",
    "Figure 3.19 In causal attention, we mask out the attention weights above the diagonal such that for a given input, the LLM can't access future tokens when computing the context vectors using the attention weights. For example, for the word \"journey\" in the second row, we only keep the attention weights for the words before (\"Your\") and in the current position (\"journey\").\n",
    "\n",
    "\n",
    "\n",
    "As illustrated in Figure 3.19, we mask out the attention weights above the diagonal, and we normalize the non-masked attention weights, such that the attention weights sum to 1 in each row. In the next section, we will implement this masking and normalization procedure in code.\n",
    "3.5.1 Applying a causal attention mask\n",
    "\n",
    "In this section, we implement the causal attention mask in code. We start with the procedure summarized in Figure 3.20.\n",
    "\n",
    "Figure 3.20 One way to obtain the masked attention weight matrix in causal attention is to apply the softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing the resulting matrix.\n",
    "\n",
    "\n",
    "\n",
    "To implement the steps to apply a causal attention mask to obtain the masked attention weights as summarized in Figure 3.20, let's work with the attention scores and weights from the previous section to code the causal attention mechanism.\n",
    "\n",
    "In the first step illustrated in Figure 3.20, we compute the attention weights using the softmax function as we have done in previous sections:\n",
    "\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)\n",
    "\n",
    "This results in the following attention weights:\n",
    "\n",
    "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
    "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
    "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
    "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
    "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<SoftmaxBackward0>)\n",
    "\n",
    "We can implement step 2 in Figure 3.20 using PyTorch's tril function to create a mask where the values above the diagonal are zero:\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n",
    "\n",
    "The resulting mask is as follows:\n",
    "\n",
    "\n",
    "tensor([[1., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 0.],\n",
    "        [1., 1., 1., 1., 1., 1.]])\n",
    "\n",
    "Now, we can multiply this mask with the attention weights to zero out the values above the diagonal:\n",
    "\n",
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)\n",
    "\n",
    "As we can see, the elements above the diagonal are successfully zeroed out:\n",
    "\n",
    "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
    "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
    "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<MulBackward0>)\n",
    " \n",
    "\n",
    " The third step in Figure 3.20 is to renormalize the attention weights to sum up to 1 again in each row. We can achieve this by dividing each element in each row by the sum in each row:\n",
    "\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)\n",
    "\n",
    "\n",
    "\n",
    "The result is an attention weight matrix where the attention weights above the diagonal are zeroed out and where the rows sum to 1:\n",
    "\n",
    "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
    "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<DivBackward0>)\n",
    "\n",
    "\n",
    "Information leakage\n",
    "\n",
    "When we apply a mask and then renormalize the attention weights, it might initially appear that information from future tokens (which we intend to mask) could still influence the current token because their values are part of the softmax calculation. However, the key insight is that when we renormalize the attention weights after masking, what we're essentially doing is recalculating the softmax over a smaller subset (since masked positions don't contribute to the softmax value).\n",
    "\n",
    "The mathematical elegance of softmax is that despite initially including all positions in the denominator, after masking and renormalizing, the effect of the masked positions is nullified — they don't contribute to the softmax score in any meaningful way.\n",
    "\n",
    "In simpler terms, after masking and renormalization, the distribution of attention weights is as if it was calculated only among the unmasked positions to begin with. This ensures there's no information leakage from future (or otherwise masked) tokens as we intended.\n",
    "\n",
    "\n",
    "While we could be technically done with implementing causal attention at this point, we can take advantage of a mathematical property of the softmax function and implement the computation of the masked attention weights more efficiently in fewer steps, as shown in Figure 3.21.\n",
    "\n",
    "Figure 3.21 A more efficient way to obtain the masked attention weight matrix in causal attention is to mask the attention scores with negative infinity values before applying the softmax function.\n",
    "\n",
    "\n",
    "\n",
    "The softmax function converts its inputs into a probability distribution. When negative infinity values (-∞) are present in a row, the softmax function treats them as zero probability. (Mathematically, this is because e-∞ approaches 0.)\n",
    "\n",
    "We can implement this more efficient masking \"trick\" by creating a mask with 1's above the diagonal and then replacing these 1's with negative infinity (-inf) values:\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)\n",
    "\n",
    "This results in the following mask:\n",
    "\n",
    "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
    "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
    "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
    "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
    "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
    "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
    "       grad_fn=<MaskedFillBackward0>)\n",
    " \n",
    "Now, all we need to do is apply the softmax function to these masked results, and we are done:\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)\n",
    "\n",
    "As we can see based on the output, the values in each row sum to 1, and no further normalization is necessary:\n",
    "\n",
    "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
    "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<SoftmaxBackward0>)\n",
    "\n",
    "We could now use the modified attention weights to compute the context vectors via context_vec = attn_weights @ values, as in section 3.4. However, in the next section, we first cover another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs.\n",
    "\n",
    "\n",
    "\n",
    "3.5.2 Masking additional attention weights with dropout\n",
    "\n",
    "Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively \"dropping\" them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It's important to emphasize that dropout is only used during training and is disabled afterward.\n",
    "\n",
    "In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied in two specific areas: after calculating the attention scores or after applying the attention weights to the value vectors.\n",
    "\n",
    "Here, we will apply the dropout mask after computing the attention weights, as illustrated in Figure 3.22, because it's the more common variant in practice.\n",
    "\n",
    "Figure 3.22 Using the causal attention mask (upper left), we apply an additional dropout mask (upper right) to zero out additional attention weights to reduce overfitting during training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the following code example, we use a dropout rate of 50%, which means masking out half of the attention weights. (When we train the GPT model in later chapters, we will use a lower dropout rate, such as 0.1 or 0.2.)\n",
    "\n",
    "In the following code, we apply PyTorch's dropout implementation first to a 6×6 tensor consisting of ones for illustration purposes:\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))\n",
    "\n",
    "s we can see, approximately half of the values are zeroed out:\n",
    "\n",
    "tensor([[2., 2., 0., 2., 2., 0.],\n",
    "        [0., 0., 0., 2., 0., 2.],\n",
    "        [2., 2., 2., 2., 0., 2.],\n",
    "        [0., 2., 2., 0., 0., 2.],\n",
    "        [0., 2., 0., 2., 0., 2.],\n",
    "        [0., 2., 2., 2., 2., 0.]])\n",
    "\n",
    "\n",
    "\n",
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 =2. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases.\n",
    "\n",
    "Now, let's apply dropout to the attention weight matrix itself:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))\n",
    "\n",
    "The resulting attention weight matrix now has additional elements zeroed out and the remaining ones rescaled:\n",
    "\n",
    "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
    "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
    "       grad_fn=<MulBackward0>\n",
    "\n",
    "\n",
    "\n",
    "Note that the resulting dropout outputs may look different depending on your operating system; you can read more about this inconsistency [here on the PyTorch issue tracker at https://github.com/pytorch/pytorch/issues/121595.\n",
    "\n",
    "Having gained an understanding of causal attention and dropout masking, we will develop a concise Python class in the following section. This class is designed to facilitate the efficient application of these two techniques.\n",
    "3.5.3 Implementing a compact causal attention class\n",
    "\n",
    "In this section, we will now incorporate the causal attention and dropout modifications into the SelfAttention Python class we developed in section 3.4. This class will then serve as a template for developing multi-head attention in the upcoming section, which is the final attention class we implement in this chapter.\n",
    "\n",
    "But before we begin, one more thing is to ensure that the code can handle batches consisting of more than one input so that the CausalAttention class supports the batch outputs produced by the data loader we implemented in chapter 2.\n",
    "\n",
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)\n",
    "\n",
    "This results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token is a 3-dimensional embedding vector:\n",
    "\n",
    "torch.Size([2, 6, 3])\n",
    "\n",
    "\n",
    "\n",
    "The following CausalAttention class is similar to the SelfAttention class we implemented earlier, except that we now added the dropout and causal mask components as highlighted in the following code:\n",
    "\n",
    "Listing 3.3 A compact causal attention class\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "           'mask',\n",
    "           torch.triu(torch.ones(context_length, context_length),\n",
    "           diagonal=1)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    " \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    " \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "\n",
    "While all added code lines should be familiar from previous sections, we now added a self.register_buffer() call in the __init__ method. The use of register_buffer in PyTorch is not strictly necessary for all use cases but offers several advantages here. For instance, when we use the CausalAttention class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will be relevant when training the LLM in future chapters. This means we don't need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors.\n",
    "\n",
    "We can use the CausalAttention class as follows, similar to SelfAttention previously:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "The resulting context vector is a 3D tensor where each token is now represented by a 2D embedding:\n",
    "\n",
    "\n",
    "context_vecs.shape: torch.Size([2, 6, 2])\n",
    "\n",
    "Figure 3.23 provides a mental model that summarizes what we have accomplished so far.\n",
    "\n",
    "\n",
    "Figure 3.23 A mental model summarizing the four different attention modules we are coding in this chapter. We began with a simplified attention mechanism, added trainable weights, and then added a casual attention mask. In the remainder of this chapter, we will extend the causal attention mechanism and code multi-head attention, which is the final module we will use in the LLM implementation in the next chapter.\n",
    "\n",
    "As illustrated in Figure 3.23, in this section, we focused on the concept and implementation of causal attention in neural networks. In the next section, we will expand on this concept and implement a multi-head attention module that implements several of such causal attention mechanisms in parallel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.6 Extending single-head attention to multi-head attention\n",
    "\n",
    "In this final section of this chapter, we are extending the previously implemented causal attention class over multiple-heads. This is also called multi-head attention.\n",
    "\n",
    "The term \"multi-head\" refers to dividing the attention mechanism into multiple \"heads,\" each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.\n",
    "\n",
    "In the following subsections, we will tackle this expansion from causal attention to multi-head attention. The first subsection will intuitively build a multi-head attention module by stacking multiple CausalAttention modules for illustration purposes. The second subsection will then implement the same multi-head attention module in a more complicated but computationally more efficient way.\n",
    "3.6.1 Stacking multiple single-head attention layers\n",
    "\n",
    "In practical terms, implementing multi-head attention involves creating multiple instances of the self-attention mechanism (depicted earlier in Figure 3.18 in section 3.4.1), each with its own weights, and then combining their outputs. Using multiple instances of the self-attention mechanism can be computationally intensive, but it's crucial for the kind of complex pattern recognition that models like transformer-based LLMs are known for.\n",
    "\n",
    "Figure 3.24 illustrates the structure of a multi-head attention module, which consists of multiple single-head attention modules, as previously depicted in Figure 3.18, stacked on top of each other.\n",
    "\n",
    "Figure 3.24 The multi-head attention module in this figure depicts two single-head attention modules stacked on top of each other. So, instead of using a single matrix Wv for computing the value matrices, in a multi-head attention module with two heads, we now have two value weight matrices: Wv1 and Wv2. The same applies to the other weight matrices, Wq and Wk. We obtain two sets of context vectors Z1 and Z2 that we can combine into a single context vector matrix Z.\n",
    "\n",
    "\n",
    "\n",
    "As mentioned before, the main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections -- the results of multiplying the input data (like the query, key, and value vectors in attention mechanisms) by a weight matrix.\n",
    "\n",
    "In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper class that stacks multiple instances of our previously implemented CausalAttention module:\n",
    "\n",
    "Listing 3.4 A wrapper class to implement multi-head attention\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via num_heads=2) and CausalAttention output dimension d_out=2, this results in a 4-dimensional context vectors (d_out*num_heads=4), as illustrated in Figure 3.25.\n",
    "\n",
    "Figure 3.25 Using the MultiHeadAttentionWrapper, we specified the number of attention heads (num_heads). If we set num_heads=2, as shown in this figure, we obtain a tensor with two sets of context vector matrices. In each context vector matrix, the rows represent the context vectors corresponding to the tokens, and the columns correspond to the embedding dimension specified via d_out=4. We concatenate these context vector matrices along the column dimension. Since we have 2 attention heads and an embedding dimension of 2, the final embedding dimension is 2 × 2 = 4.\n",
    "\n",
    "To illustrate Figure 3.25 further with a concrete example, we can use the MultiHeadAttentionWrapper class similar to the CausalAttention class before:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    " \n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "This results in the following tensor representing the context vectors:\n",
    "\n",
    "\n",
    "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
    "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
    "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
    "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
    "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
    "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
    " \n",
    "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
    "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
    "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
    "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
    "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
    "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
    "context_vecs.shape: torch.Size([2, 6, 4])\n",
    "\n",
    "\n",
    "\n",
    "The first dimension of the resulting context_vecs tensor is 2 since we have two input texts (the input texts are duplicated, which is why the context vectors are exactly the same for those). The second dimension refers to the 6 tokens in each input. The third dimension refers to the 4-dimensional embedding of each token.\n",
    "Exercise 3.2 Returning 2-dimensional embedding vectors\n",
    "\n",
    "Change the input arguments for the MultiHeadAttentionWrapper(..., num_heads=2) call such that the output context vectors are 2-dimensional instead of 4-dimensional while keeping the setting num_heads=2. Hint: You don't have to modify the class implementation; you just have to change one of the other input arguments.\n",
    "\n",
    "In this section, we implemented a MultiHeadAttentionWrapper that combined multiple single-head attention modules. However, note that these are processed sequentially via [head(x) for head in self.heads] in the forward method. We can improve this implementation by processing the heads in parallel. One way to achieve this is by computing the outputs for all attention heads simultaneously via matrix multiplication, as we will explore in the next section.\n",
    "3.6.2 Implementing multi-head attention with weight splits\n",
    "\n",
    "In the previous section, we created a MultiHeadAttentionWrapper to implement multi-head attention by stacking multiple single-head attention modules. This was done by instantiating and combining several CausalAttention objects.\n",
    "\n",
    "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and CausalAttention, we can combine both of these concepts into a single MultiHeadAttention class. Also, in addition to just merging the MultiHeadAttentionWrapper with the CausalAttention code, we will make some other modifications to implement multi-head attention more efficiently.\n",
    "\n",
    "In the MultiHeadAttentionWrapper, multiple heads are implemented by creating a list of CausalAttention objects (self.heads), each representing a separate attention head. The CausalAttention class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following MultiHeadAttention class integrates the multi-head functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention.\n",
    "\n",
    "Let's take a look at the MultiHeadAttention class before we discuss it further:\n",
    "\n",
    "Listing 3.5 An efficient multi-head attention class\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    " \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "             torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    " \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    " \n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    " \n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "  \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    " \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    " \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "\n",
    "Even though the reshaping (.view) and transposing (.transpose) of tensors inside the MultiHeadAttention class looks very complicated, mathematically, the MultiHeadAttention class implements the same concept as the MultiHeadAttentionWrapper earlier.\n",
    "\n",
    "On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked multiple single-head attention layers that we combined into a multi-head attention layer. The MultiHeadAttention class takes an integrated approach. It starts with a multi-head layer and then internally splits this layer into individual attention heads, as illustrated in Figure 3.26.\n",
    "\n",
    "Figure 3.26 In the MultiheadAttentionWrapper class with two attention heads, we initialized two weight matrices Wq1 and Wq2 and computed two query matrices Q1 and Q2 as illustrated at the top of this figure. In the MultiheadAttention class, we initialize one larger weight matrix Wq , only perform one matrix multiplication with the inputs to obtain a query matrix Q, and then split the query matrix into Q1 and Q2 as shown at the bottom of this figure. We do the same for the keys and values, which are not shown to reduce visual clutter.\n",
    "\n",
    "\n",
    "\n",
    "The splitting of the query, key, and value tensors, as depicted in Figure 3.26, is achieved through tensor reshaping and transposing operations using PyTorch's .view and .transpose methods. The input is first transformed (via linear layers for queries, keys, and values) and then reshaped to represent multiple heads.\n",
    "\n",
    "The key operation is to split the d_out dimension into num_heads and head_dim, where head_dim = d_out / num_heads. This splitting is then achieved using the .view method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens, num_heads, head_dim).\n",
    "\n",
    "The tensors are then transposed to bring the num_heads dimension before the num_tokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim). This transposition is crucial for correctly aligning the queries, keys, and values across the different heads and performing batched matrix multiplications efficiently.\n",
    "\n",
    "To illustrate this batched matrix multiplication, suppose we have the following example tensor:\n",
    "\n",
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    " \n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "Now, we perform a batched matrix multiplication between the tensor itself and a view of the tensor where we transposed the last two dimensions, num_tokens and head_dim:\n",
    "\n",
    "print(a @ a.transpose(2, 3))\n",
    "\n",
    "The result is as follows:\n",
    "\n",
    "tensor([[[[1.3208, 1.1631, 1.2879],\n",
    "          [1.1631, 2.2150, 1.8424],\n",
    "          [1.2879, 1.8424, 2.0402]],\n",
    " \n",
    "         [[0.4391, 0.7003, 0.5903],\n",
    "          [0.7003, 1.3737, 1.0620],\n",
    "          [0.5903, 1.0620, 0.9912]]]])\n",
    "\n",
    "\n",
    "\n",
    "In this case, the matrix multiplication implementation in PyTorch handles the 4-dimensional input tensor so that the matrix multiplication is carried out between the 2 last dimensions (num_tokens, head_dim) and then repeated for the individual heads.\n",
    "\n",
    "For instance, the above becomes a more compact way to compute the matrix multiplication for each head separately:\n",
    "\n",
    "\n",
    "\n",
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    " \n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)\n",
    "\n",
    "\n",
    "The results are exactly the same results that we obtained when using the batched matrix multiplication print(a @ a.transpose(2, 3)) earlier:\n",
    "\n",
    "First head:\n",
    " tensor([[1.3208, 1.1631, 1.2879],\n",
    "        [1.1631, 2.2150, 1.8424],\n",
    "        [1.2879, 1.8424, 2.0402]])\n",
    " \n",
    "Second head:\n",
    " tensor([[0.4391, 0.7003, 0.5903],\n",
    "        [0.7003, 1.3737, 1.0620],\n",
    "        [0.5903, 1.0620, 0.9912]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Continuing with MultiHeadAttention, after computing the attention weights and context vectors, the context vectors from all heads are transposed back to the shape (b, num_tokens, num_heads, head_dim). These vectors are then reshaped (flattened) into the shape (b, num_tokens, d_out), effectively combining the outputs from all heads.\n",
    "\n",
    "Additionally, we added a so-called output projection layer (self.out_proj) to MultiHeadAttention after combining the heads, which is not present in the CausalAttention class. This output projection layer is not strictly necessary (see the References section in Appendix B for more details), but it is commonly used in many LLM architectures, which is why we added it here for completeness.\n",
    "\n",
    "Even though the MultiHeadAttention class looks more complicated than the MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors, it is more efficient. The reason is that we only need one matrix multiplication to compute the keys, for instance, keys = self.W_key(x) (the same is true for the queries and values). In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication, which is computationally one of the most expensive steps, for each attention head.\n",
    "\n",
    "The MultiHeadAttention class can be used similar to the SelfAttention and CausalAttention classes we implemented earlier:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "\n",
    "s we can see based on the results, the output dimension is directly controlled by the d_out argument:\n",
    "\n",
    "tensor([[[0.3190, 0.4858],\n",
    "         [0.2943, 0.3897],\n",
    "         [0.2856, 0.3593],\n",
    "         [0.2693, 0.3873],\n",
    "         [0.2639, 0.3928],\n",
    "         [0.2575, 0.4028]],\n",
    " \n",
    "        [[0.3190, 0.4858],\n",
    "         [0.2943, 0.3897],\n",
    "         [0.2856, 0.3593],\n",
    "         [0.2693, 0.3873],\n",
    "         [0.2639, 0.3928],\n",
    "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
    "context_vecs.shape: torch.Size([2, 6, 2])\n",
    "\n",
    "\n",
    "\n",
    "In this section, we implemented the MultiHeadAttention class that we will use in the upcoming sections when implementing and training the LLM itself. Note that while the code is fully functional, we used relatively small embedding sizes and numbers of attention heads to keep the outputs readable.\n",
    "\n",
    "For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention heads and a context vector embedding size of 768. The largest GPT-2 model (1.5 billion parameters) has 25 attention heads and a context vector embedding size of 1600. Note that the embedding sizes of the token inputs and context embeddings are the same in GPT models (d_in = d_out).\n",
    "Exercise 3.3 Initializing GPT-2 size attention modules\n",
    "\n",
    "Using the MultiHeadAttention class, initialize a multi-head attention module that has the same number of attention heads as the smallest GPT-2 model (12 attention heads). Also ensure that you use the respective input and output embedding sizes similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a context length of 1024 tokens.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.7 Summary\n",
    "\n",
    "    Attention mechanisms transform input elements into enhanced context vector representations that incorporate information about all inputs.\n",
    "    A self-attention mechanism computes the context vector representation as a weighted sum over the inputs.\n",
    "    In a simplified attention mechanism, the attention weights are computed via dot products.\n",
    "    A dot product is just a concise way of multiplying two vectors element-wise and then summing the products.\n",
    "    Matrix multiplications, while not strictly required, help us to implement computations more efficiently and compactly by replacing nested for-loops.\n",
    "    In self-attention mechanisms that are used in LLMs, also called scaled-dot product attention, we include trainable weight matrices to compute intermediate transformations of the inputs: queries, values, and keys.\n",
    "    When working with LLMs that read and generate text from left to right, we add a causal attention mask to prevent the LLM from accessing future tokens.\n",
    "    Next to causal attention masks to zero out attention weights, we can also add a dropout mask to reduce overfitting in LLMs.\n",
    "    The attention modules in transformer-based LLMs involve multiple instances of causal attention, which is called multi-head attention.\n",
    "    We can create a multi-head attention module by stacking multiple instances of causal attention modules.\n",
    "    A more efficient way of creating multi-head attention modules involves batched matrix multiplications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
