{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Механизм внимания\n",
    "\n",
    "![](images/llm3.2.png)\n",
    "\n",
    "В предыдущей главе мы готовили человеческий текст к восприятию машиной.  \n",
    "Теперь рассмотрим механику больших языковых моделей, а именно четыре варианта механизма внимания:\n",
    "\n",
    "![](images/llm3.3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Проблема длины контекста\n",
    "\n",
    "Что такое перевод? - Это сопоставления токенов из разных словарей (с позиции этой книги).\n",
    "\n",
    "А т.к. у различных языков различные грамматики, то минимальный смысловой токен - предложение (рисунок). Т.е. необходим учет контекста, причем чем он длиннее - тем лучше, но сложнее вычислить - проблема длины контекста. \n",
    "\n",
    "![](images/llm3.4.png)\n",
    "\n",
    "Решением этой проблемы были рекурентные нейронные сети (RNN). В этих сетях результаты работы одного блока, идет на вход следующего, откуда и берется название (рекурентный член последовательности вычисляется на основе предыдущего): \n",
    "1. Входной текст подается в шифратор \n",
    "2. Шифратор обновляет значения слоев (состояния скрытых слоев, hidden states), так, что в последнем слое зашифрован контекст всего входящего текста.\n",
    "3. Дешифратор получает этот последний слой, чтобы генерировать перевод предложения (уже не рекурентно, опираясь каждый раз на слой-контекст, который не меняется).\n",
    "\n",
    "![](images/llm3.5.png)\n",
    "\n",
    "Т.е. нейронная сеть, построенная на рекурентном механизме способна работать с контекстом как векторным представлением, но без динамики - дешифратору доступно только последнее состояние контекста. Таким образом, по построению, возникает ограничение на длину и сложность входного текста. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Учет контекста с помощью механизмов внимания\n",
    "\n",
    "В 2014 году был предложен так называемый механизм внимания Багданау (Bahdanau attention mechanizm), который модифицирует RNN таким образом, что декодировщик может выборочно получать доступ к различным частям входной последовательности при каждом декодировании (рисунок). Так RNN может выборочно обращаться ко всем входным токенам. Это означает, что некоторые входные токены более важны, чем другие, для создания данного выходного токена. Важность определяется так называемыми весами внимания.\n",
    "\n",
    "![](images/llm3.6.png)\n",
    "\n",
    "На основе этого подхода был разработан механизм собственного внимания (self-attention). Этот механизм позоляет учесть все позиционные отношения при обработке входящей последовательности. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Внимание относительно входных элементов\n",
    "\n",
    "Целью собственного внимания является вычисление векторного представления контекста, объединяющего контекст всех других входных элементов, по отношению к выбранному. На рисунке вектор контектса z2 вычисляет контекст относительно входящего x2.\n",
    "\n",
    "![](images/llm3.8.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Рассмотрим рисунок поодробнее. Есть входящая последовательность токенов - текст «Ваше путешествие начинается с первого шага».\n",
    "Каждый элемент последовательности, например x_i, представлен как 3-мерный вектор.\n",
    "\n",
    "Наша цель — вычислить векторы контекста z^i относительно каждого элемента x_i входящей последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сперва вычислим промежуточные значения ω, называемые оценками внимания (attention score) с помощью скалярного произведения. \n",
    "\n",
    "![](images/llm3.9.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скалярное произведение определим как сумму поэлементного умножения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "res = 0\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Физический смысл\" скалярного произведения - определение степени \"перекрытия\" векторов. Так, ортогональные вектора (1, 0) и (0, 1) вообще не \"перекрываются\" и дадут 0. Т.е. чем больше значение скалярное произведение - тем больше их \"перекрытие\" и оценка внимания. После вычисления, оценки нормализуются и далее называются весами (weights). Использование нормализации весов - соглашение в индустрии для универсальности интерпретации.  \n",
    "\n",
    "![](images/llm3.10.png) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведены два варианта нормализации. Нормализация с помощью softmax считается предпочтительнее, т.к. гарантирует, что веса всегда будут положительными. Это позволяет интерпретировать выходные данные как вероятности или относительную важность (больший вес - большая важность)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    " \n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы к вычислению векторного представления контекста - просуммируем взвешеннные векторные представления токенов.\n",
    "\n",
    "![](images/llm3.11.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = inputs[1] # 2nd input token is the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Вычисление весов внимания относительно входных элементов\n",
    "\n",
    "Результаты предыдущего шага: \n",
    "![](images/llm3.12.png) \n",
    "\n",
    "1. вычисления скалярного произведения для всех пар входных данных.\n",
    "![](images/llm3.13.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# шаг 1 - вычисление оценок внимания\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Циклы for обычно работают медленно, вычисление будет оптимальнее с использованием умножения матриц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 2, as illustrated in Figure 3.12, we now normalize each row so that the values in each row sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# шаг 2 - нормализация \n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тест нормализации\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На третьем и последнем шаге мы теперь используем эти веса внимания для вычисления всех векторов контекста посредством умножения матриц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# шаг 3 - скалярное произведение векторов с помощью матричного умножения\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения вектора весов контекста z_2, полученные сейчас и в параграфе 3.3.1 совпали.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.4 Implementing self-attention with trainable weights\n",
    "\n",
    "In this section, we are implementing the self-attention mechanism that is used in the original transformer architecture, the GPT models, and most other popular LLMs. This self-attention mechanism is also called scaled dot-product attention. Figure 3.13 provides a mental model illustrating how this self-attention mechanism fits into the broader context of implementing an LLM.\n",
    "\n",
    "Figure 3.13 A mental model illustrating how the self-attention mechanism we code in this section fits into the broader context of this book and chapter. In the previous section, we coded a simplified attention mechanism to understand the basic mechanism behind attention mechanisms. In this section, we add trainable weights to this attention mechanism. In the upcoming sections, we will then extend this self-attention mechanism by adding a causal mask and multiple heads.\n",
    "\n",
    "\n",
    "\n",
    "As illustrated in Figure 3.13 the self-attention mechanism with trainable weights builds on the previous concepts: we want to compute context vectors as weighted sums over the input vectors specific to a certain input element. As you will see, there are only slight differences compared to the basic self-attention mechanism we coded earlier in section 3.3.\n",
    "\n",
    "The most notable difference is the introduction of weight matrices that are updated during model training. These trainable weight matrices are crucial so that the model (specifically, the attention module inside the model) can learn to produce \"good\" context vectors. (Note that we will train the LLM in chapter 5.)\n",
    "\n",
    "We will tackle this self-attention mechanism in the two subsections. First, we will code it step-by-step as before. Second, we will organize the code into a compact Python class that can be imported into an LLM architecture, which we will code in chapter 4.\n",
    "3.4.1 Computing the attention weights step by step\n",
    "\n",
    "We will implement the self-attention mechanism step by step by introducing the three trainable weight matrices Wq, Wk, and Wv. These three matrices are used to project the embedded input tokens, x(i), into query, key, and value vectors as illustrated in Figure 3.14.\n",
    "\n",
    "\n",
    "Figure 3.14 In the first step of the self-attention mechanism with trainable weight matrices, we compute query (q), key (k), and value (v) vectors for input elements x. Similar to previous sections, we designate the second input, x(2), as the query input. The query vector q(2) is obtained via matrix multiplication between the input x(2) and the weight matrix Wq. Similarly, we obtain the key and value vectors via matrix multiplication involving the weight matrices Wk and Wv.\n",
    "\n",
    "\n",
    "\n",
    "Earlier in section 3.3.1, we defined the second input element x(2) as the query when we computed the simplified attention weights to compute the context vector z(2). Later, in section 3.3.2, we generalized this to compute all context vectors z(1) ... z(T) for the six-word input sentence \"Your journey starts with one step.\"\n",
    "\n",
    "Similarly, we will start by computing only one context vector, z(2), for illustration purposes. In the next section, we will modify this code to calculate all context vectors.\n",
    "\n",
    "Let's begin by defining a few variables:\n",
    "\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "\n",
    "\n",
    "Note that in GPT-like models, the input and output dimensions are usually the same, but for illustration purposes, to better follow the computation, we choose different input (d_in=3) and output (d_out=2) dimensions here.\n",
    "\n",
    "Next, we initialize the three weight matrices Wq, Wk, and Wv that are shown in Figure 3.14:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "\n",
    "\n",
    "Note that we are setting requires_grad=False to reduce clutter in the outputs for illustration purposes, but if we were to use the weight matrices for model training, we would set requires_grad=True to update these matrices during model training.\n",
    "\n",
    "Next, we compute the query, key, and value vectors as shown earlier in Figure 3.14:\n",
    "\n",
    "query_2 = x_2 @ W_query \n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "\n",
    "As we can see based on the output for the query, this results in a 2-dimensional vector since we set the number of columns of the corresponding weight matrix, via d_out, to 2:\n",
    "\n",
    "tensor([0.4306, 1.4551])\n",
    "\n",
    "\n",
    "Weight parameters vs attention weights\n",
    "\n",
    "Note that in the weight matrices W, the term \"weight\" is short for \"weight parameters,\" the values of a neural network that are optimized during training. This is not to be confused with the attention weights. As we already saw in the previous section, attention weights determine the extent to which a context vector depends on the different parts of the input, i.e., to what extent the network focuses on different parts of the input.\n",
    "\n",
    "In summary, weight parameters are the fundamental, learned coefficients that define the network's connections, while attention weights are dynamic, context-specific values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Even though our temporary goal is to only compute the one context vector, z(2), we still require the key and value vectors for all input elements as they are involved in computing the attention weights with respect to the query q(2), as illustrated in Figure 3.14.\n",
    "\n",
    "We can obtain all keys and values via matrix multiplication:\n",
    "\n",
    "\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "As we can tell from the outputs, we successfully projected the 6 input tokens from a 3D onto a 2D embedding space:\n",
    "\n",
    "keys.shape: torch.Size([6, 2])\n",
    "values.shape: torch.Size([6, 2])\n",
    "\n",
    "The second step is now to compute the attention scores, as shown in Figure 3.15.\n",
    "\n",
    "Figure 3.15 The attention score computation is a dot-product computation similar to what we have used in the simplified self-attention mechanism in section 3.3. The new aspect here is that we are not directly computing the dot-product between the input elements but using the query and key obtained by transforming the inputs via the respective weight matrices.\n",
    "\n",
    "First, let's compute the attention score ω22:\n",
    "\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n",
    "\n",
    "The results in the following unnormalized attention score:\n",
    "\n",
    "tensor(1.8524)\n",
    "\n",
    "Again, we can generalize this computation to all attention scores via matrix multiplication:\n",
    "\n",
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)\n",
    "\n",
    "As we can see, as a quick check, the second element in the output matches attn_score_22 we computed previously:\n",
    "\n",
    "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
    "\n",
    "The third step is now going from the attention scores to the attention weights, as illustrated in Figure 3.16.\n",
    "\n",
    "Figure 3.16 After computing the attention scores ω, the next step is to normalize these scores using the softmax function to obtain the attention weights α.\n",
    "\n",
    "Next, as illustrated in Figure 3.16, we compute the attention weights by scaling the attention scores and using the softmax function we used earlier.. The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension of the keys, (note that taking the square root is mathematically the same as exponentiating by 0.5):\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "\n",
    "The resulting attention weights are as follows:\n",
    "\n",
    "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
    "\n",
    "\n",
    "The rationale behind scaled-dot product attention\n",
    "\n",
    "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than thousand for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "\n",
    "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention.\n",
    "\n",
    "Now, the final step is to compute the context vectors, as illustrated in Figure 3.17.\n",
    "\n",
    "Figure 3.17 In the final step of the self-attention computation, we compute the context vector by combining all value vectors via the attention weights.\n",
    "\n",
    "Similar to section 3.3, where we computed the context vector as a weighted sum over the input vectors, we now compute the context vector as a weighted sum over the value vectors. Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector. Similar to section 3.3, we can use matrix multiplication to obtain the output in one step:\n",
    "\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)\n",
    "\n",
    "The contents of the resulting vector are as follows:\n",
    "\n",
    "tensor([0.3061, 0.8210])\n",
    "\n",
    "So far, we only computed a single context vector, z(2). In the next section, we will generalize the code to compute all context vectors in the input sequence, z(1) to z(T).\n",
    "\n",
    "\n",
    "Why query, key, and value?\n",
    "\n",
    "The terms \"key,\" \"query,\" and \"value\" in the context of attention mechanisms are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search, and retrieve information.\n",
    "\n",
    "A \"query\" is analogous to a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
    "\n",
    "The \"key\" is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match with the query.\n",
    "\n",
    "The \"value\" in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values.\n",
    "\n",
    "\n",
    "3.4.2 Implementing a compact self-attention Python class\n",
    "\n",
    "In the previous sections, we have gone through a lot of steps to compute the self-attention outputs. This was mainly done for illustration purposes so we could go through one step at a time. In practice, with the LLM implementation in the next chapter in mind, it is helpful to organize this code into a Python class as follows:\n",
    "\n",
    "Listing 3.1 A compact self-attention class\n",
    "\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    " \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch models, which provides necessary functionalities for model layer creation and management.\n",
    "\n",
    "The __init__ method initializes trainable weight matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n",
    "\n",
    "During the forward pass, using the forward method, we compute the attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax. Finally, we create a context vector by weighting the values with these normalized attention scores.\n",
    "\n",
    "We can use this class as follows:\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))\n",
    "\n",
    "Since inputs contains six embedding vectors, this result in a matrix storing the six context vectors:\n",
    "\n",
    "tensor([[0.2996, 0.8053],\n",
    "        [0.3061, 0.8210],\n",
    "        [0.3058, 0.8203],\n",
    "        [0.2948, 0.7939],\n",
    "        [0.2927, 0.7891],\n",
    "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
    "\n",
    "\n",
    "\n",
    "As a quick check, notice how the second row ([0.3061, 0.8210]) matches the contents of context_vec_2 in the previous section.\n",
    "\n",
    "Figure 3.18 summarizes the self-attention mechanism we just implemented.\n",
    "\n",
    "Figure 3.18 In self-attention, we transform the input vectors in the input matrix X with the three weight matrices, Wq, Wk, and Wv. Then, we compute the attention weight matrix based on the resulting queries (Q) and keys (K). Using the attention weights and values (V), we then compute the context vectors (Z). (For visual clarity, we focus on a single input text with n tokens in this figure, not a batch of multiple inputs. Consequently, the 3D input tensor is simplified to a 2D matrix in this context. This approach allows for a more straightforward visualization and understanding of the processes involved.)\n",
    "\n",
    "\n",
    "\n",
    "As shown in Figure 3.18, self-attention involves the trainable weight matrices Wq, Wk, and Wv. These matrices transform input data into queries, keys, and values, which are crucial components of the attention mechanism. As the model is exposed to more data during training, it adjusts these trainable weights, as we will see in upcoming chapters.\n",
    "\n",
    "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's nn.Linear layers, which effectively perform matrix multiplication when the bias units are disabled. Additionally, a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training.\n",
    "\n",
    "Listing 3.2 A self-attention class using PyTorch's Linear layers\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    " \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "You can use the SelfAttention_v2 similar to SelfAttention_v1:\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n",
    "\n",
    "The output is:\n",
    "\n",
    "tensor([[-0.0739,  0.0713],\n",
    "        [-0.0748,  0.0703],\n",
    "        [-0.0749,  0.0702],\n",
    "        [-0.0760,  0.0685],\n",
    "        [-0.0763,  0.0679],\n",
    "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n",
    "\n",
    "Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they use different initial weights for the weight matrices since nn.Linear uses a more sophisticated weight initialization scheme.\n",
    "\n",
    "\n",
    "\n",
    "Exercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2\n",
    "\n",
    "Note that nn.Linear in SelfAttention_v2 uses a different weight initialization scheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1, which causes both mechanisms to produce different results. To check that both implementations, SelfAttention_v1 and SelfAttention_v2, are otherwise similar, we can transfer the weight matrices from a SelfAttention_v2 object to a SelfAttention_v1, such that both objects then produce the same results.\n",
    "\n",
    "Your task is to correctly assign the weights from an instance of SelfAttention_v2 to an instance of SelfAttention_v1. To do this, you need to understand the relationship between the weights in both versions. (Hint: nn.Linear stores the weight matrix in a transposed form.) After the assignment, you should observe that both instances produce the same outputs.\n",
    "\n",
    "\n",
    "\n",
    "In the next section, we will make enhancements to the self-attention mechanism, focusing specifically on incorporating causal and multi-head elements. The causal aspect involves modifying the attention mechanism to prevent the model from accessing future information in the sequence, which is crucial for tasks like language modeling, where each word prediction should only depend on previous words.\n",
    "\n",
    "The multi-head component involves splitting the attention mechanism into multiple \"heads.\" Each head learns different aspects of the data, allowing the model to simultaneously attend to information from different representation subspaces at different positions. This improves the model's performance in complex tasks.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.5 Hiding future words with causal attention\n",
    "\n",
    "In this section, we modify the standard self-attention mechanism to create a causal attention mechanism, which is essential for developing an LLM in the subsequent chapters.\n",
    "\n",
    "Causal attention, also known as masked attention, is a specialized form of self-attention. It restricts a model to only consider previous and current inputs in a sequence when processing any given token. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.\n",
    "\n",
    "Consequently, when computing attention scores, the causal attention mechanism ensures that the model only factors in tokens that occur at or before the current token in the sequence.\n",
    "\n",
    "To achieve this in GPT-like LLMs, for each token processed, we mask out the future tokens, which come after the current token in the input text, as illustrated in Figure 3.19.\n",
    "\n",
    "Figure 3.19 In causal attention, we mask out the attention weights above the diagonal such that for a given input, the LLM can't access future tokens when computing the context vectors using the attention weights. For example, for the word \"journey\" in the second row, we only keep the attention weights for the words before (\"Your\") and in the current position (\"journey\").\n",
    "\n",
    "\n",
    "\n",
    "As illustrated in Figure 3.19, we mask out the attention weights above the diagonal, and we normalize the non-masked attention weights, such that the attention weights sum to 1 in each row. In the next section, we will implement this masking and normalization procedure in code.\n",
    "3.5.1 Applying a causal attention mask\n",
    "\n",
    "In this section, we implement the causal attention mask in code. We start with the procedure summarized in Figure 3.20.\n",
    "\n",
    "Figure 3.20 One way to obtain the masked attention weight matrix in causal attention is to apply the softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing the resulting matrix.\n",
    "\n",
    "\n",
    "\n",
    "To implement the steps to apply a causal attention mask to obtain the masked attention weights as summarized in Figure 3.20, let's work with the attention scores and weights from the previous section to code the causal attention mechanism.\n",
    "\n",
    "In the first step illustrated in Figure 3.20, we compute the attention weights using the softmax function as we have done in previous sections:\n",
    "\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)\n",
    "\n",
    "This results in the following attention weights:\n",
    "\n",
    "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
    "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
    "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
    "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
    "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<SoftmaxBackward0>)\n",
    "\n",
    "We can implement step 2 in Figure 3.20 using PyTorch's tril function to create a mask where the values above the diagonal are zero:\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n",
    "\n",
    "The resulting mask is as follows:\n",
    "\n",
    "\n",
    "tensor([[1., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 0.],\n",
    "        [1., 1., 1., 1., 1., 1.]])\n",
    "\n",
    "Now, we can multiply this mask with the attention weights to zero out the values above the diagonal:\n",
    "\n",
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)\n",
    "\n",
    "As we can see, the elements above the diagonal are successfully zeroed out:\n",
    "\n",
    "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
    "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
    "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<MulBackward0>)\n",
    " \n",
    "\n",
    " The third step in Figure 3.20 is to renormalize the attention weights to sum up to 1 again in each row. We can achieve this by dividing each element in each row by the sum in each row:\n",
    "\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)\n",
    "\n",
    "\n",
    "\n",
    "The result is an attention weight matrix where the attention weights above the diagonal are zeroed out and where the rows sum to 1:\n",
    "\n",
    "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
    "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<DivBackward0>)\n",
    "\n",
    "\n",
    "Information leakage\n",
    "\n",
    "When we apply a mask and then renormalize the attention weights, it might initially appear that information from future tokens (which we intend to mask) could still influence the current token because their values are part of the softmax calculation. However, the key insight is that when we renormalize the attention weights after masking, what we're essentially doing is recalculating the softmax over a smaller subset (since masked positions don't contribute to the softmax value).\n",
    "\n",
    "The mathematical elegance of softmax is that despite initially including all positions in the denominator, after masking and renormalizing, the effect of the masked positions is nullified — they don't contribute to the softmax score in any meaningful way.\n",
    "\n",
    "In simpler terms, after masking and renormalization, the distribution of attention weights is as if it was calculated only among the unmasked positions to begin with. This ensures there's no information leakage from future (or otherwise masked) tokens as we intended.\n",
    "\n",
    "\n",
    "While we could be technically done with implementing causal attention at this point, we can take advantage of a mathematical property of the softmax function and implement the computation of the masked attention weights more efficiently in fewer steps, as shown in Figure 3.21.\n",
    "\n",
    "Figure 3.21 A more efficient way to obtain the masked attention weight matrix in causal attention is to mask the attention scores with negative infinity values before applying the softmax function.\n",
    "\n",
    "\n",
    "\n",
    "The softmax function converts its inputs into a probability distribution. When negative infinity values (-∞) are present in a row, the softmax function treats them as zero probability. (Mathematically, this is because e-∞ approaches 0.)\n",
    "\n",
    "We can implement this more efficient masking \"trick\" by creating a mask with 1's above the diagonal and then replacing these 1's with negative infinity (-inf) values:\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)\n",
    "\n",
    "This results in the following mask:\n",
    "\n",
    "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
    "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
    "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
    "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
    "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
    "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
    "       grad_fn=<MaskedFillBackward0>)\n",
    " \n",
    "Now, all we need to do is apply the softmax function to these masked results, and we are done:\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)\n",
    "\n",
    "As we can see based on the output, the values in each row sum to 1, and no further normalization is necessary:\n",
    "\n",
    "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
    "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "       grad_fn=<SoftmaxBackward0>)\n",
    "\n",
    "We could now use the modified attention weights to compute the context vectors via context_vec = attn_weights @ values, as in section 3.4. However, in the next section, we first cover another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs.\n",
    "\n",
    "\n",
    "\n",
    "3.5.2 Masking additional attention weights with dropout\n",
    "\n",
    "Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively \"dropping\" them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It's important to emphasize that dropout is only used during training and is disabled afterward.\n",
    "\n",
    "In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied in two specific areas: after calculating the attention scores or after applying the attention weights to the value vectors.\n",
    "\n",
    "Here, we will apply the dropout mask after computing the attention weights, as illustrated in Figure 3.22, because it's the more common variant in practice.\n",
    "\n",
    "Figure 3.22 Using the causal attention mask (upper left), we apply an additional dropout mask (upper right) to zero out additional attention weights to reduce overfitting during training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the following code example, we use a dropout rate of 50%, which means masking out half of the attention weights. (When we train the GPT model in later chapters, we will use a lower dropout rate, such as 0.1 or 0.2.)\n",
    "\n",
    "In the following code, we apply PyTorch's dropout implementation first to a 6×6 tensor consisting of ones for illustration purposes:\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))\n",
    "\n",
    "s we can see, approximately half of the values are zeroed out:\n",
    "\n",
    "tensor([[2., 2., 0., 2., 2., 0.],\n",
    "        [0., 0., 0., 2., 0., 2.],\n",
    "        [2., 2., 2., 2., 0., 2.],\n",
    "        [0., 2., 2., 0., 0., 2.],\n",
    "        [0., 2., 0., 2., 0., 2.],\n",
    "        [0., 2., 2., 2., 2., 0.]])\n",
    "\n",
    "\n",
    "\n",
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 =2. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases.\n",
    "\n",
    "Now, let's apply dropout to the attention weight matrix itself:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))\n",
    "\n",
    "The resulting attention weight matrix now has additional elements zeroed out and the remaining ones rescaled:\n",
    "\n",
    "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
    "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
    "       grad_fn=<MulBackward0>\n",
    "\n",
    "\n",
    "\n",
    "Note that the resulting dropout outputs may look different depending on your operating system; you can read more about this inconsistency [here on the PyTorch issue tracker at https://github.com/pytorch/pytorch/issues/121595.\n",
    "\n",
    "Having gained an understanding of causal attention and dropout masking, we will develop a concise Python class in the following section. This class is designed to facilitate the efficient application of these two techniques.\n",
    "3.5.3 Implementing a compact causal attention class\n",
    "\n",
    "In this section, we will now incorporate the causal attention and dropout modifications into the SelfAttention Python class we developed in section 3.4. This class will then serve as a template for developing multi-head attention in the upcoming section, which is the final attention class we implement in this chapter.\n",
    "\n",
    "But before we begin, one more thing is to ensure that the code can handle batches consisting of more than one input so that the CausalAttention class supports the batch outputs produced by the data loader we implemented in chapter 2.\n",
    "\n",
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)\n",
    "\n",
    "This results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token is a 3-dimensional embedding vector:\n",
    "\n",
    "torch.Size([2, 6, 3])\n",
    "\n",
    "\n",
    "\n",
    "The following CausalAttention class is similar to the SelfAttention class we implemented earlier, except that we now added the dropout and causal mask components as highlighted in the following code:\n",
    "\n",
    "Listing 3.3 A compact causal attention class\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "           'mask',\n",
    "           torch.triu(torch.ones(context_length, context_length),\n",
    "           diagonal=1)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    " \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    " \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "\n",
    "While all added code lines should be familiar from previous sections, we now added a self.register_buffer() call in the __init__ method. The use of register_buffer in PyTorch is not strictly necessary for all use cases but offers several advantages here. For instance, when we use the CausalAttention class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will be relevant when training the LLM in future chapters. This means we don't need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors.\n",
    "\n",
    "We can use the CausalAttention class as follows, similar to SelfAttention previously:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "The resulting context vector is a 3D tensor where each token is now represented by a 2D embedding:\n",
    "\n",
    "\n",
    "context_vecs.shape: torch.Size([2, 6, 2])\n",
    "\n",
    "Figure 3.23 provides a mental model that summarizes what we have accomplished so far.\n",
    "\n",
    "\n",
    "Figure 3.23 A mental model summarizing the four different attention modules we are coding in this chapter. We began with a simplified attention mechanism, added trainable weights, and then added a casual attention mask. In the remainder of this chapter, we will extend the causal attention mechanism and code multi-head attention, which is the final module we will use in the LLM implementation in the next chapter.\n",
    "\n",
    "As illustrated in Figure 3.23, in this section, we focused on the concept and implementation of causal attention in neural networks. In the next section, we will expand on this concept and implement a multi-head attention module that implements several of such causal attention mechanisms in parallel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.6 Extending single-head attention to multi-head attention\n",
    "\n",
    "In this final section of this chapter, we are extending the previously implemented causal attention class over multiple-heads. This is also called multi-head attention.\n",
    "\n",
    "The term \"multi-head\" refers to dividing the attention mechanism into multiple \"heads,\" each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.\n",
    "\n",
    "In the following subsections, we will tackle this expansion from causal attention to multi-head attention. The first subsection will intuitively build a multi-head attention module by stacking multiple CausalAttention modules for illustration purposes. The second subsection will then implement the same multi-head attention module in a more complicated but computationally more efficient way.\n",
    "3.6.1 Stacking multiple single-head attention layers\n",
    "\n",
    "In practical terms, implementing multi-head attention involves creating multiple instances of the self-attention mechanism (depicted earlier in Figure 3.18 in section 3.4.1), each with its own weights, and then combining their outputs. Using multiple instances of the self-attention mechanism can be computationally intensive, but it's crucial for the kind of complex pattern recognition that models like transformer-based LLMs are known for.\n",
    "\n",
    "Figure 3.24 illustrates the structure of a multi-head attention module, which consists of multiple single-head attention modules, as previously depicted in Figure 3.18, stacked on top of each other.\n",
    "\n",
    "Figure 3.24 The multi-head attention module in this figure depicts two single-head attention modules stacked on top of each other. So, instead of using a single matrix Wv for computing the value matrices, in a multi-head attention module with two heads, we now have two value weight matrices: Wv1 and Wv2. The same applies to the other weight matrices, Wq and Wk. We obtain two sets of context vectors Z1 and Z2 that we can combine into a single context vector matrix Z.\n",
    "\n",
    "\n",
    "\n",
    "As mentioned before, the main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections -- the results of multiplying the input data (like the query, key, and value vectors in attention mechanisms) by a weight matrix.\n",
    "\n",
    "In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper class that stacks multiple instances of our previously implemented CausalAttention module:\n",
    "\n",
    "Listing 3.4 A wrapper class to implement multi-head attention\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via num_heads=2) and CausalAttention output dimension d_out=2, this results in a 4-dimensional context vectors (d_out*num_heads=4), as illustrated in Figure 3.25.\n",
    "\n",
    "Figure 3.25 Using the MultiHeadAttentionWrapper, we specified the number of attention heads (num_heads). If we set num_heads=2, as shown in this figure, we obtain a tensor with two sets of context vector matrices. In each context vector matrix, the rows represent the context vectors corresponding to the tokens, and the columns correspond to the embedding dimension specified via d_out=4. We concatenate these context vector matrices along the column dimension. Since we have 2 attention heads and an embedding dimension of 2, the final embedding dimension is 2 × 2 = 4.\n",
    "\n",
    "To illustrate Figure 3.25 further with a concrete example, we can use the MultiHeadAttentionWrapper class similar to the CausalAttention class before:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    " \n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "This results in the following tensor representing the context vectors:\n",
    "\n",
    "\n",
    "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
    "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
    "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
    "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
    "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
    "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
    " \n",
    "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
    "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
    "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
    "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
    "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
    "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
    "context_vecs.shape: torch.Size([2, 6, 4])\n",
    "\n",
    "\n",
    "\n",
    "The first dimension of the resulting context_vecs tensor is 2 since we have two input texts (the input texts are duplicated, which is why the context vectors are exactly the same for those). The second dimension refers to the 6 tokens in each input. The third dimension refers to the 4-dimensional embedding of each token.\n",
    "Exercise 3.2 Returning 2-dimensional embedding vectors\n",
    "\n",
    "Change the input arguments for the MultiHeadAttentionWrapper(..., num_heads=2) call such that the output context vectors are 2-dimensional instead of 4-dimensional while keeping the setting num_heads=2. Hint: You don't have to modify the class implementation; you just have to change one of the other input arguments.\n",
    "\n",
    "In this section, we implemented a MultiHeadAttentionWrapper that combined multiple single-head attention modules. However, note that these are processed sequentially via [head(x) for head in self.heads] in the forward method. We can improve this implementation by processing the heads in parallel. One way to achieve this is by computing the outputs for all attention heads simultaneously via matrix multiplication, as we will explore in the next section.\n",
    "3.6.2 Implementing multi-head attention with weight splits\n",
    "\n",
    "In the previous section, we created a MultiHeadAttentionWrapper to implement multi-head attention by stacking multiple single-head attention modules. This was done by instantiating and combining several CausalAttention objects.\n",
    "\n",
    "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and CausalAttention, we can combine both of these concepts into a single MultiHeadAttention class. Also, in addition to just merging the MultiHeadAttentionWrapper with the CausalAttention code, we will make some other modifications to implement multi-head attention more efficiently.\n",
    "\n",
    "In the MultiHeadAttentionWrapper, multiple heads are implemented by creating a list of CausalAttention objects (self.heads), each representing a separate attention head. The CausalAttention class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following MultiHeadAttention class integrates the multi-head functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention.\n",
    "\n",
    "Let's take a look at the MultiHeadAttention class before we discuss it further:\n",
    "\n",
    "Listing 3.5 An efficient multi-head attention class\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    " \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "             torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    " \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    " \n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    " \n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "  \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    " \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    " \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "\n",
    "Even though the reshaping (.view) and transposing (.transpose) of tensors inside the MultiHeadAttention class looks very complicated, mathematically, the MultiHeadAttention class implements the same concept as the MultiHeadAttentionWrapper earlier.\n",
    "\n",
    "On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked multiple single-head attention layers that we combined into a multi-head attention layer. The MultiHeadAttention class takes an integrated approach. It starts with a multi-head layer and then internally splits this layer into individual attention heads, as illustrated in Figure 3.26.\n",
    "\n",
    "Figure 3.26 In the MultiheadAttentionWrapper class with two attention heads, we initialized two weight matrices Wq1 and Wq2 and computed two query matrices Q1 and Q2 as illustrated at the top of this figure. In the MultiheadAttention class, we initialize one larger weight matrix Wq , only perform one matrix multiplication with the inputs to obtain a query matrix Q, and then split the query matrix into Q1 and Q2 as shown at the bottom of this figure. We do the same for the keys and values, which are not shown to reduce visual clutter.\n",
    "\n",
    "\n",
    "\n",
    "The splitting of the query, key, and value tensors, as depicted in Figure 3.26, is achieved through tensor reshaping and transposing operations using PyTorch's .view and .transpose methods. The input is first transformed (via linear layers for queries, keys, and values) and then reshaped to represent multiple heads.\n",
    "\n",
    "The key operation is to split the d_out dimension into num_heads and head_dim, where head_dim = d_out / num_heads. This splitting is then achieved using the .view method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens, num_heads, head_dim).\n",
    "\n",
    "The tensors are then transposed to bring the num_heads dimension before the num_tokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim). This transposition is crucial for correctly aligning the queries, keys, and values across the different heads and performing batched matrix multiplications efficiently.\n",
    "\n",
    "To illustrate this batched matrix multiplication, suppose we have the following example tensor:\n",
    "\n",
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    " \n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "Now, we perform a batched matrix multiplication between the tensor itself and a view of the tensor where we transposed the last two dimensions, num_tokens and head_dim:\n",
    "\n",
    "print(a @ a.transpose(2, 3))\n",
    "\n",
    "The result is as follows:\n",
    "\n",
    "tensor([[[[1.3208, 1.1631, 1.2879],\n",
    "          [1.1631, 2.2150, 1.8424],\n",
    "          [1.2879, 1.8424, 2.0402]],\n",
    " \n",
    "         [[0.4391, 0.7003, 0.5903],\n",
    "          [0.7003, 1.3737, 1.0620],\n",
    "          [0.5903, 1.0620, 0.9912]]]])\n",
    "\n",
    "\n",
    "\n",
    "In this case, the matrix multiplication implementation in PyTorch handles the 4-dimensional input tensor so that the matrix multiplication is carried out between the 2 last dimensions (num_tokens, head_dim) and then repeated for the individual heads.\n",
    "\n",
    "For instance, the above becomes a more compact way to compute the matrix multiplication for each head separately:\n",
    "\n",
    "\n",
    "\n",
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    " \n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)\n",
    "\n",
    "\n",
    "The results are exactly the same results that we obtained when using the batched matrix multiplication print(a @ a.transpose(2, 3)) earlier:\n",
    "\n",
    "First head:\n",
    " tensor([[1.3208, 1.1631, 1.2879],\n",
    "        [1.1631, 2.2150, 1.8424],\n",
    "        [1.2879, 1.8424, 2.0402]])\n",
    " \n",
    "Second head:\n",
    " tensor([[0.4391, 0.7003, 0.5903],\n",
    "        [0.7003, 1.3737, 1.0620],\n",
    "        [0.5903, 1.0620, 0.9912]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Continuing with MultiHeadAttention, after computing the attention weights and context vectors, the context vectors from all heads are transposed back to the shape (b, num_tokens, num_heads, head_dim). These vectors are then reshaped (flattened) into the shape (b, num_tokens, d_out), effectively combining the outputs from all heads.\n",
    "\n",
    "Additionally, we added a so-called output projection layer (self.out_proj) to MultiHeadAttention after combining the heads, which is not present in the CausalAttention class. This output projection layer is not strictly necessary (see the References section in Appendix B for more details), but it is commonly used in many LLM architectures, which is why we added it here for completeness.\n",
    "\n",
    "Even though the MultiHeadAttention class looks more complicated than the MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors, it is more efficient. The reason is that we only need one matrix multiplication to compute the keys, for instance, keys = self.W_key(x) (the same is true for the queries and values). In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication, which is computationally one of the most expensive steps, for each attention head.\n",
    "\n",
    "The MultiHeadAttention class can be used similar to the SelfAttention and CausalAttention classes we implemented earlier:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "\n",
    "s we can see based on the results, the output dimension is directly controlled by the d_out argument:\n",
    "\n",
    "tensor([[[0.3190, 0.4858],\n",
    "         [0.2943, 0.3897],\n",
    "         [0.2856, 0.3593],\n",
    "         [0.2693, 0.3873],\n",
    "         [0.2639, 0.3928],\n",
    "         [0.2575, 0.4028]],\n",
    " \n",
    "        [[0.3190, 0.4858],\n",
    "         [0.2943, 0.3897],\n",
    "         [0.2856, 0.3593],\n",
    "         [0.2693, 0.3873],\n",
    "         [0.2639, 0.3928],\n",
    "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
    "context_vecs.shape: torch.Size([2, 6, 2])\n",
    "\n",
    "\n",
    "\n",
    "In this section, we implemented the MultiHeadAttention class that we will use in the upcoming sections when implementing and training the LLM itself. Note that while the code is fully functional, we used relatively small embedding sizes and numbers of attention heads to keep the outputs readable.\n",
    "\n",
    "For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention heads and a context vector embedding size of 768. The largest GPT-2 model (1.5 billion parameters) has 25 attention heads and a context vector embedding size of 1600. Note that the embedding sizes of the token inputs and context embeddings are the same in GPT models (d_in = d_out).\n",
    "Exercise 3.3 Initializing GPT-2 size attention modules\n",
    "\n",
    "Using the MultiHeadAttention class, initialize a multi-head attention module that has the same number of attention heads as the smallest GPT-2 model (12 attention heads). Also ensure that you use the respective input and output embedding sizes similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a context length of 1024 tokens.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.7 Summary\n",
    "\n",
    "    Attention mechanisms transform input elements into enhanced context vector representations that incorporate information about all inputs.\n",
    "    A self-attention mechanism computes the context vector representation as a weighted sum over the inputs.\n",
    "    In a simplified attention mechanism, the attention weights are computed via dot products.\n",
    "    A dot product is just a concise way of multiplying two vectors element-wise and then summing the products.\n",
    "    Matrix multiplications, while not strictly required, help us to implement computations more efficiently and compactly by replacing nested for-loops.\n",
    "    In self-attention mechanisms that are used in LLMs, also called scaled-dot product attention, we include trainable weight matrices to compute intermediate transformations of the inputs: queries, values, and keys.\n",
    "    When working with LLMs that read and generate text from left to right, we add a causal attention mask to prevent the LLM from accessing future tokens.\n",
    "    Next to causal attention masks to zero out attention weights, we can also add a dropout mask to reduce overfitting in LLMs.\n",
    "    The attention modules in transformer-based LLMs involve multiple instances of causal attention, which is called multi-head attention.\n",
    "    We can create a multi-head attention module by stacking multiple instances of causal attention modules.\n",
    "    A more efficient way of creating multi-head attention modules involves batched matrix multiplications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
