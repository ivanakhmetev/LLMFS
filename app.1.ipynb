{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix A. Introduction to PyTorch\n",
    "This chapter covers\n",
    "\n",
    "    An overview of the PyTorch deep learning library\n",
    "    Setting up an environment and workspace for deep learning\n",
    "    Tensors as a fundamental data structure for deep learning\n",
    "    The mechanics of training deep neural networks\n",
    "    Training models on GPUs\n",
    "\n",
    "This chapter is designed to equip you with the necessary skills and knowledge to put deep learning into practice and implement large language models (LLMs) from scratch.\n",
    "\n",
    "We will introduce PyTorch, a popular Python-based deep learning library, which will be our primary tool for the remainder of this book. This chapter will also guide you through setting up a deep learning workspace armed with PyTorch and GPU support.\n",
    "\n",
    "Then, you'll learn about the essential concept of tensors and their usage in PyTorch. We will also delve into PyTorch's automatic differentiation engine, a feature that enables us to conveniently and efficiently use backpropagation, which is a crucial aspect of neural network training.\n",
    "\n",
    "Note that this chapter is meant as a primer for those who are new to deep learning in PyTorch. While this chapter explains PyTorch from the ground up, it's not meant to be an exhaustive coverage of the PyTorch library. Instead, this chapter focuses on the PyTorch fundamentals that we will use to implement LLMs throughout this book. If you are already familiar with deep learning, you may skip this appendix and directly move on to chapter 2, working with text data.\n",
    "\n",
    "A.1 What is PyTorch\n",
    "\n",
    "PyTorch (https://pytorch.org/) is an open-source Python-based deep learning library. According to Papers With Code (https://paperswithcode.com/trends), a platform that tracks and analyzes research papers, PyTorch has been the most widely used deep learning library for research since 2019 by a wide margin. And according to the Kaggle Data Science and Machine Learning Survey 2022 (https://www.kaggle.com/c/kaggle-survey-2022), the number of respondents using PyTorch is approximately 40% and constantly grows every year.\n",
    "\n",
    "One of the reasons why PyTorch is so popular is its user-friendly interface and efficiency. However, despite its accessibility, it doesn't compromise on flexibility, providing advanced users the ability to tweak lower-level aspects of their models for customization and optimization. In short, for many practitioners and researchers, PyTorch offers just the right balance between usability and features.\n",
    "\n",
    "In the following subsections, we will define the main features PyTorch has to offer.\n",
    "A.1.1 The three core components of PyTorch\n",
    "\n",
    "PyTorch is a relatively comprehensive library, and one way to approach it is to focus on its three broad components, which are summarized in figure A.1.\n",
    "Figure A.1 PyTorch's three main components include a tensor library as a fundamental building block for computing, automatic differentiation for model optimization, and deep learning utility functions, making it easier to implement and train deep neural network models.\n",
    "\n",
    "\n",
    "Firstly, PyTorch is a tensor library that extends the concept of array-oriented programming library NumPy with the additional feature of accelerated computation on GPUs, thus providing a seamless switch between CPUs and GPUs.\n",
    "\n",
    "Secondly, PyTorch is an automatic differentiation engine, also known as autograd, which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization.\n",
    "\n",
    "Finally, PyTorch is a deep learning library, meaning that it offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models, catering to both researchers and developers.\n",
    "\n",
    "After defining the term deep learning and installing PyTorch in the two following subsections, the remainder of this chapter will go over these three core components of PyTorch in more detail, along with hands-on code examples.\n",
    "A.1.2 Defining deep learning\n",
    "\n",
    "LLMs are often referred to as AI models in the news. However, as illustrated in the first section of chapter 1 (1.1 What is an LLM?) LLMs are also a type of deep neural network, and PyTorch is a deep learning library. Sounds confusing? Let's take a brief moment and summarize the relationship between these terms before we proceed.\n",
    "\n",
    "AI is fundamentally about creating computer systems capable of performing tasks that usually require human intelligence. These tasks include understanding natural language, recognizing patterns, and making decisions. (Despite significant progress, AI is still far from achieving this level of general intelligence.)\n",
    "\n",
    "Machine learning represents a subfield of AI (as illustrated in figure A.2) that focuses on developing and improving learning algorithms. The key idea behind machine learning is to enable computers to learn from data and make predictions or decisions without being explicitly programmed to perform the task. This involves developing algorithms that can identify patterns and learn from historical data and improve their performance over time with more data and feedback.\n",
    "Figure A.2 Deep learning is a subcategory of machine learning that is focused on the implementation of deep neural networks. In turn, machine learning is a subcategory of AI that is concerned with algorithms that learn from data. AI is the broader concept of machines being able to perform tasks that typically require human intelligence.\n",
    "\n",
    "\n",
    "Machine learning has been integral in the evolution of AI, powering many of the advancements we see today, including LLMs. Machine learning is also behind technologies like recommendation systems used by online retailers and streaming services, email spam filtering, voice recognition in virtual assistants, and even self-driving cars. The introduction and advancement of machine learning have significantly enhanced AI's capabilities, enabling it to move beyond strict rule-based systems and adapt to new inputs or changing environments.\n",
    "\n",
    "Deep learning is a subcategory of machine learning that focuses on the training and application of deep neural networks. These deep neural networks were originally inspired by how the human brain works, particularly the interconnection between many neurons. The \"deep\" in deep learning refers to the multiple hidden layers of artificial neurons or nodes that allow them to model complex, nonlinear relationships in the data.\n",
    "\n",
    "Unlike traditional machine learning techniques that excel at simple pattern recognition, deep learning is particularly good at handling unstructured data like images, audio, or text, so deep learning is particularly well suited for LLMs.\n",
    "\n",
    "The typical predictive modeling workflow (also referred to as supervised learning) in machine learning and deep learning is summarized in figure A.3.\n",
    "Figure A.3 The supervised learning workflow for predictive modeling consists of a training stage where a model is trained on labeled examples in a training dataset. The trained model can then be used to predict the labels of new observations.\n",
    "\n",
    "Using a learning algorithm, a model is trained on a training dataset consisting of examples and corresponding labels. In the case of an email spam classifier, for example, the training dataset consists of emails and their spam and not-spam labels that a human identified. Then, the trained model can be used on new observations (new emails) to predict their unknown label (spam or not spam).\n",
    "\n",
    "Of course, we also want to add a model evaluation between the training and inference stages to ensure that the model satisfies our performance criteria before using it in a real-world application.\n",
    "\n",
    "Note that the workflow for training and using LLMs, as we will see later in this book, is similar to the workflow depicted in figure A.3 if we train them to classify texts. And if we are interested in training LLMs for generating texts, which is the main focus of this book, figure A.3 still applies. In this case, the labels during pretraining can be derived from the text itself (the next-word prediction task introduced in chapter 1). And the LLM will generate entirely new text (instead of predicting labels) given an input prompt during inference.\n",
    "A.1.3 Installing PyTorch\n",
    "\n",
    "PyTorch can be installed just like any other Python library or package. However, since PyTorch is a comprehensive library featuring CPU- and GPU-compatible codes, the installation may require additional explanation.\n",
    "Python version\n",
    "\n",
    "Many scientific computing libraries do not immediately support the newest version of Python. Therefore, when installing PyTorch, it's advisable to use a version of Python that is one or two releases older. For instance, if the latest version of Python is 3.13, using Python 3.10 or 3.11 is recommended.\n",
    "\n",
    "For instance, there are two versions of PyTorch: a leaner version that only supports CPU computing and a version that supports both CPU and GPU computing. If your machine has a CUDA-compatible GPU that can be used for deep learning (ideally an NVIDIA T4, RTX 2080 Ti, or newer), I recommend installing the GPU version. Regardless, the default command for installing PyTorch is as follows in a code terminal:\n",
    "\n",
    "pip install torch\n",
    "\n",
    "Suppose your computer supports a CUDA-compatible GPU. In that case, this will automatically install the PyTorch version that supports GPU acceleration via CUDA, given that the Python environment you're working on has the necessary dependencies (like pip) installed.\n",
    "AMD GPUs for deep learning\n",
    "\n",
    "As of this writing, PyTorch has also added experimental support for AMD GPUs via ROCm. Please see https://pytorch.org for additional instructions.\n",
    "\n",
    "However, to explicitly install the CUDA-compatible version of PyTorch, it's often better to specify the CUDA you want PyTorch to be compatible with. PyTorch's official website (https://pytorch.org) provides commands to install PyTorch with CUDA support for different operating systems as shown in figure A.4.\n",
    "Figure A.4 Access the PyTorch installation recommendation on https://pytorch.org to customize and select the installation command for your system.\n",
    "\n",
    "(Note that the command shown in figure A.4 will also install the torchvision and torchaudio libraries, which are optional for this book.)\n",
    "\n",
    "As of this writing, this book is based on PyTorch 2.0.1, so it's recommended to use the following installation command to install the exact version to guarantee compatibility with this book:\n",
    "\n",
    "pip install torch==2.0.1\n",
    "\n",
    "However, as mentioned earlier, given your operating system, the installation command might slightly differ from the one shown above. Thus, I recommend visiting the https://pytorch.org website and using the installation menu (see figure A4) to select the installation command for your operating system and replace torch with torch==2.0.1 in this command.\n",
    "\n",
    "To check the version of PyTorch, you can execute the following code in PyTorch:\n",
    "\n",
    "import torch\n",
    "torch.__version__\n",
    "\n",
    "This prints:\n",
    "\n",
    "\n",
    "'2.0.1'\n",
    "\n",
    "\n",
    "PyTorch and Torch\n",
    "\n",
    "Note that the Python library is named \"torch\" primarily because it's a continuation of the Torch library but adapted for Python (hence, \"PyTorch\"). The name \"torch\" acknowledges the library's roots in Torch, a scientific computing framework with wide support for machine learning algorithms, which was initially created using the Lua programming language.\n",
    "\n",
    "If you are looking for additional recommendations and instructions for setting up your Python environment or installing the other libraries used later in this book, I recommend visiting the supplementary GitHub repository of this book at https://github.com/rasbt/LLMs-from-scratch.\n",
    "\n",
    "After installing PyTorch, you can check whether your installation recognizes your built-in NVIDIA GPU by running the following code in Python:\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "True\n",
    "\n",
    "If the command returns True, you are all set. If the command returns False, your computer may not have a compatible GPU, or PyTorch does not recognize it. While GPUs are not required for the initial chapters in this book, which are focused on implementing LLMs for educational purposes, they can significantly speed up deep learning-related computations.\n",
    "\n",
    "If you don't have access to a GPU, there are several cloud computing providers where users can run GPU computations against an hourly cost. A popular Jupyter-notebook-like environment is Google Colab (https://colab.research.google.com), which provides time-limited access to GPUs as of this writing. Using the \"Runtime\" menu, it is possible to select a GPU, as shown in the screenshot in figure A.5.\n",
    "Figure A.5 Select a GPU device for Google Colab under the Runtime/Change runtime type menu.\n",
    "\n",
    "\n",
    "\n",
    "PyTorch on Apple Silicon\n",
    "\n",
    "If you have an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer models), you have the option to leverage its capabilities to accelerate PyTorch code execution. To use your Apple Silicon chip for PyTorch, you first need to install PyTorch as you normally would. Then, to check if your Mac supports PyTorch acceleration with its Apple Silicon chip, you can run a simple code snippet in Python:\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "If it returns True, it means that your Mac has an Apple Silicon chip that can be used to accelerate PyTorch code.\n",
    "Exercise A.1\n",
    "\n",
    "Install and set up PyTorch on your computer.\n",
    "Exercise A.2\n",
    "\n",
    "Run the supplementary Chapter 2 code at https://github.com/rasbt/LLMs-from-scratch that checks whether your environment is set up correctly..\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A.2 Understanding tensors\n",
    "\n",
    "Tensors represent a mathematical concept that generalizes vectors and matrices to potentially higher dimensions. In other words, tensors are mathematical objects that can be characterized by their order (or rank), which provides the number of dimensions. For example, a scalar (just a number) is a tensor of rank 0, a vector is a tensor of rank 1, and a matrix is a tensor of rank 2, as illustrated in figure A.6\n",
    "Figure A.6 An illustration of tensors with different ranks. Here 0D corresponds to rank 0, 1D to rank 1, and 2D to rank 2. Note that a 3D vector, which consists of 3 elements, is still a rank 1 tensor.\n",
    "\n",
    "From a computational perspective, tensors serve as data containers. For instance, they hold multi-dimensional data, where each dimension represents a different feature. Tensor libraries, such as PyTorch, can create, manipulate, and compute with these multi-dimensional arrays efficiently. In this context, a tensor library functions as an array library.\n",
    "\n",
    "PyTorch tensors are similar to NumPy arrays but have several additional features important for deep learning. For example, PyTorch adds an automatic differentiation engine, simplifying computing gradients, as discussed later in section 2.4. PyTorch tensors also support GPU computations to speed up deep neural network training, which we will discuss later in section 2.8.\n",
    "PyTorch's has a NumPy-like API\n",
    "\n",
    "As you will see in the upcoming sections, PyTorch adopts most of the NumPy array API and syntax for its tensor operations. If you are new to NumPy, you can get a brief overview of the most relevant concepts via my article Scientific Computing in Python: Introduction to NumPy and Matplotlib at https://sebastianraschka.com/blog/2020/numpy-intro.html.\n",
    "\n",
    "The following subsections will look at the basic operations of the PyTorch tensor library, showing how to create simple tensors and going over some of the essential operations.\n",
    "A.2.1 Scalars, vectors, matrices, and tensors\n",
    "\n",
    "As mentioned earlier, PyTorch tensors are data containers for array-like structures. A scalar is a 0-dimensional tensor (for instance, just a number), a vector is a 1-dimensional tensor, and a matrix is a 2-dimensional tensor. There is no specific term for higher-dimensional tensors, so we typically refer to a 3-dimensional tensor as just a 3D tensor, and so forth.\n",
    "\n",
    "We can create objects of PyTorch's Tensor class using the torch.tensor function as follows:\n",
    "Listing A.1 Creating PyTorch tensors\n",
    "\n",
    "import torch\n",
    " \n",
    " \n",
    "tensor0d = torch.tensor(1)\n",
    " \n",
    " \n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    " \n",
    " \n",
    "tensor2d = torch.tensor([[1, 2], [3, 4]])\n",
    " \n",
    " \n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "\n",
    "A.2.2 Tensor data types\n",
    "\n",
    "In the previous section, we created tensors from Python integers. In this case, PyTorch adopts the default 64-bit integer data type from Python. We can access the data type of a tensor via the .dtype attribute of a tensor:\n",
    "\n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(tensor1d.dtype)\n",
    "\n",
    "This prints:\n",
    "\n",
    "torch.int64\n",
    "\n",
    "If we create tensors from Python floats, PyTorch creates tensors with a 32-bit precision by default, as we can see below:\n",
    "\n",
    "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(floatvec.dtype)\n",
    "\n",
    "The output is:\n",
    "\n",
    "torch.float32\n",
    "\n",
    "This choice is primarily due to the balance between precision and computational efficiency. A 32-bit floating point number offers sufficient precision for most deep learning tasks, while consuming less memory and computational resources than a 64-bit floating point number. Moreover, GPU architectures are optimized for 32-bit computations, and using this data type can significantly speed up model training and inference.\n",
    "\n",
    "Moreover, it is possible to readily change the precision using a tensor's .to method. The following code demonstrates this by changing a 64-bit integer tensor into a 32-bit float tensor:\n",
    "\n",
    "floatvec = tensor1d.to(torch.float32)\n",
    "print(floatvec.dtype)\n",
    "\n",
    "\n",
    "\n",
    "This returns:\n",
    "\n",
    "\n",
    "\n",
    "torch.float32\n",
    "\n",
    "For more information about different tensor data types available in PyTorch, I recommend checking the official documentation at https://pytorch.org/docs/stable/tensors.html.\n",
    "A.2.3 Common PyTorch tensor operations\n",
    "\n",
    "Comprehensive coverage of all the different PyTorch tensor operations and commands is outside the scope of this book. However, we will briefly describe relevant operations as we introduce them throughout the book.\n",
    "\n",
    "Before we move on to the next section covering the concept behind computation graphs, below is a list of the most essential PyTorch tensor operations.\n",
    "\n",
    "We already introduced the torch.tensor() function to create new tensors.\n",
    "\n",
    "\n",
    "tensor2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(tensor2d)\n",
    "\n",
    "This prints:\n",
    "tensor([[1, 2, 3],\n",
    "        [4, 5, 6]])\n",
    "\n",
    "In addition, the .shape attribute allows us to access the shape of a tensor:\n",
    "\n",
    "print(tensor2d.shape)\n",
    "\n",
    "The output is:\n",
    "torch.Size([2, 3])\n",
    "\n",
    "As you can see above, .shape returns [2, 3], which means that the tensor has 2 rows and 3 columns. To reshape the tensor into a 3 by 2 tensor, we can use the .reshape method:\n",
    "\n",
    "print(tensor2d.reshape(3, 2))\n",
    "\n",
    "This prints:\n",
    "tensor([[1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6]])\n",
    "\n",
    "However, note that the more common command for reshaping tensors in PyTorch is .view():\n",
    "\n",
    "print(tensor2d.view(3, 2))\n",
    "\n",
    "\n",
    "The output is:\n",
    "\n",
    "tensor([[1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6]])\n",
    "\n",
    "\n",
    "\n",
    "Similar to .reshape and .view, there are several cases where PyTorch offers multiple syntax options for executing the same computation. This is because PyTorch initially followed the original Lua Torch syntax convention but then also added syntax to make it more similar to NumPy upon popular request.\n",
    "\n",
    "Next, we can use .T to transpose a tensor, which means flipping it across its diagonal. Note that this is similar from reshaping a tensor as you can see based on the result below:\n",
    "print(tensor2d.T)\n",
    "\n",
    "The output is:\n",
    "\n",
    "\n",
    "tensor([[1, 4],\n",
    "        [2, 5],\n",
    "        [3, 6]])\n",
    "\n",
    "Lastly, the common way to multiply two matrices in PyTorch is the .matmul method:\n",
    "\n",
    "print(tensor2d.matmul(tensor2d.T))\n",
    "\n",
    "The output is:\n",
    "\n",
    "tensor([[14, 32],\n",
    "        [32, 77]])\n",
    "\n",
    "owever, we can also adopt the @ operator, which accomplishes the same thing more compactly:\n",
    "\n",
    "print(tensor2d @ tensor2d.T)\n",
    "\n",
    "This prints:\n",
    "\n",
    "tensor([[14, 32],\n",
    "        [32, 77]])\n",
    "\n",
    "As mentioned earlier, we will introduce additional operations throughout this book when needed. For readers who'd like to browse through all the different tensor operations available in PyTorch (hint: we won't need most of these), I recommend checking out the official documentation at https://pytorch.org/docs/stable/tensors.html."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A.3 Seeing models as computation graphs\n",
    "\n",
    "In the previous section, we covered one of the major three components of PyTorch, namely, its tensor library. Next in line is PyTorch's automatic differentiation engine, also known as autograd. PyTorch's autograd system provides functions to compute gradients in dynamic computational graphs automatically. But before we dive deeper into computing gradients in the next section, let's define the concept of a computational graph.\n",
    "\n",
    "A computational graph (or computation graph in short) is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network -- we will need this later to compute the required gradients for backpropagation, which is the main training algorithm for neural networks.\n",
    "\n",
    "Let's look at a concrete example to illustrate the concept of a computation graph. The following code implements the forward pass (prediction step) of a simple logistic regression classifier, which can be seen as a single-layer neural network, returning a score between 0 and 1 that is compared to the true class label (0 or 1) when computing the loss:\n",
    "Listing A.2 A logistic regression forward pass\n",
    "\n",
    "import torch.nn.functional as F\n",
    " \n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2])\n",
    "b = torch.tensor([0.0])\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    " \n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "\n",
    "If not all components in the code above make sense to you, don't worry. The point of this example is not to implement a logistic regression classifier but rather to illustrate how we can think of a sequence of computations as a computation graph, as shown in figure A.7.\n",
    "Figure A.7 A logistic regression forward pass as a computation graph. The input feature x1 is multiplied by a model weight w1 and passed through an activation function Ïƒ after adding the bias. The loss is computed by comparing the model output a with a given label y.\n",
    "\n",
    "In fact, PyTorch builds such a computation graph in the background, and we can use this to calculate gradients of a loss function with respect to the model parameters (here w1 and b) to train the model, which is the topic of the upcoming sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A.4 Automatic differentiation made easy\n",
    "\n",
    "In the previous section, we introduced the concept of computation graphs. If we carry out computations in PyTorch, it will build such a graph internally by default if one of its terminal nodes has the requires_grad attribute set to True. This is useful if we want to compute gradients. Gradients are required when training neural networks via the popular backpropagation algorithm, which can be thought of as an implementation of the chain rule from calculus for neural networks, which is illustrated in figure A.8.\n",
    "Figure A.8 The most common way of computing the loss gradients in a computation graph involves applying the chain rule from right to left, which is also called reverse-model automatic differentiation or backpropagation. It means we start from the output layer (or the loss itself) and work backward through the network to the input layer. This is done to compute the gradient of the loss with respect to each parameter (weights and biases) in the network, which informs how we update these parameters during training.\n",
    "\n",
    "\n",
    "Partial derivatives and gradients\n",
    "\n",
    "Figure A.8 shows partial derivatives, which measure the rate at which a function changes with respect to one of its variables. A gradient is a vector containing all of the partial derivatives of a multivariate function, a function with more than one variable as input.\n",
    "\n",
    "If you are not familiar or don't remember the partial derivatives, gradients, or the chain rule from calculus, don't worry. On a high level, all you need to know for this book is that the chain rule is a way to compute gradients of a loss function with respect to the model's parameters in a computation graph. This provides the information needed to update each parameter in a way that minimizes the loss function, which serves as a proxy for measuring the model's performance, using a method such as gradient descent. We will revisit the computational implementation of this training loop in PyTorch in section 2.7, A typical training loop.\n",
    "\n",
    "Now, how is this all related to the second component of the PyTorch library we mentioned earlier, the automatic differentiation (autograd) engine? By tracking every operation performed on tensors, PyTorch's autograd engine constructs a computational graph in the background. Then, calling the grad function, we can compute the gradient of the loss with respect to model parameter w1 as follows:\n",
    "Listing A.3 Computing gradients via autograd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    " \n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    " \n",
    "z = x1 * w1 + b \n",
    "a = torch.sigmoid(z)\n",
    " \n",
    "loss = F.binary_cross_entropy(a, y)\n",
    " \n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)\n",
    " \n",
    "\n",
    "Let's show the resulting values of the loss with respect to the model's parameters:\n",
    "\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)\n",
    "\n",
    "The prints:\n",
    "\n",
    "\n",
    "(tensor([-0.0898]),)\n",
    "(tensor([-0.0817]),)\n",
    "\n",
    "Above, we have been using the grad function \"manually,\" which can be useful for experimentation, debugging, and demonstrating concepts. But in practice, PyTorch provides even more high-level tools to automate this process. For instance, we can call .backward on the loss, and PyTorch will compute the gradients of all the leaf nodes in the graph, which will be stored via the tensors' .grad attributes:\n",
    "\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)\n",
    "\n",
    "\n",
    "\n",
    "The outputs are:\n",
    "\n",
    "\n",
    "(tensor([-0.0898]),)\n",
    "(tensor([-0.0817]),)\n",
    "\n",
    "If this section is packed with a lot of information and you may be overwhelmed by the calculus concepts, don't worry. While this calculus jargon was a means to explain PyTorch's autograd component, all you need to take away from this section is that PyTorch takes care of the calculus for us via the .backward method -- we won't need to compute any derivatives or gradients by hand in this book."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A.5 Implementing multilayer neural networks\n",
    "\n",
    "In the previous sections, we covered PyTorch's tensor and autograd components. This section focuses on PyTorch as a library for implementing deep neural networks.\n",
    "\n",
    "To provide a concrete example, we focus on a multilayer perceptron, which is a fully connected neural network, as illustrated in figure A.9.\n",
    "Figure A.9 An illustration of a multilayer perceptron with 2 hidden layers. Each node represents a unit in the respective layer. Each layer has only a very small number of nodes for illustration purposes.\n",
    "\n",
    "When implementing a neural network in PyTorch, we typically subclass the torch.nn.Module class to define our own custom network architecture. This Module base class provides a lot of functionality, making it easier to build and train models. For instance, it allows us to encapsulate layers and operations and keep track of the model's parameters.\n",
    "\n",
    "Within this subclass, we define the network layers in the __init__ constructor and specify how they interact in the forward method. The forward method describes how the input data passes through the network and comes together as a computation graph.\n",
    "\n",
    "In contrast, the backward method, which we typically do not need to implement ourselves, is used during training to compute gradients of the loss function with respect to the model parameters, as we will see in section 2.7, A typical training loop.\n",
    "\n",
    "The following code implements a classic multilayer perceptron with two hidden layers to illustrate a typical usage of the Module class:\n",
    "Listing A.4 A multilayer perceptron with two hidden layers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
