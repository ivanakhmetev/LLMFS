{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Обработка текста\n",
    "\n",
    "В предыдущей главе мы углубились в структуру больших языковых моделей (LLM) и узнали, что они предварительно обучаются на огромных объемах текста, обрабатывая\n",
    "по одному слову (токену) за раз. \n",
    "\n",
    "![](images/llm2.1.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Векторное представление (vector embedding)\n",
    "\n",
    "Нейросетевые модели, не могут напрямую обрабатывать текст. Текст, как тип данных, несовместим с тензорными операциями. Следовательно, нужен способ представлять слова в виде векторов с непрерывными значениями. (ссылка узнать больше в Приложении A, раздел A2.2 «Понимание тензоров».)\n",
    "\n",
    "Построение векторного представления может быть проведено разными способами (различными моделями). \n",
    "Прочие типы данных также требуют преобразования в тензорный формат.  \n",
    "\n",
    "![](images/llm2.2.png)\n",
    "\n",
    "Существует несколько алгоритмов для создания векторного представления текста. Важным моментом является выбор - что считать единичным токеном. Одним из первых и наиболее популярных примеров является Word2Vec (токен - слово). Основная идея заключается в том, что слова, встречающиеся в схожем контексте, имеют схожее значение. Следовательно, при проецировании в двумерное пространство, можно увидеть, что схожие термины группируются вместе.\n",
    "\n",
    "![](images/llm2.3.png)\n",
    "\n",
    "Помимо моделей токенизации слов, существуют модели токенизации предложений, абзацев, целых документов.\n",
    "Таким образом, картинка из предыдущей главы (predicting next world) не вполне корректна. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вендоры LLM обычно создают свои собственные модели токенизации, которые являются частью входного слоя и обновляются во время обучения. Преимущество такого подхода - оптимизация для конкретной задачи и имеющихся данных. \n",
    "\n",
    "Размерность представления (двумерное на рисунке выше) может быть произвольной. Большая размерность может отражать более тонкие отношения, но за счет вычислительной эффективности. Эта размерность называется размерностью скрытых состояний модели (dimensionality of the model's hidden states). \n",
    "\n",
    "Эта размерность - компромисс между производительностью и эффективностью:  \n",
    "- GPT-2 (125M параметров) используют размерность 768  \n",
    "- GPT-3 (175B параметров) использует размерность 12 288  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Токенизация текста\n",
    "\n",
    "![](images/llm2.4.png)\n",
    "\n",
    "Будем токенизировать рассказ Эдит Уортон [«Вердикт»](https://en.wikisource.org/wiki/The_Verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для разделения текста на токены, используем сначала команду re.split, чтобы разделить текст на пробельные символы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем регулярные выражения для выделения запятых и точек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При разработке токенизатора, выделение пробельных символов зависит от приложения и его требований. Удаление пробелов снижает требования к памяти и вычислительным ресурсам. Однако сохранение пробелов может быть полезно, если мы обучаем модели, чувствительные к точной структуре текста (например, код Python, чувствительный к отступам и интервалам). \n",
    "\n",
    "Изменим код, чтобы он мог обрабатывать и другие типы пунктуации, такие как вопросительные знаки, кавычки и двойные тире, которые мы видели ранее в первых 100 символах рассказа Эдит Уортон, а также дополнительные специальные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша модель токенизации теперь может успешно обрабатывать различные специальные символы в тексте.\n",
    "\n",
    "![](images/llm2.5.png)\n",
    "\n",
    "Теперь, когда у нас есть работающий базовый токенизатор, применим его ко всему рассказу Эдит Уортон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Преобразование токенов в идентификаторы\n",
    "\n",
    "Полученные токены сортируются в алфавитном порядке, повторяющиеся удаляются. Затем уникальные токены объединяются в словарь: текстовый токен - целочисленное значение.\n",
    "\n",
    "![](images/llm2.6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 15:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее применим этот словарь для преобразования всего текста в токены.\n",
    "\n",
    "![](images/llm2.7.png)\n",
    "\n",
    "Выходные данные LLM будут поступать как последовательность числовых значений, которые поребуется преобразовать обратно в текст. Для этого мы можем создать обратную версию словаря (детокенизатор).\n",
    "\n",
    "Реализуем полный класс токенизатора и детокенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "        \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    " \n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 Добавление специальных токенов контекста\n",
    "\n",
    "Изменим токенизатор для обработки неизвестных слов. Также обсудим использование и добавление специальных токенов контекста, которые могут улучшить понимание моделью контекста или другой соответствующей информации в тексте. Эти специальные токены могут включать, например, маркеры для неизвестных слов и границ документа.\n",
    "\n",
    "В частности, мы изменим словарь и токенизатор для поддержки двух новых токенов: <|unk|> и <|endoftext|>\n",
    "\n",
    "![](images/llm2.8.png)\n",
    "\n",
    "Как показано на рисунке, мы можем изменить токенизатор, чтобы он использовал токен <|unk|>, если он встретит слово, которое не является частью словаря. Кроме того, мы добавляем токен между несвязанными текстами. Например, при обучении GPT-подобных LLM на нескольких независимых документах или книгах обычно вставляется токен перед каждым документом или книгой, следующим за предыдущим источником текста. Это помогает LLM понять, что, хотя эти текстовые источники объединены для обучения, на самом деле они не связаны друг с другом.\n",
    "\n",
    "![](images/llm2.9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n",
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    " \n",
    "print(len(vocab.items()))\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    " \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    " \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы видим, что идентификатор 1159 соответствует токену-разделителю <|endoftext|>, а идентификатор 1160 используются для неизвестных слов.\n",
    "\n",
    "Сравнивая приведенный выше детокенизированный текст с исходным входным текстом, мы знаем, что набор обучающих данных не содержал слов «Привет» и «дворец».\n",
    "\n",
    "Также рассматривают дополнительные специальные токены, такие как:\n",
    "- [BOS] (начало последовательности): этот токен отмечает начало текста.   \n",
    "- [EOS] (конец последовательности): этот токен располагается в конце текста. Например, при объединении двух разных статей или книг Википедии токен [EOS] указывает, где заканчивается одна статья и начинается следующая.  \n",
    "- [PAD] (заполнение): при обучении LLM с размерами пакетов больше единицы пакет может содержать тексты различной длины. Чтобы гарантировать одинаковую длину всех текстов, более короткие тексты расширяются или «дополняются» с помощью токена [PAD] до длины самого длинного текста в пакете.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Кодирование по парам байтов (Byte pair encoding)\n",
    "\n",
    "В этом разделе рассматривается более сложная схема токенизации. Она использовалась для обучения GPT-2, GPT-3.\n",
    "\n",
    "Поскольку реализация BPE может быть сложной, будем использовать существующую библиотеку Python с открытым исходным кодом под названием [tiktoken](https://github.com/openai/tiktoken), которая очень эффективно реализует алгоритм BPE на основе исходного кода на Rust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.6.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание:\n",
    "- Во-первых, токену <|endoftext|> присваивается относительно большой идентификатор токена, а именно 50256. Фактически, токенизатор BPE, который использовался для обучения ChatGPT имеет общий размер словаря 50 257, причем <|endoftext|> присвоен самый большой идентификатор токена.\n",
    "\n",
    "- Во-вторых, BPE правильно кодирует и декодирует неизвестные слова, такие как «someunknownPlace». Токенизатор BPE может обрабатывать любое неизвестное слово.\n",
    "\n",
    "Алгоритм, лежащий в основе BPE, разбивает слова, которых нет в его предопределенном словаре, на более мелкие подслова или даже отдельные символы, что позволяет ему обрабатывать слова, которых нет в словаре. Таким образом, благодаря алгоритму BPE, если токенизатор встречает незнакомое слово во время токенизации, он может представить его как последовательность токенов или символов подслова, как показано на рисунке.\n",
    "\n",
    "![](images/llm2.10.png)\n",
    "\n",
    "Подробное обсуждение и реализация BPE выходит за рамки этой книги, но, если коротко, словарь строится путем итеративного объединения часто встречающихся символов в подслова, а часто встречающихся подслов — в слова. Например, BPE начинает с добавления в свой словарь всех отдельных одиночных символов («a», «b», ...). На следующем этапе он объединяет комбинации символов, которые часто встречаются вместе, в подслова. Например, «d» и «e» могут быть объединены в подслово «de», которое часто встречается во многих английских словах, таких как «define», «dependent», «made» и «hidden»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Выборка данных с помощью скользящего окна\n",
    "\n",
    "Следующим шагом является создание пар вход - цель (input - target), необходимых для обучения LLM.\n",
    "\n",
    "![](images/llm2.11.png)\n",
    "\n",
    "Реализуем теперь загрузчик данных, который извлекает пары из набора обучающих данных, используя подход скользящего окна.\n",
    "\n",
    "Для чего токенизируем весь рассказ The Verdict, с помощью токенизатора BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интуитивно понятный способ создания пар данных — создание двух переменных, x и y, где x содержит входные токены, а y содержит целевые значения, которые представляют собой входные данные, сдвинутые на 1 (размер скользящего окна):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно представить задачу предсказания следующего слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все, что находится слева от стрелки (---->), относится к входным данным, которые LLM получит, а идентификатор токена в правой части стрелки представляет собой целевой идентификатор токена, который LLM должен предсказать.\n",
    "\n",
    "В целях иллюстрации давайте повторим предыдущий код, но преобразуем идентификаторы токенов в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая задача - реализация эффективного загрузчика данных, который выполняет итерацию по входному набору данных и возвращает входные и целевые значения в виде тензоров PyTorch, каждая строка которых - один вход или одна цель (на рисунке слова, вместо идентификаторов). \n",
    "\n",
    "![](images/llm2.12.png)\n",
    "\n",
    "Бдем использовать встроенные классы PyTorch Dataset и DataLoader. (ссылка Дополнительную информацию и рекомендации по установке PyTorch см. в разделе A.1.3 «Установка PyTorch» в Приложении A.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    " \n",
    "        token_ids = tokenizer.encode(txt)\n",
    " \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс GPTDatasetV1 определяет, как отдельные строки извлекаются из набора данных (каждая строка состоит из нескольких идентификаторов токенов). \n",
    "\n",
    "(ссылка прочтите раздел A.6 «Настройка эффективных загрузчиков данных» в приложении A, в котором объясняется общая структура и использование классов PyTorch Dataset и DataLoader )\n",
    "\n",
    "Определим далее загрузку тензоров с помощью PyTorch DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, \n",
    "        max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переменная first_batch содержит два тензора: первый тензор хранит идентификаторы входных токенов, а второй тензор хранит идентификаторы целевых токенов. Поскольку max_length установлено равным 4, каждый из двух тензоров содержит 4 идентификатора токена. Обратите внимание, что входной размер 4 относительно мал и выбран только для иллюстрации. Обычно LLM обучают с размером входных данных не менее 256.\n",
    "\n",
    "Чтобы проиллюстрировать значение шага=1, давайте возьмем еще один пакет из этого набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/llm2.13.png)\n",
    "\n",
    "Размеры пакетов равные 1, cлужат для иллюстративных целей. Размер пакета — это компромисс и гиперпараметр, с которым можно экспериментировать при обучении LLM. Небольшие размеры пакетов требуют меньше памяти во время обучения, но приводят к более \"шумным\" обновлениям модели.\n",
    "\n",
    "Пример загрузчика с другими параметрами - увеличиваем шаг до 4. Это необходимо для полного использования набора данных (не пропускаем ни одного слова), но также для избежания любого перекрытия между пакетами, поскольку большее перекрытие может привести к переобучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[ 1576,   407,   284,   766],\n",
      "        [ 1068,   284,  1577,   257],\n",
      "        [ 2735,   314,  2497,   326],\n",
      "        [11293,    13,   366,   464],\n",
      "        [  438, 14363, 10568,   852],\n",
      "        [  832,   438,  3826,  3892],\n",
      "        [  417,    12, 12239,    11],\n",
      "        [  198,   198,  5779, 28112]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  407,   284,   766,   607],\n",
      "        [  284,  1577,   257, 23844],\n",
      "        [  314,  2497,   326,   339],\n",
      "        [   13,   366,   464,   938],\n",
      "        [14363, 10568,   852,  5729],\n",
      "        [  438,  3826,  3892,   284],\n",
      "        [   12, 12239,    11,   475],\n",
      "        [  198,  5779, 28112, 10197]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
    " \n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Создание векторного представления\n",
    "\n",
    "Последним шагом подготовки входного текста для обучения LLM является преобразование идентификаторов токенов в векторы - представления, как показано на рисунке.Векторное представление необходимо, поскольку LLM обучаются с помощью алгоритма обратного распространения ошибки. (ссылка Если вы не знакомы с тем, как нейронные сети обучаются с помощью обратного распространения ошибки, прочтите раздел A.4 «Легкая автоматическая дифференциация» в Приложении A.)\n",
    "\n",
    "![](images/llm2.14.png)\n",
    "\n",
    "Важно отметить, что мы инициализируем эти векторы случайными значениями. Эта инициализация служит отправной точкой процесса обучения LLM. \n",
    "\n",
    "Проиллюстрируем, как работает векторное преобразование идентификатора токена на практическом примере. Предположим, у нас есть следующие четыре входных токена с идентификаторами 2, 3, 5 и 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты предположим, что у нас есть небольшой словарь, состоящий всего из 6 слов (вместо 50 257 слов в словаре токенизатора BPE), и мы хотим создать представления размерности 3 (в GPT-3 размреность = 12 288).\n",
    "\n",
    "Случайное начальное значение 123 задано в целях воспроизводимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что весовая матрица слоя внедрения содержит небольшие случайные значения. Более того, мы видим, что матрица весов имеет шесть строк и три столбца. В словаре имеется одна строка для каждого из шести возможных токенов. И есть один столбец для каждого из трех измерений.\n",
    "\n",
    "После того как мы создали экземпляр слоя представления, давайте теперь применим его к идентификатору токена, чтобы получить вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы сравним вектор для токена с идентификатором 3 с предыдущей матрицей, мы увидим, что он идентичен 4-й строке (Python начинается с нулевого индекса, поэтому эта строка соответствует индексу 3). Другими словами, уровень внедрения — это, по сути, операция поиска, которая извлекает строки из весовой матрицы слоя внедрения через идентификатор токена.\n",
    "\n",
    "\n",
    "Встраивание слоев в сравнении с умножением матриц\n",
    "\n",
    "Для тех, кто знаком с горячим кодированием, описанный выше подход уровня внедрения — это, по сути, просто более эффективный способ реализации горячего кодирования с последующим умножением матриц на полностью связном уровне, что проиллюстрировано в дополнительном коде на GitHub по адресу https. ://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/03_bonus_embedding-vs-matmul. Поскольку уровень внедрения — это просто более эффективная реализация, эквивалентная подходу «горячего кодирования» и умножения матриц, его можно рассматривать как уровень нейронной сети, который можно оптимизировать с помощью обратного распространения ошибки.\n",
    "\n",
    "Ранее мы видели, как преобразовать одиночный идентификатор токена в трехмерный вектор внедрения. Давайте теперь применим это ко всем четырем входным идентификаторам, которые мы определили ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#(torch.tensor([2, 3, 5, 1])):\n",
    "\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая строка в этой выходной матрице получается посредством операции поиска из матрицы весов внедрения, как показано на рисунке 2.16.\n",
    "\n",
    "Рисунок 2.16. Уровни внедрения выполняют операцию поиска, извлекая вектор внедрения, соответствующий идентификатору токена, из весовой матрицы уровня внедрения. Например, вектор внедрения токена с идентификатором 5 — это шестая строка матрицы весов слоя внедрения (это шестая строка вместо пятой, поскольку Python начинает отсчет с 0). В целях иллюстрации мы предполагаем, что идентификаторы токенов были созданы с помощью небольшого словаря, который мы использовали в разделе 2.3.\n",
    "![](images/llm2.16.png)\n",
    "\n",
    "\n",
    "В этом разделе описано, как мы создаем векторы внедрения на основе идентификаторов токенов. В следующем и последнем разделе этой главы к этим векторам внедрения будет добавлена ​​небольшая модификация для кодирования позиционной информации о токене в тексте."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8 Кодирование позиций слов\n",
    "\n",
    "В предыдущем разделе мы преобразовали идентификаторы токенов в непрерывное векторное представление, так называемые встраивания токенов. В принципе, это подходящий ввод для LLM. Однако незначительным недостатком LLM является то, что их механизм самообслуживания, который будет подробно описан в главе 3, не имеет представления о положении или порядке токенов внутри последовательности.\n",
    "\n",
    "Принцип работы ранее представленного уровня внедрения заключается в том, что один и тот же идентификатор токена всегда отображается в одно и то же векторное представление, независимо от того, где идентификатор токена расположен во входной последовательности, как показано на рисунке 2.17.\n",
    "\n",
    "Рисунок 2.17. Уровень внедрения преобразует идентификатор токена в одно и то же векторное представление независимо от того, где он расположен во входной последовательности. Например, идентификатор токена 5, независимо от того, находится ли он в первой или третьей позиции во входном векторе идентификатора токена, приведет к одному и тому же вектору внедрения.\n",
    "\n",
    "\n",
    "![](images/llm2.17.png)\n",
    "\n",
    "\n",
    "\n",
    "В принципе, детерминированное, независимое от позиции внедрение идентификатора токена полезно для целей воспроизводимости. Однако, поскольку механизм самообслуживания LLM сам по себе также не зависит от позиции, полезно вводить дополнительную информацию о местоположении в LLM.\n",
    "\n",
    "Для достижения этой цели существуют две широкие категории вложений с учетом позиции: относительные позиционные вложения и абсолютные позиционные вложения.\n",
    "\n",
    "Абсолютные позиционные вложения напрямую связаны с конкретными позициями в последовательности. Для каждой позиции во входной последовательности к внедрению токена добавляется уникальное вложение, чтобы передать его точное местоположение. Например, первый токен будет иметь определенное позиционное вложение, второй токен — другое отдельное вложение и т. д., как показано на рисунке 2.18.\n",
    "\n",
    "\n",
    "Рисунок 2.18. Позиционные внедрения добавляются к вектору внедрения токена для создания входных внедрений для LLM. Позиционные векторы имеют ту же размерность, что и исходные вложения токенов. Внедрения токенов для простоты показаны со значением 1.\n",
    "\n",
    "\n",
    "?????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя указанный выше token_embedding_layer, если мы выбираем данные из загрузчика данных, мы встраиваем каждый токен в каждом пакете в 256-мерный вектор. Если у нас размер пакета 8 по четыре токена в каждом, результатом будет тензор 8 x 4 x 256.\n",
    "\n",
    "Давайте сначала создадим экземпляр загрузчика данных из раздела 2.6 «Выборка данных со скользящим окном»:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, тензор идентификатора токена имеет размер 8x4, что означает, что пакет данных состоит из 8 образцов текста по 4 токена в каждом.\n",
    "\n",
    "Давайте теперь воспользуемся слоем внедрения, чтобы встроить эти идентификаторы токенов в 256-мерные векторы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы можем сказать на основе выходных данных тензора размером 8x4x256, каждый идентификатор токена теперь внедрен как 256-мерный вектор.\n",
    "\n",
    "Для подхода абсолютного внедрения модели GPT нам просто нужно создать еще один слой внедрения, который имеет то же измерение, что и token_embedding_layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как показано в предыдущем примере кода, входными данными для pos_embeddings обычно является вектор-заполнитель torch.arange(context_length), который содержит последовательность чисел 0, 1,..., вплоть до максимальной входной длины — 1. context_length равна переменная, которая представляет поддерживаемый входной размер LLM. Здесь мы выбираем его аналогично максимальной длине входного текста. На практике входной текст может быть длиннее, чем поддерживаемая длина контекста, и в этом случае нам придется обрезать текст."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, тензор позиционного вложения состоит из четырех 256-мерных векторов. Теперь мы можем добавить их непосредственно во встраивания токенов, где PyTorch добавит 4x256-мерный тензор pos_embeddings к каждому 4x256-мерному тензору встраивания токенов в каждом из 8 пакетов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданные нами input_embeddings, как показано на рис. 2.19, представляют собой примеры встроенных входных данных, которые теперь могут обрабатываться основными модулями LLM, реализацию которых мы начнем в главе 3.\n",
    "\n",
    "Рисунок 2.19. В рамках конвейера обработки ввода входной текст сначала разбивается на отдельные токены. Эти токены затем преобразуются в идентификаторы токенов с использованием словаря. Идентификаторы токенов преобразуются в векторы внедрения, к которым добавляются позиционные внедрения аналогичного размера, в результате чего входные внедрения используются в качестве входных данных для основных слоев LLM.\n",
    "![](images/llm2.18.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9 Резюме\n",
    "\n",
    "- LLM требуют преобразования текстовых данных в числовые векторы, известные как представления, поскольку они не могут обрабатывать необработанный текст. Представления преобразуют дискретные данные (например, слова или изображения) в непрерывные векторные пространства, делая их совместимыми с тензорными операциями.\n",
    "- На первом этапе необработанный текст разбивается на токены, которые могут быть словами или символами. Затем токены преобразуются в целочисленные представления, называемые идентификаторами. Специальные токены, такие как <|unk|> и <|endoftext|>, можно добавлять для улучшения понимания модели и обработки различных контекстов, например неизвестных слов или обозначения границы между несвязанными текстами.\n",
    "- Токенизатор кодирования по парам байтов (BPE) может эффективно обрабатывать неизвестные слова, разбивая их на подслова или отдельные символы.\n",
    "- Мы используем подход скользящего окна для токенизированных данных для генерации пар входных данных для обучения LLM.\n",
    "- Встраивание слоев в PyTorch выполняет функцию поиска, получая векторы, соответствующие идентификаторам токенов. Полученные векторы внедрения обеспечивают непрерывное представление токенов, что имеет решающее значение для обучения моделей глубокого обучения, таких как LLM.\n",
    "Хотя встраивания токенов обеспечивают согласованные векторные представления для каждого токена, им не хватает понимания положения токена в последовательности. Чтобы исправить это, существуют два основных типа позиционных вложений: абсолютное и относительное. Модели OpenAI GPT используют абсолютные позиционные внедрения, которые добавляются к векторам внедрения токенов и оптимизируются во время обучения модели."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
