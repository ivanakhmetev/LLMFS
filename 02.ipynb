{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Обработка текста\n",
    "\n",
    "В предыдущей главе мы углубились в структуру больших языковых моделей (LLM) и узнали, что они предварительно обучаются на огромных объемах текста, обрабатывая\n",
    "по одному слову (токену) за раз. \n",
    "\n",
    "![](images/llm2.1.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Векторное представление (vector embedding)\n",
    "\n",
    "Нейросетевые модели, не могут напрямую обрабатывать текст. Текст как тип данных несовместим с тензорными операциями. Следовательно, нужен способ представлять слова в виде векторов с непрерывными значениями. (ссылка узнать больше в Приложении A, раздел A2.2 «Понимание тензоров».)\n",
    "\n",
    "Построение векторного представления может быть проведено разными способами (различные модели представления). \n",
    "Прочие типы данных также требуют преобразования в тензорный формат.  \n",
    "\n",
    "![](images/llm2.2.png)\n",
    "\n",
    "Существует несколько алгоритмов для создания представлений слов. Одним из первых и наиболее популярных примеров является Word2Vec. Основная идея заключается в том, что слова, встречающиеся в схожем контексте, имеют схожее значение. Следовательно, при проецировании в двумерное пространство, можно увидеть, что схожие термины группируются вместе.\n",
    "\n",
    "![](images/llm2.3.png)\n",
    "\n",
    "Помимо моделей представления слов, существуют модели представления предложений, абзацев, целых документов.\n",
    "Таким образом, картинка из предыдущей главы (predicting next world) не вполне корректна. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя мы можем использовать существующие модели, LLM обычно создают свои собственные представления, которые являются частью входного слоя и обновляются во время обучения. Преимущество такого подхода - оптимизация для конкретной задачи и имеющихся данных. \n",
    "\n",
    "Размерность представления (двумерное на рисунке выше) может быть произвольной. Большая размерность может отражать более тонкие отношения, но за счет вычислительной эффективности. Эта размерность называется размерностью скрытых состояний модели (dimensionality of the model's hidden states). \n",
    "Эта размерность - компромисс между производительностью и эффективностью. \n",
    "GPT-2 (125M параметров) используют размерность 768. \n",
    "GPT-3 (175B параметров) использует размерность 12 288."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Токенизация текста\n",
    "\n",
    "![](images/llm2.4.png)\n",
    "\n",
    "Текст, который мы будем токенизировать для обучения LLM, представляет собой рассказ Эдит Уортон под названием «Вердикт». Текст доступен в Wikisource по адресу https://en.wikisource.org/wiki/The_Verdict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# Чтение короткого рассказа в виде образца текста на Python\n",
    "\n",
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как нам лучше всего разделить этот текст, чтобы получить список токенов? \n",
    "\n",
    "Используя простой пример текста, мы можем использовать команду re.split со следующим синтаксисом, чтобы разделить текст на пробельные символы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Изменим разделение регулярных выражений на пробелы (\\s), запятые и точки ([,.]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробелы и прочие специальные символы. При разработке простого токенизатора, следует ли нам кодировать пробелы как отдельные символы или просто удалять их, зависит от нашего приложения и его требований. Удаление пробелов снижает требования к памяти и вычислительным ресурсам. Однако сохранение пробелов может быть полезно, если мы обучаем модели, чувствительные к точной структуре текста (например, код Python, чувствительный к отступам и интервалам). Здесь мы удаляем пробелы для простоты и краткости токенизированных результатов. Позже мы переключимся на схему токенизации, включающую пробелы.\n",
    "\n",
    "Изменим код, чтобы он мог обрабатывать и другие типы пунктуации, такие как вопросительные знаки, кавычки и двойные тире, которые мы видели ранее в первых 100 символах рассказа Эдит Уортон, а также дополнительные специальные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша модель токенизации теперь может успешно обрабатывать различные специальные символы в тексте.\n",
    "\n",
    "![](images/llm2.5.png)\n",
    "\n",
    "Теперь, когда у нас есть работающий базовый токенизатор, применим его ко всему рассказу Эдит Уортон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Преобразование токенов в идентификаторы\n",
    "\n",
    "Это преобразование является промежуточным шагом перед преобразованием в векторное представление.\n",
    "\n",
    "Чтобы сопоставить ранее сгенерированные токены с идентификаторами, нужно создать словарь. Этот словарь определяет, как мы сопоставляем каждое уникальное слово и специальный символ с уникальным целым числом, как показано на рисунке. Мы создаем словарь, разбивая весь текст обучающего набора данных на отдельные токены. Эти отдельные токены затем сортируются в алфавитном порядке, а повторяющиеся токены удаляются. Затем уникальные токены объединяются в словарь, который определяет сопоставление каждого уникального токена с уникальным целочисленным значением. \n",
    "\n",
    "![](images/llm2.6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n",
      "('Her', 51)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, словарь содержит отдельные токены, связанные с уникальными целочисленными метками.\n",
    "Наша следующая цель — применить этот словарь для преобразования нового текста в идентификаторы токенов, как показано на рисунке.\n",
    "\n",
    "![](images/llm2.7.png)\n",
    "\n",
    "\n",
    "Далее, когда мы захотим преобразовать выходные данные LLM из чисел обратно в текст, нам также понадобится способ превратить идентификаторы в текст. Для этого мы можем создать обратную версию словаря, которая сопоставляет идентификаторы с соответствующими текстовыми токенами.\n",
    "\n",
    "Реализуем полный класс токенизатора и детокенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "        \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    " \n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 Добавление специальных токенов контекста\n",
    "\n",
    "Изменим токенизатор для обработки неизвестных слов. Также обсудим использование и добавление специальных токенов контекста, которые могут улучшить понимание моделью контекста или другой соответствующей информации в тексте. Эти специальные токены могут включать, например, маркеры для неизвестных слов и границ документа.\n",
    "\n",
    "В частности, мы изменим словарь и токенизатор для поддержки двух новых токенов: <|unk|> и <|endoftext|>\n",
    "\n",
    "![](images/llm2.9.png)\n",
    "\n",
    "Как показано на рисунке, мы можем изменить токенизатор, чтобы он использовал токен <|unk|>, если он встретит слово, которое не является частью словаря. Кроме того, мы добавляем токен между несвязанными текстами. Например, при обучении GPT-подобных LLM на нескольких независимых документах или книгах обычно вставляется токен перед каждым документом или книгой, следующим за предыдущим источником текста Это помогает LLM понять, что, хотя эти текстовые источники объединены для обучения, на самом деле они не связаны друг с другом.\n",
    "\n",
    "![](images/llm2.10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n",
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    " \n",
    "print(len(vocab.items()))\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    " \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    " \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы видим, что идентификатор 1159 соответствует токену-разделителю <|endoftext|>, а идентификатор 1160 используются для неизвестных слов.\n",
    "\n",
    "Сравнивая приведенный выше детокенизированный текст с исходным входным текстом, мы знаем, что набор обучающих данных, рассказ Эдит Уортон «Вердикт», не содержал слов «Привет» и «дворец».\n",
    "\n",
    "Также рассматривают дополнительные специальные токены, такие как следующие:\n",
    "\n",
    "[BOS] (начало последовательности): этот токен отмечает начало текста.   \n",
    "[EOS] (конец последовательности): этот токен располагается в конце текста. Например, при объединении двух разных статей или книг Википедии токен [EOS] указывает, где заканчивается одна статья и начинается следующая.  \n",
    "[PAD] (заполнение): при обучении LLM с размерами пакетов больше единицы пакет может содержать тексты различной длины. Чтобы гарантировать одинаковую длину всех текстов, более короткие тексты расширяются или «дополняются» с помощью токена [PAD] до длины самого длинного текста в пакете.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Кодирование пар байтов (Byte pair encoding)\n",
    "\n",
    "В этом разделе рассматривается более сложная схема токенизации, основанная на концепции, называемой кодированием пар байтов (BPE). Такой токенизатор использовался для обучения LLM, таких как GPT-2, GPT-3 и исходной модели, используемой в ChatGPT.\n",
    "\n",
    "Поскольку реализация BPE может быть сложной, мы будем использовать существующую библиотеку Python с открытым исходным кодом под названием tiktoken (https://github.com/openai/tiktoken), которая очень эффективно реализует алгоритм BPE на основе исходного кода на Rust. Как и другие библиотеки Python, мы можем установить библиотеку tiktoken через установщик pip Python из терминала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.6.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе идентификаторов токенов и декодированного текста выше мы можем сделать два примечательных наблюдения. Во-первых, токену <|endoftext|> присваивается относительно большой идентификатор токена, а именно 50256. Фактически, токенизатор BPE, который использовался для обучения таких моделей, как GPT-2, GPT-3, и исходная модель, используемая в ChatGPT имеет общий размер словаря 50 257, причем <|endoftext|> присвоен самый большой идентификатор токена.\n",
    "\n",
    "Во-вторых, токенизатор BPE, указанный выше, правильно кодирует и декодирует неизвестные слова, такие как «someunknownPlace». Токенизатор BPE может обрабатывать любое неизвестное слово. Как это достигается без использования токенов <|unk|>?\n",
    "\n",
    "Алгоритм, лежащий в основе BPE, разбивает слова, которых нет в его предопределенном словаре, на более мелкие подслова или даже отдельные символы, что позволяет ему обрабатывать слова, которых нет в словаре. Таким образом, благодаря алгоритму BPE, если токенизатор встречает незнакомое слово во время токенизации, он может представить его как последовательность токенов или символов подслова, как показано на рисунке.\n",
    "\n",
    "![](images/llm2.11.png)\n",
    "\n",
    "Подробное обсуждение и реализация BPE выходит за рамки этой книги, но, если коротко, словарь строится путем итеративного объединения часто встречающихся символов в подслова, а часто встречающихся подслов — в слова. Например, BPE начинает с добавления в свой словарь всех отдельных одиночных символов («a», «b», ...). На следующем этапе он объединяет комбинации символов, которые часто встречаются вместе, в подслова. Например, «d» и «e» могут быть объединены в подслово «de», которое часто встречается во многих английских словах, таких как «define», «dependent», «made» и «hidden»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Выборка данных с помощью скользящего окна\n",
    "\n",
    "Следующим шагом является создание пар вход-выход, необходимых для обучения LLM.\n",
    "Как выглядят эти пары вход-выход? \n",
    "\n",
    "![](images/llm2.12.png)\n",
    "\n",
    "Реализуем теперь загрузчик данных, который извлекает пары вход-вызод из набора обучающих данных, используя подход скользящего окна.\n",
    "\n",
    "Для чего токенизируем весь рассказ The Verdict, с которым мы работали ранее, с помощью токенизатора BPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из самых простых и интуитивно понятных способов создания пар входных данных для задачи прогнозирования следующего слова — это создание двух переменных, x и y, где x содержит входные токены, а y содержит целевые значения, которые представляют собой входные данные, сдвинутые на 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрабатывая входные данные вместе с целевыми объектами, которые представляют собой входные данные, сдвинутые на одну позицию, мы можем затем создать задачи прогнозирования следующего слова, следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все, что находится слева от стрелки (---->), относится к входным данным, которые LLM получит, а идентификатор токена в правой части стрелки представляет собой целевой идентификатор токена, который LLM должен предсказать.\n",
    "\n",
    "В целях иллюстрации давайте повторим предыдущий код, но преобразуем идентификаторы токенов в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы создали пары вход-цель, которые можно использовать для обучения LLM в следующих главах.\n",
    "\n",
    "Прежде чем мы сможем превратить токены во встраивания, есть еще только одна задача, как мы упоминали в начале этой главы: реализация эффективного загрузчика данных, который выполняет итерацию по входному набору данных и возвращает входные и целевые значения в виде тензоров PyTorch, что можно представить себе как как многомерные массивы.\n",
    "\n",
    "В частности, мы заинтересованы в возврате двух тензоров: входного, содержащего текст, который видит LLM, и целевого тензора, который включает в себя цели, которые LLM должен предсказать, как показано на рисунке 2.13.\n",
    "\n",
    "Рисунок 2.13. Чтобы реализовать эффективные загрузчики данных, мы собираем входные данные в тензор x, где каждая строка представляет один входной контекст. Второй тензор y содержит соответствующие цели прогнозирования (следующие слова), которые создаются путем сдвига входных данных на одну позицию.\n",
    "\n",
    "![](images/llm2.13.png)\n",
    "\n",
    "Хотя на рисунке 2.13 для иллюстрации токены показаны в строковом формате, реализация кода будет работать непосредственно с идентификаторами токенов, поскольку метод кодирования токенизатора BPE выполняет как токенизацию, так и преобразование в идентификаторы токенов за один шаг.\n",
    "\n",
    "Для эффективной реализации загрузчика данных мы будем использовать встроенные классы PyTorch Dataset и DataLoader. Дополнительную информацию и рекомендации по установке PyTorch см. в разделе A.1.3 «Установка PyTorch» в Приложении A.\n",
    "\n",
    "Код класса набора данных показан в листинге 2.5:\n",
    "\n",
    "Листинг 2.5. Набор данных для пакетных входных и целевых значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    " \n",
    "        token_ids = tokenizer.encode(txt)\n",
    " \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс GPTDatasetV1 в листинге 2.5 основан на классе набора данных PyTorch и определяет, как отдельные строки извлекаются из набора данных, где каждая строка состоит из нескольких идентификаторов токенов (на основе max_length), назначенных тензору input_chunk. Тензор target_chunk содержит соответствующие цели. Я рекомендую прочитать дальше, чтобы увидеть, как выглядят данные, возвращаемые из этого набора данных, когда мы объединяем набор данных с PyTorch DataLoader — это принесет дополнительную интуицию и ясность.\n",
    "\n",
    "Если вы впервые знакомы со структурой классов набора данных PyTorch, например, показанной в листинге 2.5, прочтите раздел A.6 «Настройка эффективных загрузчиков данных» в приложении A, в котором объясняется общая структура и использование классов PyTorch Dataset и DataLoader.\n",
    "\n",
    "Следующий код будет использовать GPTDatasetV1 для пакетной загрузки входных данных через PyTorch DataLoader:\n",
    "\n",
    "Листинг 2.6. Загрузчик данных для генерации пакетов с парами вход-с"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, \n",
    "        max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте протестируем загрузчик данных с размером пакета 1 для LLM с размером контекста 4, чтобы понять, как класс GPTDatasetV1 из листинга 2.5 и функция create_dataloader_v1 из листинга 2.6 работают вместе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_338334/3364176621.py:15: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  self.input_ids.append(torch.tensor(input_chunk))\n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переменная first_batch содержит два тензора: первый тензор хранит идентификаторы входных токенов, а второй тензор хранит идентификаторы целевых токенов. Поскольку max_length установлено равным 4, каждый из двух тензоров содержит 4 идентификатора токена. Обратите внимание, что входной размер 4 относительно мал и выбран только для иллюстрации. Обычно LLM обучают с размером входных данных не менее 256.\n",
    "\n",
    "Чтобы проиллюстрировать значение шага=1, давайте возьмем еще один пакет из этого набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы сравним первый и второй пакет, мы увидим, что идентификаторы токенов второго пакета сдвинуты на одну позицию по сравнению с первым пакетом (например, второй идентификатор во входных данных первого пакета равен 367, что является первым идентификатором вход второго пакета). Настройка шага определяет количество позиций, на которые смещаются входные данные между пакетами, имитируя подход скользящего окна, как показано на рисунке 2.14.\n",
    "\n",
    "Рисунок 2.14. При создании нескольких пакетов из входного набора данных мы перемещаем окно ввода по тексту. Если шаг установлен на 1, мы сдвигаем окно ввода на 1 позицию при создании следующего пакета. Если мы установим шаг равным размеру окна ввода, мы сможем предотвратить перекрытие между пакетами.\n",
    "\n",
    "![](images/llm2.14.png)\n",
    "\n",
    "\n",
    "Упражнение 2.2. Загрузчики данных с разными шагами и размерами контекста.\n",
    "\n",
    "Чтобы лучше понять, как работает загрузчик данных, попробуйте запустить его с различными настройками, такими как max_length=2 и шаг=2 и max_length=8 и шаг=2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Размеры пакетов, равные 1, например, которые мы до сих пор выбирали из загрузчика данных, полезны для иллюстративных целей. Если у вас есть предыдущий опыт глубокого обучения, вы, возможно, знаете, что небольшие размеры пакетов требуют меньше памяти во время обучения, но приводят к более шумным обновлениям модели. Как и в обычном глубоком обучении, размер пакета — это компромисс и гиперпараметр, с которым можно экспериментировать при обучении LLM.\n",
    "\n",
    "Прежде чем мы перейдем к двум последним разделам этой главы, посвященным созданию векторов внедрения из идентификаторов токенов, давайте кратко рассмотрим, как мы можем использовать загрузчик данных для выборки с размером пакета больше 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[37733,  2346,   284,   884],\n",
      "        [  764,   764,   764,   198],\n",
      "        [  286,   511,   512,  1741],\n",
      "        [ 1781,   314,  1422,   470],\n",
      "        [  438, 14363, 10568,   852],\n",
      "        [  502,    11,   290,   284],\n",
      "        [  674,  1611,   286,  1242],\n",
      "        [  766,   326,   314,  3521]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 2346,   284,   884, 14177],\n",
      "        [  764,   764,   198,   198],\n",
      "        [  511,   512,  1741,    13],\n",
      "        [  314,  1422,   470,  1560],\n",
      "        [14363, 10568,   852,  5729],\n",
      "        [   11,   290,   284,  3285],\n",
      "        [ 1611,   286,  1242,   526],\n",
      "        [  326,   314,  3521,   470]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
    " \n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы увеличиваем шаг до 4. Это необходимо для полного использования набора данных (мы не пропускаем ни одного слова), но также для избежания любого перекрытия между пакетами, поскольку большее перекрытие может привести к увеличению переобучения.\n",
    "\n",
    "В последних двух разделах этой главы мы реализуем уровни внедрения, которые преобразуют идентификаторы токенов в непрерывные векторные представления, которые служат форматом входных данных для LLM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.7 Creating token embeddings\n",
    "\n",
    "The last step for preparing the input text for LLM training is to convert the token IDs into embedding vectors, as illustrated in Figure 2.15, which will be the focus of these two last remaining sections of this chapter.\n",
    "\n",
    "Figure 2.15 Preparing the input text for an LLM involves tokenizing text, converting text tokens to token IDs, and converting token IDs into vector embedding vectors. In this section, we consider the token IDs created in previous sections to create the token embedding vectors.\n",
    "\n",
    "\n",
    "\n",
    "In addition to the processes outlined in Figure 2.15, it is important to note that we initialize these embedding weights with random values as a preliminary step. This initialization serves as the starting point for the LLM's learning process. We will optimize the embedding weights as part of the LLM training in chapter 5.\n",
    "\n",
    "A continuous vector representation, or embedding, is necessary since GPT-like LLMs are deep neural networks trained with the backpropagation algorithm. If you are unfamiliar with how neural networks are trained with backpropagation, please read section A.4, Automatic differentiation made easy, in Appendix A.\n",
    "\n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 123 for reproducibility purposes:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "The print statement in the preceding code example prints the embedding layer's underlying weight matrix:\n",
    "\n",
    "Parameter containing:\n",
    "tensor([[ 0.3374, -0.1778, -0.1690],\n",
    "        [ 0.9178,  1.5810,  1.3010],\n",
    "        [ 1.2753, -0.2010, -0.1606],\n",
    "        [-0.4015,  0.9666, -1.1481],\n",
    "        [-1.1589,  0.3255, -0.6315],\n",
    "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "We can see that the weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself, as we will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions.\n",
    "\n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the embedding vector:\n",
    "\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "\n",
    "The returned embedding vector is as follows:\n",
    "\n",
    "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
    "\n",
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row (Python starts with a zero index, so it's the row corresponding to index 3). In other words, the embedding layer is essentially a look-up operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n",
    "\n",
    "\n",
    "Embedding layers versus matrix multiplication\n",
    "\n",
    "For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully connected layer, which is illustrated in the supplementary code on GitHub at https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/03_bonus_embedding-vs-matmul. Because the embedding layer is just a more efficient implementation equivalent to the one-hot encoding and matrix-multiplication approach, it can be seen as a neural network layer that can be optimized via backpropagation.\n",
    "\n",
    "Previously, we have seen how to convert a single token ID into a three-dimensional embedding vector. Let's now apply that to all four input IDs we defined earlier (torch.tensor([2, 3, 5, 1])):\n",
    "\n",
    "print(embedding_layer(input_ids))\n",
    "\n",
    "The print output reveals that this results in a 4x3 matrix:\n",
    "\n",
    "tensor([[ 1.2753, -0.2010, -0.1606],\n",
    "        [-0.4015,  0.9666, -1.1481],\n",
    "        [-2.8400, -0.7849, -1.4096],\n",
    "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n",
    "\n",
    "Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix, as illustrated in Figure 2.16.\n",
    "\n",
    "Figure 2.16 Embedding layers perform a look-up operation, retrieving the embedding vector corresponding to the token ID from the embedding layer's weight matrix. For instance, the embedding vector of the token ID 5 is the sixth row of the embedding layer weight matrix (it is the sixth instead of the fifth row because Python starts counting at 0). For illustration purposes, we assume that the token IDs were produced by the small vocabulary we used in section 2.3.\n",
    "\n",
    "This section covered how we create embedding vectors from token IDs. The next and final section of this chapter will add a small modification to these embedding vectors to encode positional information about a token within a text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Создание вложений токенов\n",
    "\n",
    "Последним шагом подготовки входного текста для обучения LLM является преобразование идентификаторов токенов в векторы внедрения, как показано на рисунке 2.15, который будет в центре внимания двух последних оставшихся разделов этой главы.\n",
    "\n",
    "Рисунок 2.15. Подготовка входного текста для LLM включает токенизацию текста, преобразование текстовых токенов в идентификаторы токенов и преобразование идентификаторов токенов в векторы векторного внедрения. В этом разделе мы рассматриваем идентификаторы токенов, созданные в предыдущих разделах, для создания векторов внедрения токенов.\n",
    "\n",
    "![](images/llm2.15.png)\n",
    "\n",
    "\n",
    "\n",
    "В дополнение к процессам, показанным на рисунке 2.15, важно отметить, что мы инициализируем эти веса внедрения случайными значениями в качестве предварительного шага. Эта инициализация служит отправной точкой процесса обучения LLM. Мы оптимизируем веса внедрения в рамках обучения LLM в главе 5.\n",
    "\n",
    "Непрерывное векторное представление или встраивание необходимо, поскольку GPT-подобные LLM представляют собой глубокие нейронные сети, обученные с помощью алгоритма обратного распространения ошибки. Если вы не знакомы с тем, как нейронные сети обучаются с помощью обратного распространения ошибки, прочтите раздел A.4 «Легкая автоматическая дифференциация» в Приложении A.\n",
    "\n",
    "Давайте проиллюстрируем, как работает векторное преобразование идентификатора токена для внедрения, на практическом примере. Предположим, у нас есть следующие четыре входных токена с идентификаторами 2, 3, 5 и 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "#For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты и иллюстрации предположим, что у нас есть небольшой словарь, состоящий всего из 6 слов (вместо 50 257 слов в словаре токенизатора BPE), и мы хотим создать вложения размером 3 (в GPT-3 размер встраивания составляет 12 288 измерений):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя vocab_size и output_dim, мы можем создать экземпляр слоя внедрения в PyTorch, установив случайное начальное значение 123 в целях воспроизводимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что весовая матрица слоя внедрения содержит небольшие случайные значения. Эти значения оптимизируются во время обучения LLM как часть самой оптимизации LLM, как мы увидим в следующих главах. Более того, мы видим, что матрица весов имеет шесть строк и три столбца. В словаре имеется одна строка для каждого из шести возможных токенов. И есть один столбец для каждого из трех измерений внедрения.\n",
    "\n",
    "После того как мы создали экземпляр слоя внедрения, давайте теперь применим его к идентификатору токена, чтобы получить вектор внедрения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы сравним вектор внедрения для токена с идентификатором 3 с предыдущей матрицей внедрения, мы увидим, что он идентичен 4-й строке (Python начинается с нулевого индекса, поэтому эта строка соответствует индексу 3). Другими словами, уровень внедрения — это, по сути, операция поиска, которая извлекает строки из весовой матрицы слоя внедрения через идентификатор токена.\n",
    "\n",
    "\n",
    "Встраивание слоев в сравнении с умножением матриц\n",
    "\n",
    "Для тех, кто знаком с горячим кодированием, описанный выше подход уровня внедрения — это, по сути, просто более эффективный способ реализации горячего кодирования с последующим умножением матриц на полностью связном уровне, что проиллюстрировано в дополнительном коде на GitHub по адресу https. ://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/03_bonus_embedding-vs-matmul. Поскольку уровень внедрения — это просто более эффективная реализация, эквивалентная подходу «горячего кодирования» и умножения матриц, его можно рассматривать как уровень нейронной сети, который можно оптимизировать с помощью обратного распространения ошибки.\n",
    "\n",
    "Ранее мы видели, как преобразовать одиночный идентификатор токена в трехмерный вектор внедрения. Давайте теперь применим это ко всем четырем входным идентификаторам, которые мы определили ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#(torch.tensor([2, 3, 5, 1])):\n",
    "\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая строка в этой выходной матрице получается посредством операции поиска из матрицы весов внедрения, как показано на рисунке 2.16.\n",
    "\n",
    "Рисунок 2.16. Уровни внедрения выполняют операцию поиска, извлекая вектор внедрения, соответствующий идентификатору токена, из весовой матрицы уровня внедрения. Например, вектор внедрения токена с идентификатором 5 — это шестая строка матрицы весов слоя внедрения (это шестая строка вместо пятой, поскольку Python начинает отсчет с 0). В целях иллюстрации мы предполагаем, что идентификаторы токенов были созданы с помощью небольшого словаря, который мы использовали в разделе 2.3.\n",
    "![](images/llm2.16.png)\n",
    "\n",
    "\n",
    "В этом разделе описано, как мы создаем векторы внедрения на основе идентификаторов токенов. В следующем и последнем разделе этой главы к этим векторам внедрения будет добавлена ​​небольшая модификация для кодирования позиционной информации о токене в тексте."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8 Кодирование позиций слов\n",
    "\n",
    "В предыдущем разделе мы преобразовали идентификаторы токенов в непрерывное векторное представление, так называемые встраивания токенов. В принципе, это подходящий ввод для LLM. Однако незначительным недостатком LLM является то, что их механизм самообслуживания, который будет подробно описан в главе 3, не имеет представления о положении или порядке токенов внутри последовательности.\n",
    "\n",
    "Принцип работы ранее представленного уровня внедрения заключается в том, что один и тот же идентификатор токена всегда отображается в одно и то же векторное представление, независимо от того, где идентификатор токена расположен во входной последовательности, как показано на рисунке 2.17.\n",
    "\n",
    "Рисунок 2.17. Уровень внедрения преобразует идентификатор токена в одно и то же векторное представление независимо от того, где он расположен во входной последовательности. Например, идентификатор токена 5, независимо от того, находится ли он в первой или третьей позиции во входном векторе идентификатора токена, приведет к одному и тому же вектору внедрения.\n",
    "\n",
    "\n",
    "![](images/llm2.17.png)\n",
    "\n",
    "\n",
    "\n",
    "В принципе, детерминированное, независимое от позиции внедрение идентификатора токена полезно для целей воспроизводимости. Однако, поскольку механизм самообслуживания LLM сам по себе также не зависит от позиции, полезно вводить дополнительную информацию о местоположении в LLM.\n",
    "\n",
    "Для достижения этой цели существуют две широкие категории вложений с учетом позиции: относительные позиционные вложения и абсолютные позиционные вложения.\n",
    "\n",
    "Абсолютные позиционные вложения напрямую связаны с конкретными позициями в последовательности. Для каждой позиции во входной последовательности к внедрению токена добавляется уникальное вложение, чтобы передать его точное местоположение. Например, первый токен будет иметь определенное позиционное вложение, второй токен — другое отдельное вложение и т. д., как показано на рисунке 2.18.\n",
    "\n",
    "\n",
    "Рисунок 2.18. Позиционные внедрения добавляются к вектору внедрения токена для создания входных внедрений для LLM. Позиционные векторы имеют ту же размерность, что и исходные вложения токенов. Внедрения токенов для простоты показаны со значением 1.\n",
    "\n",
    "\n",
    "?????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя указанный выше token_embedding_layer, если мы выбираем данные из загрузчика данных, мы встраиваем каждый токен в каждом пакете в 256-мерный вектор. Если у нас размер пакета 8 по четыре токена в каждом, результатом будет тензор 8 x 4 x 256.\n",
    "\n",
    "Давайте сначала создадим экземпляр загрузчика данных из раздела 2.6 «Выборка данных со скользящим окном»:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, тензор идентификатора токена имеет размер 8x4, что означает, что пакет данных состоит из 8 образцов текста по 4 токена в каждом.\n",
    "\n",
    "Давайте теперь воспользуемся слоем внедрения, чтобы встроить эти идентификаторы токенов в 256-мерные векторы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы можем сказать на основе выходных данных тензора размером 8x4x256, каждый идентификатор токена теперь внедрен как 256-мерный вектор.\n",
    "\n",
    "Для подхода абсолютного внедрения модели GPT нам просто нужно создать еще один слой внедрения, который имеет то же измерение, что и token_embedding_layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context_lengthe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m context_length \u001b[39m=\u001b[39m max_length\n\u001b[0;32m----> 2\u001b[0m pos_embedding_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mEmbedding(context_lengthe, output_dim)\n\u001b[1;32m      3\u001b[0m pos_embeddings \u001b[39m=\u001b[39m pos_embedding_layer(torch\u001b[39m.\u001b[39marange(context_length))\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(pos_embeddings\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'context_lengthe' is not defined"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_lengthe, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как показано в предыдущем примере кода, входными данными для pos_embeddings обычно является вектор-заполнитель torch.arange(context_length), который содержит последовательность чисел 0, 1,..., вплоть до максимальной входной длины — 1. context_length равна переменная, которая представляет поддерживаемый входной размер LLM. Здесь мы выбираем его аналогично максимальной длине входного текста. На практике входной текст может быть длиннее, чем поддерживаемая длина контекста, и в этом случае нам придется обрезать текст."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, тензор позиционного вложения состоит из четырех 256-мерных векторов. Теперь мы можем добавить их непосредственно во встраивания токенов, где PyTorch добавит 4x256-мерный тензор pos_embeddings к каждому 4x256-мерному тензору встраивания токенов в каждом из 8 пакетов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданные нами input_embeddings, как показано на рис. 2.19, представляют собой примеры встроенных входных данных, которые теперь могут обрабатываться основными модулями LLM, реализацию которых мы начнем в главе 3.\n",
    "\n",
    "Рисунок 2.19. В рамках конвейера обработки ввода входной текст сначала разбивается на отдельные токены. Эти токены затем преобразуются в идентификаторы токенов с использованием словаря. Идентификаторы токенов преобразуются в векторы внедрения, к которым добавляются позиционные внедрения аналогичного размера, в результате чего входные внедрения используются в качестве входных данных для основных слоев LLM.\n",
    "![](images/llm2.18.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9 Резюме\n",
    "\n",
    "LLM требуют преобразования текстовых данных в числовые векторы, известные как внедрения, поскольку они не могут обрабатывать необработанный текст. Вложения преобразуют дискретные данные (например, слова или изображения) в непрерывные векторные пространства, делая их совместимыми с операциями нейронных сетей.\n",
    "На первом этапе необработанный текст разбивается на токены, которые могут быть словами или символами. Затем токены преобразуются в целочисленные представления, называемые идентификаторами токенов.\n",
    "Специальные токены, такие как <|unk|> и <|endoftext|>, можно добавлять для улучшения понимания модели и обработки различных контекстов, например неизвестных слов или обозначения границы между несвязанными текстами.\n",
    "Токенизатор кодирования пар байтов (BPE), используемый для LLM, таких как GPT-2 и GPT-3, может эффективно обрабатывать неизвестные слова, разбивая их на подслова или отдельные символы.\n",
    "Мы используем подход скользящего окна для токенизированных данных для генерации пар входных данных для обучения LLM.\n",
    "Встраивание слоев в PyTorch выполняет функцию поиска, получая векторы, соответствующие идентификаторам токенов. Полученные векторы внедрения обеспечивают непрерывное представление токенов, что имеет решающее значение для обучения моделей глубокого обучения, таких как LLM.\n",
    "Хотя встраивания токенов обеспечивают согласованные векторные представления для каждого токена, им не хватает понимания положения токена в последовательности. Чтобы исправить это, существуют два основных типа позиционных вложений: абсолютное и относительное. Модели OpenAI GPT используют абсолютные позиционные внедрения, которые добавляются к векторам внедрения токенов и оптимизируются во время обучения модели."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
