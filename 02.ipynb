{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Обработка текста\n",
    "\n",
    "В предыдущей главе мы углубились в структуру больших языковых моделей (LLM) и узнали, что они предварительно обучаются на огромных объемах текста, обрабатывая\n",
    "по одному слову (токену) за раз. \n",
    "\n",
    "![](images/llm2.1.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Векторное представление (vector embedding)\n",
    "\n",
    "Нейросетевые модели, не могут напрямую обрабатывать текст. Текст как тип данных несовместим с тензорными операциями. Следовательно, нужен способ представлять слова в виде векторов с непрерывными значениями. (ссылка узнать больше в Приложении A, раздел A2.2 «Понимание тензоров».)\n",
    "\n",
    "Построение векторного представления может быть проведено разными способами (различные модели представления). \n",
    "Прочие типы данных также требуют преобразования в тензорный формат.  \n",
    "\n",
    "![](images/llm2.2.png)\n",
    "\n",
    "Существует несколько алгоритмов для создания представлений слов. Одним из первых и наиболее популярных примеров является Word2Vec. Основная идея заключается в том, что слова, встречающиеся в схожем контексте, имеют схожее значение. Следовательно, при проецировании в двумерное пространство, можно увидеть, что схожие термины группируются вместе.\n",
    "\n",
    "![](images/llm2.3.png)\n",
    "\n",
    "Помимо моделей представления слов, существуют модели представления предложений, абзацев, целых документов.\n",
    "Таким образом, картинка из предыдущей главы (predicting next world) не вполне корректна. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя мы можем использовать существующие модели, LLM обычно создают свои собственные представления, которые являются частью входного слоя и обновляются во время обучения. Преимущество такого подхода - оптимизация для конкретной задачи и имеющихся данных. \n",
    "\n",
    "Размерность представления (двумерное на рисунке выше) может быть произвольной. Большая размерность может отражать более тонкие отношения, но за счет вычислительной эффективности. Эта размерность называется размерностью скрытых состояний модели (dimensionality of the model's hidden states). \n",
    "Эта размерность - компромисс между производительностью и эффективностью. \n",
    "GPT-2 (125M параметров) используют размерность 768. \n",
    "GPT-3 (175B параметров) использует размерность 12 288."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Токенизация текста\n",
    "\n",
    "![](images/llm2.4.png)\n",
    "\n",
    "Текст, который мы будем токенизировать для обучения LLM, представляет собой рассказ Эдит Уортон под названием «Вердикт». Текст доступен в Wikisource по адресу https://en.wikisource.org/wiki/The_Verdict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# Чтение короткого рассказа в виде образца текста на Python\n",
    "\n",
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как нам лучше всего разделить этот текст, чтобы получить список токенов? \n",
    "\n",
    "Используя простой пример текста, мы можем использовать команду re.split со следующим синтаксисом, чтобы разделить текст на пробельные символы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Изменим разделение регулярных выражений на пробелы (\\s), запятые и точки ([,.]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробелы и прочие специальные символы. При разработке простого токенизатора, следует ли нам кодировать пробелы как отдельные символы или просто удалять их, зависит от нашего приложения и его требований. Удаление пробелов снижает требования к памяти и вычислительным ресурсам. Однако сохранение пробелов может быть полезно, если мы обучаем модели, чувствительные к точной структуре текста (например, код Python, чувствительный к отступам и интервалам). Здесь мы удаляем пробелы для простоты и краткости токенизированных результатов. Позже мы переключимся на схему токенизации, включающую пробелы.\n",
    "\n",
    "Изменим код, чтобы он мог обрабатывать и другие типы пунктуации, такие как вопросительные знаки, кавычки и двойные тире, которые мы видели ранее в первых 100 символах рассказа Эдит Уортон, а также дополнительные специальные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша модель токенизации теперь может успешно обрабатывать различные специальные символы в тексте.\n",
    "\n",
    "![](images/llm2.5.png)\n",
    "\n",
    "Теперь, когда у нас есть работающий базовый токенизатор, применим его ко всему рассказу Эдит Уортон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Преобразование токенов в идентификаторы\n",
    "\n",
    "Это преобразование является промежуточным шагом перед преобразованием в векторное представление.\n",
    "\n",
    "Чтобы сопоставить ранее сгенерированные токены с идентификаторами, нужно создать словарь. Этот словарь определяет, как мы сопоставляем каждое уникальное слово и специальный символ с уникальным целым числом, как показано на рисунке. Мы создаем словарь, разбивая весь текст обучающего набора данных на отдельные токены. Эти отдельные токены затем сортируются в алфавитном порядке, а повторяющиеся токены удаляются. Затем уникальные токены объединяются в словарь, который определяет сопоставление каждого уникального токена с уникальным целочисленным значением. \n",
    "\n",
    "![](images/llm2.6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n",
      "('Her', 51)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, словарь содержит отдельные токены, связанные с уникальными целочисленными метками.\n",
    "Наша следующая цель — применить этот словарь для преобразования нового текста в идентификаторы токенов, как показано на рисунке.\n",
    "\n",
    "![](images/llm2.7.png)\n",
    "\n",
    "\n",
    "Далее, когда мы захотим преобразовать выходные данные LLM из чисел обратно в текст, нам также понадобится способ превратить идентификаторы в текст. Для этого мы можем создать обратную версию словаря, которая сопоставляет идентификаторы с соответствующими текстовыми токенами.\n",
    "\n",
    "Реализуем полный класс токенизатора и детокенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "        \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    " \n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 Добавление специальных токенов контекста\n",
    "\n",
    "Изменим токенизатор для обработки неизвестных слов. Также обсудим использование и добавление специальных токенов контекста, которые могут улучшить понимание моделью контекста или другой соответствующей информации в тексте. Эти специальные токены могут включать, например, маркеры для неизвестных слов и границ документа.\n",
    "\n",
    "В частности, мы изменим словарь и токенизатор для поддержки двух новых токенов: <|unk|> и <|endoftext|>\n",
    "\n",
    "![](images/llm2.9.png)\n",
    "\n",
    "Как показано на рисунке, мы можем изменить токенизатор, чтобы он использовал токен <|unk|>, если он встретит слово, которое не является частью словаря. Кроме того, мы добавляем токен между несвязанными текстами. Например, при обучении GPT-подобных LLM на нескольких независимых документах или книгах обычно вставляется токен перед каждым документом или книгой, следующим за предыдущим источником текста Это помогает LLM понять, что, хотя эти текстовые источники объединены для обучения, на самом деле они не связаны друг с другом.\n",
    "\n",
    "![](images/llm2.10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n",
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    " \n",
    "print(len(vocab.items()))\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    " \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    " \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы видим, что идентификатор 1159 соответствует токену-разделителю <|endoftext|>, а идентификатор 1160 используются для неизвестных слов.\n",
    "\n",
    "Сравнивая приведенный выше детокенизированный текст с исходным входным текстом, мы знаем, что набор обучающих данных, рассказ Эдит Уортон «Вердикт», не содержал слов «Привет» и «дворец».\n",
    "\n",
    "Также рассматривают дополнительные специальные токены, такие как следующие:\n",
    "\n",
    "[BOS] (начало последовательности): этот токен отмечает начало текста.   \n",
    "[EOS] (конец последовательности): этот токен располагается в конце текста. Например, при объединении двух разных статей или книг Википедии токен [EOS] указывает, где заканчивается одна статья и начинается следующая.  \n",
    "[PAD] (заполнение): при обучении LLM с размерами пакетов больше единицы пакет может содержать тексты различной длины. Чтобы гарантировать одинаковую длину всех текстов, более короткие тексты расширяются или «дополняются» с помощью токена [PAD] до длины самого длинного текста в пакете.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Кодирование пар байтов (Byte pair encoding)\n",
    "\n",
    "В этом разделе рассматривается более сложная схема токенизации, основанная на концепции, называемой кодированием пар байтов (BPE). Такой токенизатор использовался для обучения LLM, таких как GPT-2, GPT-3 и исходной модели, используемой в ChatGPT.\n",
    "\n",
    "Поскольку реализация BPE может быть сложной, мы будем использовать существующую библиотеку Python с открытым исходным кодом под названием tiktoken (https://github.com/openai/tiktoken), которая очень эффективно реализует алгоритм BPE на основе исходного кода на Rust. Как и другие библиотеки Python, мы можем установить библиотеку tiktoken через установщик pip Python из терминала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.6.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе идентификаторов токенов и декодированного текста выше мы можем сделать два примечательных наблюдения. Во-первых, токену <|endoftext|> присваивается относительно большой идентификатор токена, а именно 50256. Фактически, токенизатор BPE, который использовался для обучения таких моделей, как GPT-2, GPT-3, и исходная модель, используемая в ChatGPT имеет общий размер словаря 50 257, причем <|endoftext|> присвоен самый большой идентификатор токена.\n",
    "\n",
    "Во-вторых, токенизатор BPE, указанный выше, правильно кодирует и декодирует неизвестные слова, такие как «someunknownPlace». Токенизатор BPE может обрабатывать любое неизвестное слово. Как это достигается без использования токенов <|unk|>?\n",
    "\n",
    "Алгоритм, лежащий в основе BPE, разбивает слова, которых нет в его предопределенном словаре, на более мелкие подслова или даже отдельные символы, что позволяет ему обрабатывать слова, которых нет в словаре. Таким образом, благодаря алгоритму BPE, если токенизатор встречает незнакомое слово во время токенизации, он может представить его как последовательность токенов или символов подслова, как показано на рисунке.\n",
    "\n",
    "![](images/llm2.11.png)\n",
    "\n",
    "Подробное обсуждение и реализация BPE выходит за рамки этой книги, но, если коротко, словарь строится путем итеративного объединения часто встречающихся символов в подслова, а часто встречающихся подслов — в слова. Например, BPE начинает с добавления в свой словарь всех отдельных одиночных символов («a», «b», ...). На следующем этапе он объединяет комбинации символов, которые часто встречаются вместе, в подслова. Например, «d» и «e» могут быть объединены в подслово «de», которое часто встречается во многих английских словах, таких как «define», «dependent», «made» и «hidden»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Выборка данных с помощью скользящего окна\n",
    "\n",
    "Следующим шагом является создание пар вход-выход, необходимых для обучения LLM.\n",
    "Как выглядят эти пары вход-выход? \n",
    "\n",
    "![](images/llm2.12.png)\n",
    "\n",
    "Реализуем теперь загрузчик данных, который извлекает пары вход-вызод из набора обучающих данных, используя подход скользящего окна.\n",
    "\n",
    "Для чего токенизируем весь рассказ The Verdict, с которым мы работали ранее, с помощью токенизатора BPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из самых простых и интуитивно понятных способов создания пар входных данных для задачи прогнозирования следующего слова — это создание двух переменных, x и y, где x содержит входные токены, а y содержит целевые значения, которые представляют собой входные данные, сдвинутые на 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрабатывая входные данные вместе с целевыми объектами, которые представляют собой входные данные, сдвинутые на одну позицию, мы можем затем создать задачи прогнозирования следующего слова, следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все, что находится слева от стрелки (---->), относится к входным данным, которые LLM получит, а идентификатор токена в правой части стрелки представляет собой целевой идентификатор токена, который LLM должен предсказать.\n",
    "\n",
    "В целях иллюстрации давайте повторим предыдущий код, но преобразуем идентификаторы токенов в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы создали пары вход-цель, которые можно использовать для обучения LLM в следующих главах.\n",
    "\n",
    "Прежде чем мы сможем превратить токены во встраивания, есть еще только одна задача, как мы упоминали в начале этой главы: реализация эффективного загрузчика данных, который выполняет итерацию по входному набору данных и возвращает входные и целевые значения в виде тензоров PyTorch, что можно представить себе как как многомерные массивы.\n",
    "\n",
    "В частности, мы заинтересованы в возврате двух тензоров: входного, содержащего текст, который видит LLM, и целевого тензора, который включает в себя цели, которые LLM должен предсказать, как показано на рисунке 2.13.\n",
    "\n",
    "Рисунок 2.13. Чтобы реализовать эффективные загрузчики данных, мы собираем входные данные в тензор x, где каждая строка представляет один входной контекст. Второй тензор y содержит соответствующие цели прогнозирования (следующие слова), которые создаются путем сдвига входных данных на одну позицию.\n",
    "\n",
    "![](images/llm2.13.png)\n",
    "\n",
    "Хотя на рисунке 2.13 для иллюстрации токены показаны в строковом формате, реализация кода будет работать непосредственно с идентификаторами токенов, поскольку метод кодирования токенизатора BPE выполняет как токенизацию, так и преобразование в идентификаторы токенов за один шаг.\n",
    "\n",
    "Для эффективной реализации загрузчика данных мы будем использовать встроенные классы PyTorch Dataset и DataLoader. Дополнительную информацию и рекомендации по установке PyTorch см. в разделе A.1.3 «Установка PyTorch» в Приложении A.\n",
    "\n",
    "Код класса набора данных показан в листинге 2.5:\n",
    "\n",
    "Листинг 2.5. Набор данных для пакетных входных и целевых значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    " \n",
    "        token_ids = tokenizer.encode(txt)\n",
    " \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс GPTDatasetV1 в листинге 2.5 основан на классе набора данных PyTorch и определяет, как отдельные строки извлекаются из набора данных, где каждая строка состоит из нескольких идентификаторов токенов (на основе max_length), назначенных тензору input_chunk. Тензор target_chunk содержит соответствующие цели. Я рекомендую прочитать дальше, чтобы увидеть, как выглядят данные, возвращаемые из этого набора данных, когда мы объединяем набор данных с PyTorch DataLoader — это принесет дополнительную интуицию и ясность.\n",
    "\n",
    "Если вы впервые знакомы со структурой классов набора данных PyTorch, например, показанной в листинге 2.5, прочтите раздел A.6 «Настройка эффективных загрузчиков данных» в приложении A, в котором объясняется общая структура и использование классов PyTorch Dataset и DataLoader.\n",
    "\n",
    "Следующий код будет использовать GPTDatasetV1 для пакетной загрузки входных данных через PyTorch DataLoader:\n",
    "\n",
    "Листинг 2.6. Загрузчик данных для генерации пакетов с парами вход-с"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, \n",
    "        max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте протестируем загрузчик данных с размером пакета 1 для LLM с размером контекста 4, чтобы понять, как класс GPTDatasetV1 из листинга 2.5 и функция create_dataloader_v1 из листинга 2.6 работают вместе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_202984/3364176621.py:15: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  self.input_ids.append(torch.tensor(input_chunk))\n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переменная first_batch содержит два тензора: первый тензор хранит идентификаторы входных токенов, а второй тензор хранит идентификаторы целевых токенов. Поскольку max_length установлено равным 4, каждый из двух тензоров содержит 4 идентификатора токена. Обратите внимание, что входной размер 4 относительно мал и выбран только для иллюстрации. Обычно LLM обучают с размером входных данных не менее 256.\n",
    "\n",
    "Чтобы проиллюстрировать значение шага=1, давайте возьмем еще один пакет из этого набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы сравним первый и второй пакет, мы увидим, что идентификаторы токенов второго пакета сдвинуты на одну позицию по сравнению с первым пакетом (например, второй идентификатор во входных данных первого пакета равен 367, что является первым идентификатором вход второго пакета). Настройка шага определяет количество позиций, на которые смещаются входные данные между пакетами, имитируя подход скользящего окна, как показано на рисунке 2.14.\n",
    "\n",
    "Рисунок 2.14. При создании нескольких пакетов из входного набора данных мы перемещаем окно ввода по тексту. Если шаг установлен на 1, мы сдвигаем окно ввода на 1 позицию при создании следующего пакета. Если мы установим шаг равным размеру окна ввода, мы сможем предотвратить перекрытие между пакетами.\n",
    "\n",
    "![](images/llm2.14.png)\n",
    "\n",
    "\n",
    "Упражнение 2.2. Загрузчики данных с разными шагами и размерами контекста.\n",
    "\n",
    "Чтобы лучше понять, как работает загрузчик данных, попробуйте запустить его с различными настройками, такими как max_length=2 и шаг=2 и max_length=8 и шаг=2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Размеры пакетов, равные 1, например, которые мы до сих пор выбирали из загрузчика данных, полезны для иллюстративных целей. Если у вас есть предыдущий опыт глубокого обучения, вы, возможно, знаете, что небольшие размеры пакетов требуют меньше памяти во время обучения, но приводят к более шумным обновлениям модели. Как и в обычном глубоком обучении, размер пакета — это компромисс и гиперпараметр, с которым можно экспериментировать при обучении LLM.\n",
    "\n",
    "Прежде чем мы перейдем к двум последним разделам этой главы, посвященным созданию векторов внедрения из идентификаторов токенов, давайте кратко рассмотрим, как мы можем использовать загрузчик данных для выборки с размером пакета больше 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[ 5032,    11,   339,  5257],\n",
      "        [   11,  3595,   520,  5493],\n",
      "        [  286, 18113,   544,   338],\n",
      "        [  373,   866,   287,  6245],\n",
      "        [  340,   373,   262,   938],\n",
      "        [ 6687,   284,  2107,  1231],\n",
      "        [  314,  3947,   284,   766],\n",
      "        [  257,   410,  5040,   329]])\n",
      "\n",
      "Targets:\n",
      " tensor([[   11,   339,  5257,   284],\n",
      "        [ 3595,   520,  5493,     0],\n",
      "        [18113,   544,   338, 10953],\n",
      "        [  866,   287,  6245,   684],\n",
      "        [  373,   262,   938,  1517],\n",
      "        [  284,  2107,  1231,   326],\n",
      "        [ 3947,   284,   766,   257],\n",
      "        [  410,  5040,   329,   257]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
    " \n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы увеличиваем шаг до 4. Это необходимо для полного использования набора данных (мы не пропускаем ни одного слова), но также для избежания любого перекрытия между пакетами, поскольку большее перекрытие может привести к увеличению переобучения.\n",
    "\n",
    "В последних двух разделах этой главы мы реализуем уровни внедрения, которые преобразуют идентификаторы токенов в непрерывные векторные представления, которые служат форматом входных данных для LLM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it results in a slightly more interesting text passage in the next steps:\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "One of the easiest and most intuitive ways to create the input-target pairs for the next-word prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1:\n",
    "\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "\n",
    "Running the above code prints the following output:\n",
    "\n",
    "x: [290, 4920, 2241, 287]\n",
    "y:      [4920, 2241, 287, 257]\n",
    "\n",
    "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks depicted earlier in figure 2.12, as follows:\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)\n",
    "\n",
    "The code above prints the following:\n",
    "\n",
    "[290] ----> 4920\n",
    "[290, 4920] ----> 2241\n",
    "[290, 4920, 2241] ----> 287\n",
    "[290, 4920, 2241, 287] ----> 257\n",
    "\n",
    "\n",
    "\n",
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict.\n",
    "\n",
    "For illustration purposes, let's repeat the previous code but convert the token IDs into text:\n",
    "\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\n",
    "\n",
    "\n",
    "The following outputs show how the input and outputs look in text format:\n",
    "\n",
    " and ---->  established\n",
    " and established ---->  himself\n",
    " and established himself ---->  in\n",
    " and established himself in ---->  a\n",
    "\n",
    "\n",
    "\n",
    "We've now created the input-target pairs that we can turn into use for the LLM training in upcoming chapters.\n",
    "\n",
    "There's only one more task before we can turn the tokens into embeddings, as we mentioned at the beginning of this chapter: implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays.\n",
    "\n",
    "In particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict, as depicted in Figure 2.13.\n",
    "\n",
    "Figure 2.13 To implement efficient data loaders, we collect the inputs in a tensor, x, where each row represents one input context. A second tensor, y, contains the corresponding prediction targets (next words), which are created by shifting the input by one position.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "While Figure 2.13 shows the tokens in string format for illustration purposes, the code implementation will operate on token IDs directly since the encode method of the BPE tokenizer performs both tokenization and conversion into token IDs as a single step.\n",
    "\n",
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and DataLoader classes. For additional information and guidance on installing PyTorch, please see section A.1.3, Installing PyTorch, in Appendix A.\n",
    "\n",
    "The code for the dataset class is shown in code listing 2.5:\n",
    "\n",
    "Listing 2.5 A dataset for batched inputs and targets\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    " \n",
    "        token_ids = tokenizer.encode(txt)\n",
    " \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The GPTDatasetV1 class in listing 2.5 is based on the PyTorch Dataset class and defines how individual rows are fetched from the dataset, where each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor. The target_chunk tensor contains the corresponding targets. I recommend reading on to see how the data returned from this dataset looks like when we combine the dataset with a PyTorch DataLoader -- this will bring additional intuition and clarity.\n",
    "\n",
    "If you are new to the structure of PyTorch Dataset classes, such as shown in listing 2.5, please read section A.6, Setting up efficient data loaders, in Appendix A, which explains the general structure and usage of PyTorch Dataset and DataLoader classes.\n",
    "\n",
    "The following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader:\n",
    "\n",
    "Listing 2.6 A data loader to generate batches with input-with pairs\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, \n",
    "        max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader\n",
    "\n",
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4 to develop an intuition of how the GPTDatasetV1 class from listing 2.5 and the create_dataloader_v1 function from listing 2.6 work together:\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "Executing the preceding code prints the following:\n",
    "\n",
    "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs. Since the max_length is set to 4, each of the two tensors contains 4 token IDs. Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least 256.\n",
    "\n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this dataset:\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)\n",
    "\n",
    "\n",
    "The second batch has the following contents:\n",
    "\n",
    "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
    "\n",
    "If we compare the first with the second batch, we can see that the second batch's token IDs are shifted by one position compared to the first batch (for example, the second ID in the first batch's input is 367, which is the first ID of the second batch's input). The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach, as demonstrated in Figure 2.14.\n",
    "\n",
    "Figure 2.14 When creating multiple batches from the input dataset, we slide an input window across the text. If the stride is set to 1, we shift the input window by 1 position when creating the next batch. If we set the stride equal to the input window size, we can prevent overlaps between the batches.\n",
    "\n",
    "\n",
    "Exercise 2.2 Data loaders with different strides and context sizes\n",
    "\n",
    "To develop more intuition for how the data loader works, try to run it with different settings such as max_length=2 and stride=2 and max_length=8 and stride=2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Batch sizes of 1, such as we have sampled from the data loader so far, are useful for illustration purposes. If you have previous experience with deep learning, you may know that small batch sizes require less memory during training but lead to more noisy model updates. Just like in regular deep learning, the batch size is a trade-off and hyperparameter to experiment with when training LLMs.\n",
    "\n",
    "Before we move on to the two final sections of this chapter that are focused on creating the embedding vectors from the token IDs, let's have a brief look at how we can use the data loader to sample with a batch size greater than 1:\n",
    "\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
    " \n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "\n",
    "\n",
    "This prints the following:\n",
    "\n",
    "Inputs:\n",
    " tensor([[   40,   367,  2885,  1464],\n",
    "        [ 1807,  3619,   402,   271],\n",
    "        [10899,  2138,   257,  7026],\n",
    "        [15632,   438,  2016,   257],\n",
    "        [  922,  5891,  1576,   438],\n",
    "        [  568,   340,   373,   645],\n",
    "        [ 1049,  5975,   284,   502],\n",
    "        [  284,  3285,   326,    11]])\n",
    " \n",
    "Targets:\n",
    " tensor([[  367,  2885,  1464,  1807],\n",
    "        [ 3619,   402,   271, 10899],\n",
    "        [ 2138,   257,  7026, 15632],\n",
    "        [  438,  2016,   257,   922],\n",
    "        [ 5891,  1576,   438,   568],\n",
    "        [  340,   373,   645,  1049],\n",
    "        [ 5975,   284,   502,   284],\n",
    "        [ 3285,   326,    11,   287]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note that we increase the stride to 4. This is to utilize the data set fully (we don't skip a single word) but also avoid any overlap between the batches, since more overlap could lead to increased overfitting.\n",
    "\n",
    "In the final two sections of this chapter, we will implement embedding layers that convert the token IDs into continuous vector representations, which serve as input data format for LLMs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.7 Creating token embeddings\n",
    "\n",
    "The last step for preparing the input text for LLM training is to convert the token IDs into embedding vectors, as illustrated in Figure 2.15, which will be the focus of these two last remaining sections of this chapter.\n",
    "\n",
    "Figure 2.15 Preparing the input text for an LLM involves tokenizing text, converting text tokens to token IDs, and converting token IDs into vector embedding vectors. In this section, we consider the token IDs created in previous sections to create the token embedding vectors.\n",
    "\n",
    "\n",
    "\n",
    "In addition to the processes outlined in Figure 2.15, it is important to note that we initialize these embedding weights with random values as a preliminary step. This initialization serves as the starting point for the LLM's learning process. We will optimize the embedding weights as part of the LLM training in chapter 5.\n",
    "\n",
    "A continuous vector representation, or embedding, is necessary since GPT-like LLMs are deep neural networks trained with the backpropagation algorithm. If you are unfamiliar with how neural networks are trained with backpropagation, please read section A.4, Automatic differentiation made easy, in Appendix A.\n",
    "\n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 123 for reproducibility purposes:\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "The print statement in the preceding code example prints the embedding layer's underlying weight matrix:\n",
    "\n",
    "Parameter containing:\n",
    "tensor([[ 0.3374, -0.1778, -0.1690],\n",
    "        [ 0.9178,  1.5810,  1.3010],\n",
    "        [ 1.2753, -0.2010, -0.1606],\n",
    "        [-0.4015,  0.9666, -1.1481],\n",
    "        [-1.1589,  0.3255, -0.6315],\n",
    "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "We can see that the weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself, as we will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions.\n",
    "\n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the embedding vector:\n",
    "\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "\n",
    "The returned embedding vector is as follows:\n",
    "\n",
    "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
    "\n",
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row (Python starts with a zero index, so it's the row corresponding to index 3). In other words, the embedding layer is essentially a look-up operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n",
    "\n",
    "\n",
    "Embedding layers versus matrix multiplication\n",
    "\n",
    "For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully connected layer, which is illustrated in the supplementary code on GitHub at https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/03_bonus_embedding-vs-matmul. Because the embedding layer is just a more efficient implementation equivalent to the one-hot encoding and matrix-multiplication approach, it can be seen as a neural network layer that can be optimized via backpropagation.\n",
    "\n",
    "Previously, we have seen how to convert a single token ID into a three-dimensional embedding vector. Let's now apply that to all four input IDs we defined earlier (torch.tensor([2, 3, 5, 1])):\n",
    "\n",
    "print(embedding_layer(input_ids))\n",
    "\n",
    "The print output reveals that this results in a 4x3 matrix:\n",
    "\n",
    "tensor([[ 1.2753, -0.2010, -0.1606],\n",
    "        [-0.4015,  0.9666, -1.1481],\n",
    "        [-2.8400, -0.7849, -1.4096],\n",
    "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n",
    "\n",
    "Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix, as illustrated in Figure 2.16.\n",
    "\n",
    "Figure 2.16 Embedding layers perform a look-up operation, retrieving the embedding vector corresponding to the token ID from the embedding layer's weight matrix. For instance, the embedding vector of the token ID 5 is the sixth row of the embedding layer weight matrix (it is the sixth instead of the fifth row because Python starts counting at 0). For illustration purposes, we assume that the token IDs were produced by the small vocabulary we used in section 2.3.\n",
    "\n",
    "This section covered how we create embedding vectors from token IDs. The next and final section of this chapter will add a small modification to these embedding vectors to encode positional information about a token within a text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.8 Encoding word positions\n",
    "\n",
    "In the previous section, we converted the token IDs into a continuous vector representation, the so-called token embeddings. In principle, this is a suitable input for an LLM. However, a minor shortcoming of LLMs is that their self-attention mechanism, which will be covered in detail in chapter 3, doesn't have a notion of position or order for the tokens within a sequence.\n",
    "\n",
    "The way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence, as illustrated in Figure 2.17.\n",
    "\n",
    "Figure 2.17 The embedding layer converts a token ID into the same vector representation regardless of where it is located in the input sequence. For example, the token ID 5, whether it's in the first or third position in the token ID input vector, will result in the same embedding vector.\n",
    "\n",
    "\n",
    "\n",
    "In principle, the deterministic, position-independent embedding of the token ID is good for reproducibility purposes. However, since the self-attention mechanism of LLMs itself is also position-agnostic, it is helpful to inject additional position information into the LLM.\n",
    "\n",
    "To achieve this, there are two broad categories of position-aware embeddings: relative positional embeddings and absolute positional embeddings.\n",
    "\n",
    "Absolute positional embeddings are directly associated with specific positions in a sequence. For each position in the input sequence, a unique embedding is added to the token's embedding to convey its exact location. For instance, the first token will have a specific positional embedding, the second token another distinct embedding, and so on, as illustrated in Figure 2.18.\n",
    "\n",
    "\n",
    "Figure 2.18 Positional embeddings are added to the token embedding vector to create the input embeddings for an LLM. The positional vectors have the same dimension as the original token embeddings. The token embeddings are shown with value 1 for simplicity.\n",
    "\n",
    "\n",
    "\n",
    "Instead of focusing on the absolute position of a token, the emphasis of relative positional embeddings is on the relative position or distance between tokens. This means the model learns the relationships in terms of \"how far apart\" rather than \"at which exact position.\" The advantage here is that the model can generalize better to sequences of varying lengths, even if it hasn't seen such lengths during training.\n",
    "\n",
    "Both types of positional embeddings aim to augment the capacity of LLMs to understand the order and relationships between tokens, ensuring more accurate and context-aware predictions. The choice between them often depends on the specific application and the nature of the data being processed.\n",
    "\n",
    "OpenAI's GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the positional encodings in the original Transformer model. This optimization process is part of the model training itself, which we will implement later in this book. For now, let's create the initial positional embeddings to create the LLM inputs for the upcoming chapters.\n",
    "\n",
    "Previously, we focused on very small embedding sizes in this chapter for illustration purposes. We now consider more realistic and useful embedding sizes and encode the input tokens into a 256-dimensional vector representation. This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable for experimentation. Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257:\n",
    "\n",
    "\n",
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "\n",
    "\n",
    "Using the token_embedding_layer above, if we sample data from the data loader, we embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
    "\n",
    "Let's instantiate the data loader from section 2.6, Data sampling with a sliding window, first:\n",
    "\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "he preceding code prints the following output:\n",
    "\n",
    "Token IDs:\n",
    " tensor([[   40,   367,  2885,  1464],\n",
    "        [ 1807,  3619,   402,   271],\n",
    "        [10899,  2138,   257,  7026],\n",
    "        [15632,   438,  2016,   257],\n",
    "        [  922,  5891,  1576,   438],\n",
    "        [  568,   340,   373,   645],\n",
    "        [ 1049,  5975,   284,   502],\n",
    "        [  284,  3285,   326,    11]])\n",
    " \n",
    "Inputs shape:\n",
    " torch.Size([8, 4])\n",
    "\n",
    "\n",
    "\n",
    "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch consists of 8 text samples with 4 tokens each.\n",
    "\n",
    "Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:\n",
    "\n",
    "\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "The preceding print function call returns the following:\n",
    "\n",
    "\n",
    "torch.Size([8, 4, 256])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now embedded as a 256-dimensional vector.\n",
    "\n",
    "For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same dimension as the token_embedding_layer:\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_lengthe, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    " shown in the preceding code example, the input to the pos_embeddings is usually a placeholder vector torch.arange(context_length), which contains a sequence of numbers 0, 1, ..., up to the maximum input length − 1. The context_length is a variable that represents the supported input size of the LLM. Here, we choose it similar to the maximum length of the input text. In practice, input text can be longer than the supported context length, in which case we have to truncate the text.\n",
    "\n",
    "The output of the print statement is as follows:\n",
    "\n",
    "\n",
    "torch.Size([4, 256])\n",
    "\n",
    "\n",
    "s we can see, the positional embedding tensor consists of four 256-dimensional vectors. We can now add these directly to the token embeddings, where PyTorch will add the 4x256-dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in each of the 8 batches:\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "The print output is as follows:\n",
    "\n",
    "torch.Size([8, 4, 256])\n",
    "\n",
    "The input_embeddings we created, as summarized in Figure 2.19, are the embedded input examples that can now be processed by the main LLM modules, which we will begin implementing in chapter 3\n",
    "\n",
    "Figure 2.19 As part of the input processing pipeline, input text is first broken up into individual tokens. These tokens are then converted into token IDs using a vocabulary. The token IDs are converted into embedding vectors to which positional embeddings of a similar size are added, resulting in input embeddings that are used as input for the main LLM layers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.9 Summary\n",
    "\n",
    "    LLMs require textual data to be converted into numerical vectors, known as embeddings since they can't process raw text. Embeddings transform discrete data (like words or images) into continuous vector spaces, making them compatible with neural network operations.\n",
    "    As the first step, raw text is broken into tokens, which can be words or characters. Then, the tokens are converted into integer representations, termed token IDs.\n",
    "    Special tokens, such as <|unk|> and <|endoftext|>, can be added to enhance the model's understanding and handle various contexts, such as unknown words or marking the boundary between unrelated texts.\n",
    "    The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3 can efficiently handle unknown words by breaking them down into subword units or individual characters.\n",
    "    We use a sliding window approach on tokenized data to generate input-target pairs for LLM training.\n",
    "    Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token IDs. The resulting embedding vectors provide continuous representations of tokens, which is crucial for training deep learning models like LLMs.\n",
    "    While token embeddings provide consistent vector representations for each token, they lack a sense of the token's position in a sequence. To rectify this, two main types of positional embeddings exist: absolute and relative. OpenAI's GPT models utilize absolute positional embeddings that are added to the token embedding vectors and are optimized during the model training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
