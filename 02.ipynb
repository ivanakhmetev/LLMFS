{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Обработка текста\n",
    "\n",
    "В предыдущей главе мы углубились в структуру больших языковых моделей (LLM) и узнали, что они предварительно обучаются на огромных объемах текста, обрабатывая\n",
    "по одному слову (токену) за раз. \n",
    "\n",
    "![](images/llm2.1.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Векторное представление (vector embedding)\n",
    "\n",
    "Нейросетевые модели, не могут напрямую обрабатывать текст. Текст, как тип данных, несовместим с тензорными операциями. Следовательно, нужен способ представлять слова в виде векторов с непрерывными значениями. (ссылка узнать больше в Приложении A, раздел A2.2 «Понимание тензоров».)\n",
    "\n",
    "Построение векторного представления может быть проведено разными способами (различными моделями). \n",
    "Прочие типы данных также требуют преобразования в тензорный формат.  \n",
    "\n",
    "![](images/llm2.2.png)\n",
    "\n",
    "Существует несколько алгоритмов для создания векторного представления текста. Важным моментом является выбор - что считать единичным токеном. Одним из первых и наиболее популярных примеров является Word2Vec (токен - слово). Основная идея заключается в том, что слова, встречающиеся в схожем контексте, имеют схожее значение. Следовательно, при проецировании в двумерное пространство, можно увидеть, что схожие термины группируются вместе.\n",
    "\n",
    "![](images/llm2.3.png)\n",
    "\n",
    "Помимо моделей токенизации слов, существуют модели токенизации предложений, абзацев, целых документов.\n",
    "Таким образом, картинка из предыдущей главы (predicting next world) не вполне корректна - предсказывается следующий токен."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вендоры LLM обычно создают свои собственные модели токенизации, которые являются частью входного слоя и обновляются во время обучения. Преимущество такого подхода - оптимизация для конкретной задачи и имеющихся данных. \n",
    "\n",
    "Размерность представления (двумерное на рисунке выше) может быть произвольной. Большая размерность может отражать более тонкие отношения, но за счет вычислительной эффективности. Эта размерность называется размерностью скрытых состояний модели (dimensionality of the model's hidden states). \n",
    "\n",
    "Эта размерность - компромисс между производительностью и эффективностью:  \n",
    "- GPT-2 (125M параметров) использует размерность 768  \n",
    "- GPT-3 (175B параметров) использует размерность 12 288  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Токенизация текста\n",
    "\n",
    "![](images/llm2.4.png)\n",
    "\n",
    "Будем токенизировать рассказ Эдит Уортон [«Вердикт»](https://en.wikisource.org/wiki/The_Verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для разделения текста на токены, используем сначала команду re.split, чтобы разделить текст на пробельные символы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем регулярные выражения для выделения запятых и точек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При разработке токенизатора, выделение пробельных символов зависит от приложения и его требований. Удаление пробелов снижает требования к памяти и вычислительным ресурсам. Однако сохранение пробелов может быть полезно, если мы обучаем модели, чувствительные к точной структуре текста (например, код Python, чувствительный к отступам и интервалам). \n",
    "\n",
    "Изменим код, чтобы он мог обрабатывать и другие типы пунктуации, такие как вопросительные знаки, кавычки и двойные тире, которые мы видели ранее в первых 100 символах рассказа Эдит Уортон, а также дополнительные специальные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша модель токенизации теперь может успешно обрабатывать различные специальные символы в тексте.\n",
    "\n",
    "![](images/llm2.5.png)\n",
    "\n",
    "Теперь, когда у нас есть работающий базовый токенизатор, применим его ко всему рассказу Эдит Уортон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Преобразование токенов в идентификаторы\n",
    "\n",
    "Полученные токены сортируются в алфавитном порядке, повторяющиеся удаляются. Затем уникальные токены объединяются в словарь: текстовый токен - целочисленное значение.\n",
    "\n",
    "![](images/llm2.6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 15:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выходные данные LLM будут поступать как последовательность числовых значений, которые поребуется преобразовать обратно в текст. Для этого мы можем создать обратную версию словаря (детокенизатор).\n",
    "\n",
    "![](images/llm2.7.png)\n",
    "\n",
    "Реализуем полный класс токенизатора и детокенизатора и применим этот словарь для преобразования всего текста в токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "        \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    " \n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 Добавление специальных токенов контекста\n",
    "\n",
    "Изменим токенизатор для обработки неизвестных слов. Также обсудим использование и добавление специальных токенов контекста, которые могут улучшить понимание моделью контекста или другой соответствующей информации в тексте. Эти специальные токены могут включать, например, маркеры для неизвестных слов и границ документа.\n",
    "\n",
    "В частности, мы изменим словарь и токенизатор для поддержки двух новых токенов: <|unk|> и <|endoftext|>\n",
    "\n",
    "![](images/llm2.8.png)\n",
    "\n",
    "Как показано на рисунке, мы можем изменить токенизатор, чтобы он использовал токен <|unk|>, если он встретит слово, которое не является частью словаря. Кроме того, мы добавляем токен между несвязанными текстами. Например, при обучении GPT-подобных LLM на нескольких независимых документах или книгах обычно вставляется токен перед каждым документом или книгой, следующим за предыдущим источником текста. Это помогает LLM понять, что, хотя эти текстовые источники объединены для обучения, на самом деле они не связаны друг с другом.\n",
    "\n",
    "![](images/llm2.9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n",
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    " \n",
    "print(len(vocab.items()))\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    " \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    " \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы видим, что идентификатор 1159 соответствует токену-разделителю <|endoftext|>, а идентификатор 1160 используются для неизвестных слов.\n",
    "\n",
    "Сравнивая приведенный выше детокенизированный текст с исходным входным текстом, мы знаем, что набор обучающих данных не содержал слов «Привет» и «дворец».\n",
    "\n",
    "Также рассматривают дополнительные специальные токены, такие как:\n",
    "- [BOS] (начало последовательности): этот токен отмечает начало текста.   \n",
    "- [EOS] (конец последовательности): этот токен располагается в конце текста. Например, при объединении двух разных статей или книг Википедии токен [EOS] указывает, где заканчивается одна статья и начинается следующая.  \n",
    "- [PAD] (заполнение): при обучении LLM с размерами пакетов больше единицы пакет может содержать тексты различной длины. Чтобы гарантировать одинаковую длину всех текстов, более короткие тексты расширяются или «дополняются» с помощью токена [PAD] до длины самого длинного текста в пакете.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Кодирование по парам байтов (Byte pair encoding)\n",
    "\n",
    "В этом разделе рассматривается более сложная схема токенизации. Она использовалась для обучения GPT-2, GPT-3.\n",
    "\n",
    "Поскольку реализация BPE может быть сложной, будем использовать существующую библиотеку Python с открытым исходным кодом под названием [tiktoken](https://github.com/openai/tiktoken), которая очень эффективно реализует алгоритм BPE на основе исходного кода на Rust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.6.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание:\n",
    "- Во-первых, токену <|endoftext|> присваивается относительно большой идентификатор токена, а именно 50256. Фактически, токенизатор BPE, который использовался для обучения ChatGPT имеет общий размер словаря 50 257, причем <|endoftext|> присвоен самый большой идентификатор токена.\n",
    "\n",
    "- Во-вторых, BPE правильно кодирует и декодирует неизвестные слова, такие как «someunknownPlace». Токенизатор BPE может обрабатывать любое неизвестное слово.\n",
    "\n",
    "Алгоритм, лежащий в основе BPE, разбивает слова, которых нет в его предопределенном словаре, на более мелкие подслова или даже отдельные символы, что позволяет ему обрабатывать слова, которых нет в словаре. Таким образом, благодаря алгоритму BPE, если токенизатор встречает незнакомое слово во время токенизации, он может представить его как последовательность токенов или символов подслова, как показано на рисунке.\n",
    "\n",
    "![](images/llm2.10.png)\n",
    "\n",
    "Подробное обсуждение и реализация BPE выходит за рамки этой книги, но, если коротко, словарь строится путем итеративного объединения часто встречающихся символов в подслова, а часто встречающихся подслов — в слова. Например, BPE начинает с добавления в свой словарь всех отдельных одиночных символов («a», «b», ...). На следующем этапе он объединяет комбинации символов, которые часто встречаются вместе, в подслова. Например, «d» и «e» могут быть объединены в подслово «de», которое часто встречается во многих английских словах, таких как «define», «dependent», «made» и «hidden»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Выборка данных с помощью скользящего окна\n",
    "\n",
    "Следующим шагом является создание пар вход - цель (input - target), необходимых для обучения LLM.\n",
    "Реализуем теперь загрузчик данных, который извлекает пары из набора обучающих данных, используя подход скользящего окна.\n",
    "Для чего токенизируем весь рассказ The Verdict, с помощью токенизатора BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интуитивно понятный способ создания пар данных — создание двух переменных, x и y, где x содержит входные токены, а y содержит целевые значения, которые представляют собой входные данные, сдвинутые на 1 (размер скользящего окна):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно представить задачу предсказания следующего слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все, что находится слева от стрелки (---->), относится к входным данным, которые LLM получит, а идентификатор токена в правой части стрелки представляет собой целевой идентификатор токена, который LLM должен предсказать.\n",
    "\n",
    "В целях иллюстрации давайте повторим предыдущий код, но преобразуем идентификаторы токенов в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/llm2.11.png)\n",
    "\n",
    "Следующая задача - реализация эффективного загрузчика данных, который выполняет итерацию по входному набору данных и возвращает входные и целевые значения в виде тензоров PyTorch, каждая строка которых - один вход или одна цель (на рисунке слова, вместо идентификаторов). \n",
    "\n",
    "![](images/llm2.12.png)\n",
    "\n",
    "Бдем использовать встроенные классы PyTorch Dataset и DataLoader. (ссылка Дополнительную информацию и рекомендации по установке PyTorch см. в разделе A.1.3 «Установка PyTorch» в Приложении A.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    " \n",
    "        token_ids = tokenizer.encode(txt)\n",
    " \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс GPTDatasetV1 определяет, как отдельные строки извлекаются из набора данных (каждая строка состоит из нескольких идентификаторов токенов). \n",
    "\n",
    "(ссылка прочтите раздел A.6 «Настройка эффективных загрузчиков данных» в приложении A, в котором объясняется общая структура и использование классов PyTorch Dataset и DataLoader )\n",
    "\n",
    "Определим далее загрузку тензоров с помощью PyTorch DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, \n",
    "        max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  40,  367, 2885, 1464]])\n",
      "     tensor([[ 367, 2885, 1464, 1807]])\n"
     ]
    }
   ],
   "source": [
    "with open(\"source/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch[0])\n",
    "print('    ', first_batch[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переменная first_batch содержит два тензора: первый тензор хранит идентификаторы входных токенов, а второй тензор хранит идентификаторы целевых токенов. Поскольку max_length установлено равным 4, каждый из двух тензоров содержит 4 идентификатора токена. Обратите внимание, что входной размер 4 относительно мал и выбран только для иллюстрации. Обычно LLM обучают с размером входных данных не менее 256.\n",
    "\n",
    "Чтобы проиллюстрировать значение этого шага, возьмем еще один блок (batch) из этого набора данных (hазмеры блоков равные 1, cлужат для иллюстративных целей. Размер пакета — это компромисс и гиперпараметр, с которым можно экспериментировать при обучении LLM. Небольшие размеры пакетов требуют меньше памяти во время обучения, но приводят к более \"шумным\" обновлениям модели):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 367, 2885, 1464, 1807]])\n",
      "     tensor([[2885, 1464, 1807, 3619]])\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch[0])\n",
    "print('    ', second_batch[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/llm2.13.png)\n",
    "\n",
    "Пример загрузчика с другими параметрами - увеличиваем шаг до 4. Это необходимо для полного использования данных (не пропускаем ни одного слова), и для избежания перекрытия между блоками (перекрытие может привести к переобучению)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  198,   198,  1544,  6204],\n",
      "        [   13,  8759,  2763,   438],\n",
      "        [  503,   465,  7721,   257],\n",
      "        [  286,   511,  6799,   454],\n",
      "        [  314,  2993,  1576,   284],\n",
      "        [  355,  1752,   530,   550],\n",
      "        [  340,   656,  5563,   286],\n",
      "        [  262, 23755,   286,   262]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  198,  1544,  6204,  2045],\n",
      "        [ 8759,  2763,   438,  1169],\n",
      "        [  465,  7721,   257,  1310],\n",
      "        [  511,  6799,   454,    30],\n",
      "        [ 2993,  1576,   284,  2666],\n",
      "        [ 1752,   530,   550,   890],\n",
      "        [  656,  5563,   286,  1242],\n",
      "        [23755,   286,   262, 14005]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
    " \n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Создание векторного представления из токенов\n",
    "\n",
    "Последним шагом подготовки входного текста для обучения LLM является преобразование идентификаторов токенов в векторы - представления, как показано на рисунке. Векторное представление содержит информацию о связях токена с другими. Многообразие этих связей зависит от размерности представления (dimensionality hidden state). Сила этих связей изменяется в процессе обучения (изначально эти связи - веса матриц инициализированы случайными значениями). В части вычислений за изменения отвечает алгоритм обратного распространения ошибки.  (ссылка Если вы не знакомы с тем, как нейронные сети обучаются с помощью обратного распространения ошибки, прочтите раздел A.4 «Легкая автоматическая дифференциация» в Приложении A.)\n",
    "\n",
    "![](images/llm2.14.png)\n",
    "\n",
    "Проиллюстрируем, как работает векторное преобразование идентификатора токена на практическом примере. Предположим, у нас есть следующие четыре входных токена с идентификаторами 2, 3, 5 и 1. Для простоты предположим, что у нас есть небольшой словарь, состоящий всего из 6 слов (вместо 50 257 слов в словаре токенизатора BPE), и мы хотим создать представления размерности 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица имеет шесть (размер словаря) строк и три столбца (размерность представления, dimensionality hidden state). Значения в матрице (веса) инициализированы случайными числами и изменяются в процессе обучения. Значения в матрице как точки в трехмерном пространстве соответствуют связанности токенов: контекстно-близкие имеют меньшую метрику расстояния, контекстно-далекие - большую (более наглядно расстояние между токенами иллюстрируется на картинке для размерности 2 в начале главы). Один токен - одна строка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))\n",
    "# выведены строки - токены с индексами input_ids = [2, 3, 5, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/llm2.16.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Кодирование позиций слов / векторно - позиционное представление (positional embedding)\n",
    "\n",
    "Оставшимся неудобством в полученном векторном представлении токенов - независимость токена от положения во входящей последовательности. Т.е. один и тот же идентификатор токена всегда отображается в одно и то же векторное представление, независимо от того, где идентификатор токена расположен во входной последовательности (как на предыдущем рисунке).\n",
    "\n",
    "В принципе, детерминированное, независимое от позиции представление полезно для целей воспроизводимости. Однако, различное представление в зависимости от положения в последовательности будет полезно для обучения, т.к. кодировать контекст. \n",
    "\n",
    "Добавить информацию о положении токена можно абсолютным способом: для каждой позиции токена в последовательности добавляется её код - к токену прибавляется значение, рисунок. \n",
    "\n",
    "![](images/llm2.17.png)\n",
    "\n",
    "Или относительным способом, когда существенно относительное расстояние между токенами в последовательности (токен в начале предложения и в конце). В таком случае модель может лучше обобщать последовательности различной длины, даже если во время обучения она не встречала такой входящей длины.\n",
    "\n",
    "Оба типа позиционных представлений направлены на расширение возможностей LLM понимать порядок и взаимосвязи между токенами, обеспечивая более точные и контекстно-зависимые результаты. Выбор между ними часто зависит от конкретного приложения и характера обрабатываемых данных.\n",
    "\n",
    "Рассмотрим векторно - позиционное представление для относительно большой размерности 256 и словарь токенизатора BPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "# блок (батч) данных состоит из 8 кусков текста по 4 токена в каждом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# пропустив вход через токенизатор, получим 256 слоев размерности. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы можем сказать на основе выходных данных тензора размером 8x4x256, каждый идентификатор токена теперь внедрен как 256-мерный вектор.\n",
    "\n",
    "Для подхода абсолютного внедрения модели GPT нам просто нужно создать еще один слой внедрения, который имеет то же измерение, что и token_embedding_layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# добавляем позиционный токенизатор, которому нужно закодировать 4 позиции во входящей строке"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входными данными для pos_embeddings обычно является вектор-заполнитель torch.arange(context_length), который содержит последовательность чисел 0, 1,..., вплоть до максимальной входной длины (4 в примере). На практике входной текст может быть длиннее, чем поддерживаемая длина контекста, и в этом случае нам придется обрезать текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "# итоговое векторное представление текста с учетом позиции, подходящее для обучения LLM. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданные нами input_embeddings, как показано на рисунке представляют собой примеры входных данных, которые теперь могут обрабатываться основными модулями LLM.\n",
    "\n",
    "![](images/llm2.18.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9 Резюме\n",
    "\n",
    "- LLM требуют преобразования текстовых данных в числовые векторы, известные как векторные представления, поскольку они не могут обрабатывать \"сырой\" текст. Представления преобразуют дискретные данные (например, слова или изображения) в непрерывные векторные пространства, делая их совместимыми с тензорными операциями.\n",
    "- На первом этапе необработанный текст разбивается на токены, которые могут быть словами или символами. Затем токены преобразуются в целочисленные представления, называемые идентификаторами. Специальные токены, такие как <|unk|> и <|endoftext|>, можно добавлять для улучшения понимания модели и обработки различных контекстов, например неизвестных слов или обозначения границы между несвязанными текстами.\n",
    "- Токенизатор кодирования по парам байтов (BPE) может эффективно обрабатывать неизвестные слова, разбивая их на подслова или отдельные символы.\n",
    "- Мы используем подход скользящего окна для токенизированных данных для генерации пар входных-целевых данных для обучения LLM.\n",
    "- Представление создается стандартными функциями, с учетом длины блока входящих даннных и размерностью скрытых состояний модели. \n",
    "- Представление токена можно улучшить, добавив представление позиции во входящем блоке. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
